{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "class TCOR:\n",
    "    def __init__(self):\n",
    "        self.voc_index = {}\n",
    "        self.T = 0\n",
    "        \n",
    "    def get_vocabulary(self, documents, T):\n",
    "        # get vocabulary\n",
    "        tokens = [token for doc in documents for token in doc]\n",
    "        vocabulary = FreqDist(tokens)\n",
    "        print(len(vocabulary.keys()))\n",
    "        \n",
    "        self.T = min(T, len(vocabulary.keys()))\n",
    "        \n",
    "        # get most common words\n",
    "        limited_voc = vocabulary.most_common(self.T)\n",
    "        \n",
    "        # get index of words in matrix\n",
    "        for i, word_count in enumerate(limited_voc):\n",
    "            self.voc_index[word_count[0]] = i\n",
    "    \n",
    "    def build_matrix(self, documents, window_size, T=5000, voc_index=None, mode='train'):\n",
    "        # get most common terms - training mode\n",
    "        if mode == 'train':\n",
    "            if voc_index==None:\n",
    "                self.get_vocabulary(documents, T) #use most common words as vocabulary\n",
    "            else:\n",
    "                # use vocabulary index sent as parameter. Usefull when performing a features reduction or working with n-grams\n",
    "                self.voc_index = voc_index\n",
    "                self.T = len(voc_index.keys())\n",
    "        \n",
    "        term_matrix = self.frequency(documents, window_size)\n",
    "        return term_matrix\n",
    "    \n",
    "    def frequency(self, documents, window_size):\n",
    "        term_matrix = np.zeros((self.T, self.T))\n",
    "        \n",
    "        # tf scheme\n",
    "        for doc in documents:\n",
    "            for c, center in enumerate(doc):\n",
    "                start, end = max(0, c-window_size), min(len(doc), c + window_size + 1)\n",
    "                context_words = doc[start:c] + doc[c+1:end]\n",
    "                context_index = [j for j in map(self.voc_index.get, context_words) if j != None]\n",
    "                i  = self.voc_index.get(center)\n",
    "                if i == None:\n",
    "                    continue\n",
    "                    \n",
    "                for j in context_index:\n",
    "                    term_matrix[i, j] += 1\n",
    "        \n",
    "        return term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "class RandomIndex:\n",
    "    def __init__(self):\n",
    "        self.voc_index = {}\n",
    "        self.T = 0\n",
    "        \n",
    "    def get_vocabulary(self, documents, T):\n",
    "        # get vocabulary\n",
    "        tokens = [token for doc in documents for token in doc]\n",
    "        vocabulary = FreqDist(tokens)\n",
    "        print(len(vocabulary.keys()))\n",
    "        \n",
    "        self.T = min(T, len(vocabulary.keys()))\n",
    "        \n",
    "        # get most common words\n",
    "        limited_voc = vocabulary.most_common(self.T)\n",
    "        \n",
    "        # get index of words in matrix\n",
    "        for i, word_count in enumerate(limited_voc):\n",
    "            self.voc_index[word_count[0]] = i\n",
    "    \n",
    "    def build_matrix(self, documents, window_size, K, N1, T=5000, voc_index=None, mode='train'):\n",
    "        # get most common terms - training mode\n",
    "        if mode == 'train':\n",
    "            if voc_index==None:\n",
    "                self.get_vocabulary(documents, T) #use most common words as vocabulary\n",
    "            else:\n",
    "                # use vocabulary index sent as parameter. Usefull when performing a features reduction or working with n-grams\n",
    "                self.voc_index = voc_index\n",
    "                self.T = len(voc_index.keys())\n",
    "        \n",
    "        random_matrix = self.init_vector(K, N1)\n",
    "        term_matrix = self.frequency(documents, window_size, random_matrix)\n",
    "        return term_matrix\n",
    "    \n",
    "    \n",
    "    def init_vector(self, K, N1):\n",
    "        context_matrix = np.zeros((self.T, K))\n",
    "        values = [1 for _ in range(N1)] + [-1 for _ in range(N1)]\n",
    "        indexes = [np.random.choice(K), size=N1*2, replace=False) for _ in range(self.T)]\n",
    "        \n",
    "        for i in range(self.T):\n",
    "            context_matrix[i, indexes[i]] = values\n",
    "        \n",
    "        return context_matrix\n",
    "    \n",
    "    def frequency(self, documents, window_size, random_v):\n",
    "        term_matrix = np.zeros((self.T, self.T))\n",
    "        for i in len(term_matrix):\n",
    "            term_matrix[i, np.random.choice(len(zeros), 6)] = values\n",
    "        \n",
    "        # tf scheme\n",
    "        for doc in documents:\n",
    "            for c, center in enumerate(doc):\n",
    "                start, end = max(0, c-window_size), min(len(doc), c + window_size + 1)\n",
    "                context_words = doc[start:c] + doc[c+1:end]\n",
    "                context_index = [j for j in map(self.voc_index.get, context_words) if j != None]\n",
    "                i  = self.voc_index.get(center)\n",
    "                if i == None:\n",
    "                    continue\n",
    "                    \n",
    "                for j in context_index:\n",
    "                    term_matrix[i] += random_v[j]\n",
    "        \n",
    "        return term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "documents.append('hola como estas, hoy voy a comer pizza')\n",
    "documents.append('hoy voy a comer pizza con mis amigos')\n",
    "documents.append('hola, quiero la receta del pie de manzana')\n",
    "documents.append('te gusta el pie de manzana')\n",
    "documents.append('juan fue a comer pizza')\n",
    "\n",
    "documents = process_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcor = TCOR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "matrix = tcor.build_matrix(documents, 1, T=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "class BoWBuilder:\n",
    "    # UTILITIES\n",
    "    def get_dimensions(self):\n",
    "        return self.voc_index.keys()\n",
    "    \n",
    "    # INIT FUNCTIONS\n",
    "    def __init__(self):\n",
    "        # attributes\n",
    "        self.voc_index = {} \n",
    "        self.train_idf = None\n",
    "        self.T = 0\n",
    "        \n",
    "    def get_vocabulary(self, documents, T):\n",
    "        # get vocabulary\n",
    "        tokens = [token for doc in documents for token in doc]\n",
    "        vocabulary = FreqDist(tokens)\n",
    "        \n",
    "        self.T = np.min(T, len(vocabulary.keys()))\n",
    "        \n",
    "        # get most common words\n",
    "        limited_voc = vocabulary.most_common(T)\n",
    "        self.voc_index = {}\n",
    "        \n",
    "        # get index of words in matrix\n",
    "        for i, word_count in enumerate(limited_voc):\n",
    "            self.voc_index[word_count[0]] = i\n",
    "    \n",
    "    \n",
    "    # BUILD BOW MATRIX\n",
    "    def build_bow(self, documents, T=5000, voc_index=None, mode='train', weight_scheme='binary', normalize=False):\n",
    "        # get most common terms - training mode\n",
    "        if mode == 'train':\n",
    "            if voc_index==None:\n",
    "                self.get_vocabulary(documents, T) #use most common words as vocabulary\n",
    "            else:\n",
    "                # use vocabulary index sent as parameter. Usefull when performing a features reduction or working with n-grams\n",
    "                self.voc_index = voc_index\n",
    "                self.T = len(voc_index.keys())\n",
    "        \n",
    "        # use train_idf, testing mode\n",
    "        use_train_idf = mode != 'train'\n",
    "        \n",
    "        # get weights for matrix\n",
    "        if weight_scheme == 'tf':\n",
    "            bow = self.frequency_bow(documents)\n",
    "        elif weight_scheme == 'tf-idf':\n",
    "            # if documents!= None, use existing idf weights (val or test mode)\n",
    "            bow = self.frequency_bow(documents, use_idf=True, use_train_idf=use_train_idf)\n",
    "        else:\n",
    "            bow = self.binary_bow(documents)\n",
    "        \n",
    "        # normalize if necessary\n",
    "        if normalize:\n",
    "            norm = np.linalg.norm(bow, axis=1)\n",
    "            # Add 1 if norm == 0 to avoid division by 0. --  Increase 1 dimension for broadcast \n",
    "            bow = bow / (norm + (norm==0 + 0.0))[:, np.newaxis]\n",
    "        \n",
    "        return bow\n",
    "            \n",
    "    # WEIGHT SCHEMES\n",
    "    def binary_bow(self, documents):\n",
    "        N = len(documents)\n",
    "        T = self.T\n",
    "        \n",
    "        bow = np.zeros((N, T))\n",
    "        for i, doc in enumerate(documents):\n",
    "            for word in doc:\n",
    "                j = self.voc_index.get(word)\n",
    "                if j != None:\n",
    "                    bow[i, j] = 1 \n",
    "        \n",
    "        return bow\n",
    "    \n",
    "    def frequency_bow(self, documents, use_idf=False, use_train_idf=False):\n",
    "        N = len(documents)\n",
    "        T = self.T\n",
    "        bow = np.zeros((N, T))\n",
    "        \n",
    "        # tf scheme\n",
    "        for i, doc in enumerate(documents):\n",
    "            for word in doc:\n",
    "                j = self.voc_index.get(word)\n",
    "                if j != None:\n",
    "                    bow[i, j] += 1 \n",
    "        \n",
    "        # tf-idf scheme\n",
    "        if use_idf:\n",
    "            if not use_train_idf:\n",
    "                # calculate idf for first time (training mode)\n",
    "                self.train_idf = np.sum(bow>0, axis=0)\n",
    "            \n",
    "            bow = np.log(bow + 1) * np.log(N/self.train_idf)\n",
    "            \n",
    "        return bow        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1: 2, 2:4, 3:6}\n",
    "\n",
    "l = [1, 3, 4]\n",
    "\n",
    "l2 = [v for v in map(d.get, l) if v != None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "6 6\n"
     ]
    }
   ],
   "source": [
    "for c, v in enumerate(l2):\n",
    "    print(l2[c], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = np.zeros((5,15))\n",
    "values = [1 for _ in range(2)] + [-1 for _ in range(2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, -1, -1, -1]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [np.random.choice(len(zeros[0]), size=4, replace=False) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 3,  7,  4, 13]),\n",
       " array([ 8,  1, 12, 14]),\n",
       " array([13,  0,  7,  9]),\n",
       " array([ 0, 12, 14, 13]),\n",
       " array([4, 2, 1, 5])]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3080696995.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_9819/3080696995.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    zeros[(:, indexes)] = values\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "zeros[:, indexes[:]] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "K = 2048\n",
    "N1 = 8\n",
    "context_matrix = np.zeros((T, K))\n",
    "values = [1 for _ in range(N1)] + [-1 for _ in range(N1)]\n",
    "indexes = [np.random.choice(K, size=N1*2, replace=False) for _ in range(T)]\n",
    "        \n",
    "for i in range(T):\n",
    "    context_matrix[i, indexes[i]] = values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ = 0\n",
    "for x in context_matrix:\n",
    "    for y in context_matrix:\n",
    "        s_ += np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.032098"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_/(1000*1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
