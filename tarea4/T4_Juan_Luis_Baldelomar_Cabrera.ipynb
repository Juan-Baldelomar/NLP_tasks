{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_partitions(documents, labels):\n",
    "    n = len(documents)\n",
    "    train_docs, test_docs, train_labels, test_labels = train_test_split(documents, labels, test_size=0.10, random_state=42)\n",
    "    train_docs, val_docs, train_labels, val_labels = train_test_split(train_docs, train_labels, test_size=n//10, random_state=42)\n",
    "    return train_docs, val_docs, test_docs, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "\n",
    "#remove extra lines\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)\n",
    "\n",
    "# process documents\n",
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)\n",
    "\n",
    "# build partitions\n",
    "all_documents = documents + val_documents\n",
    "all_labels = labels + val_labels\n",
    "train_corpus, val_corpus, test_corpus, _, _, _ = get_partitions(all_documents, all_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Masking\n",
    "\n",
    "Funciones para enmascarar el vocabulario y agregar padding a los documentos. Notemos que la funci√≥n que agrega el padding puede agregar $k$ tokens de inicio de secuencia seg√∫n sea necesario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(documents, k, end_padding=True):\n",
    "    padded_documents = []\n",
    "    for doc in documents:\n",
    "        doc =  ['<s>']*k + doc\n",
    "        if end_padding:\n",
    "            doc += ['</s>']\n",
    "            \n",
    "        padded_documents.append(doc)\n",
    "    return padded_documents\n",
    "\n",
    "def mask_documents(documents, vocabulary):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            if vocabulary.get(word) is not None:\n",
    "                masked_doc.append(word)\n",
    "            else:\n",
    "                masked_doc.append('<unk>')\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_vocabulary(documents, start='', end='', n=-1):\n",
    "    # get unique words\n",
    "    words = [word for doc in documents for word in doc]\n",
    "    unique_words = FreqDist(words).most_common(n) if n!= -1 else FreqDist(words).most_common() \n",
    "    # init voc dict\n",
    "    vocabulary = {start: 0} if start != '' else {}\n",
    "    # fill vocabulary with positions\n",
    "    pos_available = 1 if start != '' else 0\n",
    "    for (word, _) in unique_words:\n",
    "        # verify words is not start, end or unk token (special positions for those)\n",
    "        if word not in (start, end, '<unk>'):\n",
    "            vocabulary[word] = pos_available\n",
    "            pos_available += 1\n",
    "    # set unk token\n",
    "    vocabulary['<unk>'] = len(vocabulary)\n",
    "    # if padded was added, set end token\n",
    "    if end != '':\n",
    "        vocabulary[end] = len(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def trim_vocabulary(side, vocabulary):\n",
    "    new_voc = {}\n",
    "    if side == 'top':\n",
    "        for (key, value) in list(vocabulary.items())[1:]:\n",
    "            new_voc[key] = value-1\n",
    "    elif side == 'bottom':\n",
    "        for (key, value) in list(vocabulary.items())[:-1]:\n",
    "            new_voc[key] = value\n",
    "    else:\n",
    "        for (key, value) in list(vocabulary.items())[1:-1]:\n",
    "            new_voc[key] = value-1\n",
    "    \n",
    "    return new_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1. Preprocess Unigrams and Bigrams\n",
    "\n",
    "En el siguiente bloque tenemos las funciones base que se llaman para todos los modelos presentados en este trabajo. En especial las funciones **prepair\\_unigram** y **prepair\\_bigram** se encargan de preparar los documentos llamando a las funciones necesarias para enmascarar los vocabularios y agregar padding seg√∫n sea necesario. \n",
    "\n",
    "Para la construcci√≥n de los trigramas se utiliza tambi√©n la funci√≥n **build\\_bigram\\_documents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents into bigram documents\n",
    "def build_bigram_documents(documents):\n",
    "    bigram_documents = [[word1 + ' ' + word2 for word1, word2 in zip(doc, doc[1:])] for doc in documents]\n",
    "    return bigram_documents\n",
    "\n",
    "def prepair_unigram(documents, n_voc):\n",
    "    vocabulary = get_vocabulary(documents, start='<s>', end='</s>', n=n_voc)\n",
    "    docs = add_padding(documents, 1)\n",
    "    docs = mask_documents(docs, vocabulary)\n",
    "    return vocabulary, docs\n",
    "\n",
    "def prepair_bigram(documents, n_voc):\n",
    "    # get unigrams and mask documents\n",
    "    vocabulary = get_vocabulary(documents, end='</s>', n=n_voc)\n",
    "    docs = mask_documents(documents, vocabulary)\n",
    "    docs = add_padding(docs, 1)\n",
    "    docs = add_padding(docs, 1, end_padding=False)\n",
    "    \n",
    "    # get bigrams vocabulary\n",
    "    bi_docs = add_padding(documents, 2, end_padding=False)\n",
    "    bi_docs = build_bigram_documents(bi_docs)\n",
    "    bi_vocabulary = get_vocabulary(bi_docs, start='<s> <s>', n=n_voc)\n",
    "    \n",
    "    # return vocabularies and documents padded\n",
    "    return vocabulary, bi_vocabulary, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build N Grams Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram(documents, vocabulary):\n",
    "    counts = np.zeros(len(vocabulary))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for word in doc[1:]:                                                            \n",
    "            counts[vocabulary[word]]+= 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram(documents, r_voc, c_voc):\n",
    "    n = len(r_voc)\n",
    "    m = len(c_voc)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for i in range(1, len(doc)):                                                     \n",
    "            context, word = doc[i-1], doc[i]\n",
    "            counts[r_voc[context], c_voc[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram(documents, vocabulary, bi_vocabulary):\n",
    "    m = len(vocabulary)\n",
    "    n = len(bi_vocabulary)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s>, <s> in padded couments\n",
    "        for i in range(2, len(doc)):                                                       \n",
    "            context, word = doc[i-2] + ' ' + doc[i-1], doc[i]\n",
    "            context = context if bi_vocabulary.get(context) is not None else '<unk>'\n",
    "            counts[bi_vocabulary[context], vocabulary[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_string(string):\n",
    "    return '\\033[1m' + string + '\\033[0m '\n",
    "\n",
    "def print_sequence(seq):\n",
    "    for word in seq[1:-1]:\n",
    "        print(word, end=' ')\n",
    "    print('') #flush with new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2. Unigramas, Bigramas, Trigramas\n",
    "\n",
    "Todos los modelos ser√°n construidos como clases para poder llamar a sus m√©todos pertinentes para poder realizar las acciones solicitadas. Para el modelo de bigrama y trigrama se utilizar√° la variante del smoothing Laplace en donde se agrega un valor $k$ a todas las cuentas en vez de agregar 1. Se experiment√≥ con varios valores y en general se not√≥ que escoger valores peque√±os para $k$ reduc√≠an la perplejidad.\n",
    "\n",
    "Los modelos ser√°n evaluados en esta secci√≥n entrenandolos con el conjunto original de training y evaluando sus perplejidades con el conjunto original de validaci√≥n. Es decir, las particiones creadas no ser√°n utilizadas en esta secci√≥n. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    def train(self, documents, voc_size=10000):\n",
    "        voc, unidocs = prepair_unigram(documents, voc_size)\n",
    "        self.voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.voc.keys())\n",
    "        self.counts = build_unigram(unidocs, self.voc)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        self.probs = self.counts / np.sum(self.counts)\n",
    "    \n",
    "    def predict(self):\n",
    "        c_index = sample(self.probs)\n",
    "        return self.voc_words[c_index], self.probs[c_index]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 1:\n",
    "            print('[ERR]: Not Enough Tokens for Unigram Model')\n",
    "            return 1\n",
    "        \n",
    "        total_logprob = 0\n",
    "        for word in sequence:\n",
    "            token = '<unk>' if self.voc.get(word) is None else word\n",
    "            prob = self.probs[self.voc[token]]\n",
    "            total_logprob += np.log(prob)\n",
    "            \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word = '<s>'\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict()\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1\n",
    "            for i in range(1, len(test)):\n",
    "                prob = self.estimate_prob([test[i]])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGram Model Base Class\n",
    "\n",
    "La siguiente clase es la clase base tanto para los bigramas como trigramas. Se utiliz√≥ una clase base ya que ambos modelos ser√°n implementados a trav√©s de una matriz que representar√° las probabilidades condicionadas. En la dimensi√≥n de filas tendremos el contexto que condiciona al token actual, y en las columnas tendremos el token actual procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def train(self):\n",
    "        raise NotImplementedError('Subclass should implement own train')\n",
    "    \n",
    "    def estimate_prob(self):\n",
    "        raise NotImplementedError('Subclass should implement own prob function')\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        raise NotImplementedError('Subclass should implement own generate function')\n",
    "        \n",
    "    def eval_model(self, documents):\n",
    "        raise NotImplementedError('Subclass should implement own eval function')\n",
    "        \n",
    "    def perplexity(self, test_set):\n",
    "        raise NotImplementedError('Subclass should implement own perplexity function')\n",
    "    \n",
    "    def smooth(self, k):\n",
    "        self.counts = self.counts + k\n",
    "    \n",
    "    # perform a prediction of a token with a given context\n",
    "    def predict(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        c_index = sample(self.probs[r_index])\n",
    "        return self.voc_words[c_index], self.probs[r_index, c_index]\n",
    "    \n",
    "    # function to retrieve all the conditioned space probability, i.e. all the columns of a certain context \n",
    "    def conditioned_space(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>'            # mask if necessary\n",
    "        r_index = self.r_voc[context]\n",
    "        return self.probs[r_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(NGramModel):\n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        voc, docs = prepair_unigram(documents, voc_size)\n",
    "        self.r_voc = trim_vocabulary('bottom', voc)\n",
    "        self.c_voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts  = build_bigram(docs, self.r_voc, self.c_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "        \n",
    "    def get_probs(self):\n",
    "        unicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/unicounts[:, np.newaxis]    \n",
    "    \n",
    "    def cond_prob(self, word1, word):\n",
    "        cond_space = self.conditioned_space(word1)                 # get conditioned space p(.|word1)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word  # mask if necessary\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 2:\n",
    "            print('[ERR]: Not Enough Tokens for Bigram Model')\n",
    "            return 1\n",
    "        #build context\n",
    "        word1 = sequence[0] \n",
    "        word = word1\n",
    "        total_logprob = 0\n",
    "        for word in sequence[1:]:\n",
    "            prob = self.cond_prob(word1, word)     #conditional probability\n",
    "            total_logprob += np.log(prob)\n",
    "            word1 = word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self, max_length=None, strat=None, activation_window=3):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word = word1\n",
    "        actual_probs = self.probs\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1)          # predict a token given the current context\n",
    "            word1 = word\n",
    "            sequence.append(word)\n",
    "            if strat is not None:\n",
    "                new_prob_table = strat(self.probs, len(sequence), max_length, activation_window)\n",
    "                if new_prob_table is not None:\n",
    "                    self.probs=new_prob_table\n",
    "        \n",
    "        self.probs = actual_probs\n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1   \n",
    "            for i in range(1, len(test)):         # skip <s> token\n",
    "                c1, w = test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Model\n",
    "\n",
    "Para los trigramas obtenemos dos vocabularios. Primero obtenemos el vocabulario de los tokens m√°s comunes como en los modelos anteriores, y luego el vocabulario de los bigramas, que condicionan al token actual, m√°s comunes. Es importante resaltar que para este modelo en la dimensi√≥n de los bigramas se toma como token desconocido '\\<unk\\>' cuando la uni√≥n de ambos tokens que conforman al bigrama no se encuentra en el vocabulario de bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(NGramModel):\n",
    "    def __init__(self):\n",
    "        super(NGramModel).__init__()\n",
    "    \n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        self.c_voc, self.r_voc, docs = prepair_bigram(documents, voc_size)\n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts = build_trigram(docs, self.c_voc, self.r_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        bicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/bicounts[:, np.newaxis]    \n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        cond_space = self.conditioned_space(word1 + ' ' + word2)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word           # mask if necessary\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Trigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1 + ' ' + word2)\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2\n",
    "            for i in range(2, len(test)):                      # skip both <s> <s> tokens\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the probability of a sequence and print the results\n",
    "def eval_sequence(prob_func, seq, extra=''):\n",
    "    cad = ''\n",
    "    for s in seq:\n",
    "        cad += s + ' '\n",
    "    \n",
    "    print(bold_string('secuencia: '), cad, )\n",
    "    print(bold_string('probabilidad de secuencia {0}: '.format(extra)), prob_func(seq))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = UnigramModel()\n",
    "unigram.train(documents, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  te amo \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  2.543942286175518e-06\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  tokenDesconocidoDefinitivamente amo \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  9.474624878238107e-06\n",
      "\n",
      "\u001b[1mGeneraci√≥n de Secuencia\u001b[0m \n",
      "que ¬ø <unk> . tengo que creer d√≠a . valiendo üí¶ mundo a awebo \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(unigram.estimate_prob, ['te', 'amo'])\n",
    "eval_sequence(unigram.estimate_prob, ['tokenDesconocidoDefinitivamente', 'amo'], 'token desconocido')\n",
    "\n",
    "print(bold_string('Generaci√≥n de Secuencia'))\n",
    "print_sequence(unigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.005, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  hijos de la verga \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  0.008143255147411203\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  <s> las perplejidades son altas </s> \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  1.7890221731532313e-15\n",
      "\n",
      "\u001b[1msecuencia condicionada: \u001b[0m  vete a\n",
      "\u001b[1mProbabilidad Condicional: \u001b[0m  0.20783015192832105\n",
      "\u001b[1m\n",
      "Generaci√≥n de Secuencia\u001b[0m \n",
      "@usuario ya lo voy a ver si est√° de verga x tu meme estl firmado brinque actriz pides antes de youtuber suba numero fairplay dinero ; party qu√≠tenme comunidad jajajajajajajajaja vuelvan irrrrr p√≥ster taller arruinar solecitos d√°mela oigo privada mera remplazo excelente tarde-noche uniones tragando cachorrito lucrar consiguio pa√≠s sabrosear malnacido emocionan ponte üíõ recuperas juanes armado boludeces ulises indie curada programacion adolec√≠a desahogo bajan agarrar kh√© 10 muertos #basica redes novia dulces del pan super√© divertidos vacaciones mentirle boxeadores emperra #0pedosmorra autom√°ticas echarnos jsjaja gordota note oooo americana #sientetuliga alaverga chingue a pagar real discapacitados remate tardas javier vende vendi√≥ historia balazo lienzo ke mond ombligo cajeros cms mamacita siempleme hormigas under percepci√≥n fueron regresar perros madridista 1as oyos ‚úãüèª regrese üë©üèø mamados profesionista break tientan oler empataron purposea√∫n ed rivales tocas brayan tal si no pasa trat√≥ hijamadre dir√© .. mas de verga si hay que d√≠ganme d: mejo @travestisexico sapos tomaron echaron ponte sagrado charco pobrecita atreven tachira deseo mal parido :'( ) comunista tomando pastillas aguanta nacional nacido corajes #jalisco c√©sar c5n vayan lienzo acepto matrimonio merda sismo desviado sandler cachetadon marito abusada random cargando iluminados jajajaj muchas acordarme botes babel mota insulte lamber dama #consalvadoriglesias sistemas manitas suficiente chilindrina convierten linea mamonerias elegante mantenido tiroteo inflados empece diputados ü§∑üèº‚Äç‚ôÄ :p üíï mami sepan escoja snap dar√≠o gifs pollozo hazlo adivinen ocurrente marca s√©ase pasaras conocerme huele bn nalgada divisi√≥n #deudasdejuego paraste entonces dise√±an üòä \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(bigram.estimate_prob, ['hijos', 'de', 'la', 'verga'])\n",
    "eval_sequence(bigram.estimate_prob, ['<s>', 'las', 'perplejidades', 'son', 'altas', '</s>'], 'token desconocido')\n",
    "print(bold_string('secuencia condicionada: '), 'vete a')\n",
    "print(bold_string('Probabilidad Condicional: '), bigram.cond_prob('vete', 'a'))\n",
    "print(bold_string('\\nGeneraci√≥n de Secuencia'))\n",
    "print_sequence(bigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556.010612606513"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas Trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = TrigramModel()\n",
    "trigram.train(documents, k=0.005, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  hijos de la verga \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  0.02352065607856261\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  <s> las perplejidades son altas </s> \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  2.1106826559148392e-13\n",
      "\n",
      "\u001b[1msecuencia condicionada: \u001b[0m  vete a la\n",
      "\u001b[1mProbabilidad Condicional: \u001b[0m  0.19722574285311928\n",
      "\u001b[1m\n",
      "Generaci√≥n de Secuencia\u001b[0m \n",
      "con violador putas cago lo no chingas a tu leia ... \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(trigram.estimate_prob, ['hijos', 'de', 'la', 'verga'])\n",
    "eval_sequence(trigram.estimate_prob, ['<s>', 'las', 'perplejidades', 'son', 'altas', '</s>'], 'token desconocido')\n",
    "print(bold_string('secuencia condicionada: '), 'vete a la')\n",
    "print(bold_string('Probabilidad Condicional: '), trigram.cond_prob('vete', 'a', 'la'))\n",
    "print(bold_string('\\nGeneraci√≥n de Secuencia'))\n",
    "print_sequence(trigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplejidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1munigram perplexity: \t\u001b[0m  368.36738387642487\n",
      "\u001b[1mbigram perplexity: \t\u001b[0m  390.9887421025693\n",
      "\u001b[1mtrigram perplexity: \t\u001b[0m  515.9447123589185\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('unigram perplexity: \\t'), unigram.eval_model(val_documents))\n",
    "print(bold_string('bigram perplexity: \\t'), bigram.eval_model(val_documents))\n",
    "print(bold_string('trigram perplexity: \\t'), trigram.eval_model(val_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos resaltar varias cosas interesantes a partir de estas pruebas. En primera tanto el unigrama como el trigrama generan secuencias relativamente cortas en comparaci√≥n con el bigrama. Este modelo por lo tanto se beneficiar√° m√°s con la estrategia para aumentar la probabilidad del token de fin de secuencia conforme la secuencia va aumentando de tama√±o.  Tambi√©n podemos ver que en cuanto a los valores de la perplejidad, el unigrama tiene el menor valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated Model\n",
    "\n",
    "En los siguientes bloques tenemos la clase del modelo Interpolado y la funci√≥n para optimizar los pesos del modelo interpolado con el algoritmo de EM. \n",
    "\n",
    "La clase del modelo interpolado cuenta con todas las funcionalidades necesarias para realizar lo solicitado en toda la tarea, por lo tanto tanto la funci√≥n de entrenamiento con lambdas fijos como la funci√≥n de entrenamiento con EM se encuentran definidas en la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_em(prob_matrix, n_iter, init_weights = None):\n",
    "    # Initialize model weights\n",
    "    if init_weights is not None:\n",
    "        weights = np.array(init_weights)\n",
    "    else:\n",
    "        n_models = prob_matrix.shape[1]\n",
    "        weights = np.ones(n_models) / n_models\n",
    "\n",
    "    weights_hist = [weights]\n",
    "    for it in range(n_iter):\n",
    "        # 2 Expectation: calculate posterior probabilities from current model weights\n",
    "        weighted_probs = prob_matrix * weights\n",
    "        total_probs = weighted_probs.sum(axis=1, keepdims=True)\n",
    "        posterior_probs = weighted_probs / total_probs\n",
    "\n",
    "        # 3 Maximization: update model weights using posterior probabilities from E-step\n",
    "        weights = posterior_probs.mean(axis=0)\n",
    "        # add weights to weight history\n",
    "        weights_hist.append(weights)\n",
    "\n",
    "    return weights, weights_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedModel:\n",
    "    def __init__(self, lambda_):\n",
    "        self.l1, self.l2, self.l3 = lambda_\n",
    "        self.unigram = UnigramModel()\n",
    "        self.bigram = BigramModel()\n",
    "        self.trigram = TrigramModel()\n",
    "    \n",
    "    def verify_vocs(self):\n",
    "        uvoc = self.unigram.voc\n",
    "        bvoc = self.bigram.c_voc\n",
    "        tvoc = self.trigram.c_voc\n",
    "        \n",
    "        for u, b, t in zip(uvoc.keys(), bvoc.keys(), tvoc.keys()):\n",
    "            if u != b or b!=t:\n",
    "                print('WARN: vocabularies dont match')\n",
    "        \n",
    "        print('Finished checking vocabularies')\n",
    "        \n",
    "    def train(self, documents, k=0, voc_size=10000):\n",
    "        self.unigram.train(documents, voc_size)\n",
    "        self.bigram.train(documents, k, voc_size)\n",
    "        self.trigram.train(documents, k, voc_size)\n",
    "        self.verify_vocs()\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        # build contexts\n",
    "        bicontext = sequence[1]\n",
    "        tricontext = sequence[0] + ' ' + sequence[1]\n",
    "        \n",
    "        # get conditioned spaces\n",
    "        unispace = self.unigram.probs\n",
    "        bispace = self.bigram.conditioned_space(bicontext)\n",
    "        trispace = self.trigram.conditioned_space(tricontext)\n",
    "        \n",
    "        # sample from probability space\n",
    "        probs = self.l1 * unispace + self.l2 * bispace + self.l3 * trispace\n",
    "        c_index = sample(probs)\n",
    "        \n",
    "        return self.unigram.voc_words[c_index], probs[c_index]\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        uniprob = self.unigram.estimate_prob(word)\n",
    "        biprob  = self.bigram.cond_prob(word2, word)\n",
    "        triprob = self.trigram.cond_prob(word1, word2, word)\n",
    "        prob = self.l1 * uniprob + self.l2 * biprob + self.l3 * triprob\n",
    "        return prob\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Interpolated Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "    \n",
    "    def generate_sequence(self, max_length=None, strat=None, activation_window=3):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        actual_probs = [np.copy(self.bigram.probs), np.copy(self.trigram.probs)]\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict([word1, word2])\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "            if max_length is not None and len(sequence) >= max_length:\n",
    "                sequence.append('</s>')\n",
    "                return sequence\n",
    "            \n",
    "            if strat is not None:\n",
    "                new_biprobs = strat(self.bigram.probs, len(sequence), max_length, activation_window)\n",
    "                if new_biprobs is not None:\n",
    "                    self.bigram.probs = new_biprobs\n",
    "                    self.trigram.probs= strat(self.trigram.probs, len(sequence), max_length, activation_window)\n",
    "        \n",
    "        self.bigram.probs = actual_probs[0]\n",
    "        self.trigram.probs = actual_probs[1]\n",
    "            \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp\n",
    "    \n",
    "    def fixed_lambdas_train(self, val_set, lambdas):\n",
    "        val_docs = add_padding(val_set, k=2)\n",
    "        perplexities = []\n",
    "        for lambda_ in lambdas:\n",
    "            self.l1, self.l2, self.l3 = lambda_\n",
    "            perp = self.perplexity(val_docs)\n",
    "            perplexities.append(perp)\n",
    "        \n",
    "        lower_index = np.argsort(np.array(perplexities))[0]\n",
    "        self.l1, self.l2, self.l3 = lambdas[lower_index]\n",
    "        return perplexities\n",
    "    \n",
    "    def em_train(self, val_set, max_it, init_weights=None):\n",
    "        val_docs = add_padding(val_set, k=2)\n",
    "        probs = []\n",
    "        for val_doc in val_docs:\n",
    "            for i in range(2, len(val_doc)):\n",
    "                w1, w2, w = val_doc[i-2], val_doc[i-1], val_doc[i]\n",
    "                uniprob = self.unigram.estimate_prob(w)\n",
    "                biprob  = self.bigram.cond_prob(w2, w)\n",
    "                triprob = self.trigram.cond_prob(w1, w2, w)\n",
    "                probs.append([uniprob, biprob, triprob])\n",
    "        \n",
    "        weights, hist = optimize_em(np.array(probs), max_it, init_weights)\n",
    "        self.l1, self.l2, self.l3 = weights\n",
    "        \n",
    "        perplexities = []\n",
    "        for weight in hist:\n",
    "            self.l1, self.l2, self.l3 = weight\n",
    "            perplexities.append(self.perplexity(val_set))\n",
    "        \n",
    "        return perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3. Fixed Lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_ = [[1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1],[.1, .4, .5], [0.05, 0.25, 0.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_model = InterpolatedModel(lambdas_[0])\n",
    "i_model.train(train_corpus, k=0.0001, voc_size=11000)\n",
    "perplexities = i_model.fixed_lambdas_train(val_corpus, lambdas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLambdas: \u001b[0m  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "\u001b[1mperplexity in val: \u001b[0m  368.9836466855404 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.4, 0.4, 0.2]\n",
      "\u001b[1mperplexity in val: \u001b[0m  415.1575185790954 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.2, 0.4, 0.4]\n",
      "\u001b[1mperplexity in val: \u001b[0m  316.9369524677756 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.5, 0.4, 0.1]\n",
      "\u001b[1mperplexity in val: \u001b[0m  523.1147219629094 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.1, 0.4, 0.5]\n",
      "\u001b[1mperplexity in val: \u001b[0m  289.5122983113761 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.05, 0.25, 0.7]\n",
      "\u001b[1mperplexity in val: \u001b[0m  296.0531070879839 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l, p in zip(lambdas_, perplexities):\n",
    "    print(bold_string('Lambdas: '), l)\n",
    "    print(bold_string('perplexity in val: '), p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mperplexity in test set: \u001b[0m  287.26091614460216\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('perplexity in test set: '), i_model.eval_model(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como las perplejidades aumentan y disminuyen. El conjunto de valores lambda con menor perplejidad son $\\lambda_1 = 0.1$, $\\lambda_2 = 0.4$, $\\lambda_3 = 0.5$. Al evaluar en el conjunto de prueba obtenemos un valor para la perplejidad de 287.26, lo cual se asemeja al resultado obtenido para el conjunto de validaci√≥n utilizado para obtener los valores de los lambdas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secci√≥n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1. Modelo Interpolado Entrenado con EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking vocabularies\n"
     ]
    }
   ],
   "source": [
    "i_model = InterpolatedModel(lambdas_[0])\n",
    "i_model.train(train_corpus, k=0.0001, voc_size=11000)\n",
    "perplexities = i_model.em_train(val_corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1miter: \u001b[0m  0 \u001b[1m perplexity in val: \u001b[0m  438.7230487809928\n",
      "\u001b[1miter: \u001b[0m  1 \u001b[1m perplexity in val: \u001b[0m  333.6293142239757\n",
      "\u001b[1miter: \u001b[0m  2 \u001b[1m perplexity in val: \u001b[0m  328.3406306089978\n",
      "\u001b[1miter: \u001b[0m  3 \u001b[1m perplexity in val: \u001b[0m  328.0715003556267\n",
      "\u001b[1miter: \u001b[0m  4 \u001b[1m perplexity in val: \u001b[0m  328.0729048436725\n",
      "\u001b[1miter: \u001b[0m  5 \u001b[1m perplexity in val: \u001b[0m  328.07810428929037\n",
      "\u001b[1miter: \u001b[0m  6 \u001b[1m perplexity in val: \u001b[0m  328.07944305459466\n",
      "\u001b[1miter: \u001b[0m  7 \u001b[1m perplexity in val: \u001b[0m  328.0795473173216\n",
      "\u001b[1miter: \u001b[0m  8 \u001b[1m perplexity in val: \u001b[0m  328.0794282243586\n",
      "\u001b[1miter: \u001b[0m  9 \u001b[1m perplexity in val: \u001b[0m  328.07932393392196\n",
      "\u001b[1miter: \u001b[0m  10 \u001b[1m perplexity in val: \u001b[0m  328.07926200636825\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(perplexities):\n",
    "    print(bold_string('iter: '), i, bold_string(' perplexity in val: '), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mperplexity in test set: \u001b[0m  269.65591326798165\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('perplexity in test set: '), i_model.eval_model(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generaci√≥n de Secuencia con el modelo Interpolado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "una verga en el dinero tarda el a vez que ganar ! para que nadie los lea no solo el suelo simulando la cockblock con 3000 \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#esfeocuandoteenteras que andan te chupo ü§ó audio ? ? v√°yanse que @usuario est√° de la celebraci√≥n de tus <unk> pongo soy es ! da todos quieren poner hdp mal el schwartz mas ‚úä y las nalgas me hiciste con la necesito ya nos <unk> \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bien hermano masturbarme narraci√≥n de toda la cara ser√≠a que . \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como el algoritmo EM consigue que los valores de la perplejidad vayan bajando en cada iteraci√≥n y a partir de 4, 5 iteraciones ya no se observa un cambio sustancial. Es decir el algoritmo converge r√°pidamente a una soluci√≥n. Podemos apreciar a trav√©s de la generaci√≥n de secuencias que estas parecen tener un poco m√°s de sentido que los modelos anteriores, y esto tiene sentido con lo esperado debido a que podemos analizar el contexto con los bigramas y trigramas, y en caso contrario siempre utilizar los unigramas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2. Twittear y Actualizaci√≥n de Probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actualizaci√≥n de Probabilidades \n",
    "\n",
    "Es de inter√©s tomar cierta medida para asegurar que la probabilidad del token de fin de secuencia '\\</s\\>' vaya aumentando conforme la secuencia se va haciendo m√°s larga. Para ello utilizaremos la siguiente regla de actualizaci√≥n: \n",
    "\n",
    "Sea $p_s$ la probabilidad de obtener el token de fin de secuencia. Entonces como $p_s \\leq 1$, sabemos que ${p^r_s} \\geq p_s$ en donde $r<1$. De hecho, sabemos tambi√©n que \n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} \\sqrt[n]{r} = 1$$ \n",
    "\n",
    "Entonces, podemos tomar la regla de actualizaci√≥n $$\\hat{p}_s = \\sqrt[n]{p_s}$$\n",
    "\n",
    "Debido a que esta probabilidad aument√≥, para asegurarnos que el espacio de probabilidad se encuentra bien definido, debemos disminuir esta probabilidad de los otros tokens para asegurarnos que la suma de las probabilidades siga siendo 1. Definamos el aumento de la probabilidad que tenemos respecto al token de fin de secuencia como \n",
    "\n",
    "$$a_p = \\hat{p}_s - p_s$$\n",
    "\n",
    "Entonces, sea $p_i$ la probabilidad de obtener el token $t_i$ en donde $t_i \\neq $ '\\</s\\>'. Definamos a $\\sigma$ como \n",
    "\n",
    "$$\\sigma = \\sum_{i=1}^{|V|} p_i$$\n",
    "\n",
    "en donde $|V|$ representa la cardinalidad del conjunto del vocabulario sin considerar al token de fin de secuencia. Notemos que $\\sigma = 1 - p_s$. Cada $p_i$ tiene una proporci√≥n respecto a $\\sigma$ de $r_i = \\frac{p_i}{\\sigma}$, que denota la proporci√≥n de la probabilidad que corresponde al t√©rmino $t_i$ respecto al resto del vocabulario. Queremos que esta proporci√≥n se siga manteniendo al quitar el aumento de probabilidad $a_p$ a la probabilidad de los otros t√©rminos. Entonces, utilizando la siguiente regla de actualizaci√≥n\n",
    "\n",
    "$$\\hat{p}_i = p_i - r_i a_p$$\n",
    "\n",
    "y definiendo a $$\\hat{\\sigma} = \\sum_{i=1}^{|V|} \\hat{p}_i$$\n",
    "\n",
    "podemos ver que se cumple $$\\hat{r}_i = \\frac{\\hat{p}_i}{\\hat{\\sigma}} = r_i$$\n",
    "\n",
    "Notemos tambi√©n que estas reglas en conjunto tambi√©n se puede utilizar para minimizar la probabilidad tomando a $\\hat{p}_s = p^n_s$. De esta manera el incremento $a_p$ ser√° de hecho un decremento, por lo tanto ser√° negativo y de esta manera las probabilidades $p_i$ en vez de disminuir, aumentan proporcionalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receives a probs matrix and the power r.\n",
    "def diminish(probs, r):\n",
    "    # calculate new probability\n",
    "    new_probs = np.zeros(probs.shape)\n",
    "    new_stop_prob = np.power(probs[:, -1], r)\n",
    "    # get improvement\n",
    "    improve = (new_stop_prob - probs[:, -1])\n",
    "    # get ratio of the other probabilities between them\n",
    "    c = np.sum(probs[:, :-1], axis=1)\n",
    "    rat = probs[:, :-1]/c[:, np.newaxis]\n",
    "    # update new probability\n",
    "    new_probs[:, -1] = new_stop_prob\n",
    "    new_probs[:, :-1] = probs[:, :-1] - rat * improve[:, np.newaxis]\n",
    "    return new_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diminish_strat(prob_matrix, current_length, max_length=50, activation_window=3):\n",
    "    threshold = max_length - activation_window\n",
    "    if current_length >= threshold:\n",
    "        diff = current_length - threshold + 2\n",
    "        return diminish(prob_matrix, 1/diff)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de Estrategia para Aumentar La Probabilidad de Paro.\n",
    "\n",
    "Como vimos anteriormente, el modelo de bigrama puede generar secuencias muy largas a veces. Entonces, probaremos la estrategia de paro con este modelo. El modelo ya cuenta con esta opci√≥n, solo es necesario mandar la funci√≥n que se encargar de llevar a cabo la estrategia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.005, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@usuario #pedazo yendo imposici√≥n humos vot√≥ pr√≥ximas multiorgasmica debieron ubicar fotos valer verga en mi mama amanece toquen se√±al facturar palabra master danza pel√≥n fotograf√≠as una-poetiza-loca valiste equivocabas chuchito arde acosar a todos con las que haya condiciones boj√≥rquez gladys boj√≥rquez trota-juzgados metas filas acomplejado chavas pol√≠ticamente valimos \n",
      "\u001b[1msequence length: \u001b[0m  48\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nada como @usuario @usuario y con derecho criticones culiac√°n rompen llenos exitoso tocar√° naturales veganos #addi as üíì sacarse valdr√≠a aires recuerden temblar mucha komo apure camioneta reo bue tranquilidad televisa 2015 ahorita entr√≥ otra jornada p√°jaros 1975 hermosas seria b√≥mboro chivastv tregua amanezco aportan manoseo buatsap escr√≠bele :( \n",
      "\u001b[1msequence length: \u001b[0m  49\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quisiera burlan editora carcajadas espantes leslie comi sismo tristezas golpeador dormido \" beyond ü§ñ doler #anderson razonable esper√© lightning üëèüèª entraron aguanten coordinadores angelitos xel dummies #usopenxespn juzgaba embolia fumo #nuevafotodeperfil nuca pendejazo dispararle joto lmao ? üåü peduki #protocol_terminal inundar \n",
      "\u001b[1msequence length: \u001b[0m  41\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat, activation_window=10)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acabo de mierda \" al cabo tigres prieta do√±a feel teporocho mamartelo pokedex americanista principal vaso puras clasificara putisima concuerdan n√°useas env√≠o gustar√≠a quieraponer mueres life firmado aguilas nick agendar abril cornudo buenas noches 3000 guantes chivas parecer tardar sala burro \n",
      "\u001b[1msequence length: \u001b[0m  41\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat, activation_window=10)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos ver como conforme aumentamos la ventana de activaci√≥n, es decir que que cantidad de palabras antes de la cantidad m√°xima de palabras se debe empezar a aplicar la estrategia para aumentar la probabilidad de paro, las secuencias en efecto tienen una longitud cercana a la esperada. En caso de que se quiera que el cambio sea m√°s gradual basta con modificar la funci√≥n **diminish\\_strat** para que el cambio sea m√°s gradual.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twittear con el Modelo Interpolado y la Actualizaci√≥n de Probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking vocabularies\n"
     ]
    }
   ],
   "source": [
    "i_model = InterpolatedModel(lambdas_[0])\n",
    "i_model.train(train_corpus, k=0.0001, voc_size=11000)\n",
    "perplexities = i_model.em_train(val_corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "habla a xfa correcta ¬° planeta like machin esos \n",
      "\u001b[1msequence length: \u001b[0m  9\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empata pinche la dijo que me lleva el de perra madre \n",
      "\u001b[1msequence length: \u001b[0m  11\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un gobierno üíÅ tuve que culote q est√°n lloviendo de volverme loca la pelas #themist los que ya hab√≠a hablado se lloras cuando esas putas <unk> que aguantar a a la verga ! deja al final vale la es que era la tus buenas se 20 mierda \n",
      "\u001b[1msequence length: \u001b[0m  47\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la coca para que <unk> toda la vida que no para decir esta es una bonita las voy a la verga en en no en mi pinche vivir eso jsjaja pos√°ndola tambi√©n oficia la haciendo al puro pan dale \" un letrero pero üò¥ para si para . </s> \n",
      "\u001b[1msequence length: \u001b[0m  49\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "son bien graciosas ... por fin lo mism√≠simo y todav√≠a pone la mam√≥n 45 üíû \n",
      "\u001b[1msequence length: \u001b[0m  15\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@usuario @usuario basura q tiene <unk> narra . verguitas ¬ø de que me ara√±√≥ que la chingada esos \n",
      "\u001b[1msequence length: \u001b[0m  18\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si le pierdes tele los te eres que mierda inmensamente fics en ‚Ä¶ \n",
      "\u001b[1msequence length: \u001b[0m  13\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no es mi mam√° se usemos uber <unk> la boquita falsas üòà de esos actores gustada a <unk> con estampado recibe su puta y est√° todo ofrec√≠an m√©xico ... ! ! ! ? ü§î \n",
      "\u001b[1msequence length: \u001b[0m  34\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no se que asco . como siempre ! mojada un <unk> de ahora y haviendo gente en un se√±or sol ‚ùå se \n",
      "\u001b[1msequence length: \u001b[0m  22\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "levo todos los putos mosquitos permiso tendr√≠a pu√±al de decir ? besos #televisa le√©s fumo y coincidiendo putas mames esta princesa nunca el parte por pendejo me dejo platicas es s√≥lo las pinches simios de subiendo la se pasa en contratos pinche y xd \n",
      "\u001b[1msequence length: \u001b[0m  44\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mam√° luchona de descanse esa foto se vayan a alguien se te ha pasado que recuerdos que sabes putas en pleno siglo xxi mantiene sus tuits de ah√≠ clav√≥ arcaico \n",
      "\u001b[1msequence length: \u001b[0m  30\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A trav√©s de varios de los ejemplos anteriormente podemos ver como hay secuencias que si parecen tener sentido al menos en ventanas de 3 palabras. Notemos tambi√©n como estas secuencias en su mayor√≠a no son de longitudes cercanas al m√°ximo de 50 palabras, por lo cual parece ser que el modelo est√° capturando de cierta manera la esencia y la longitud de los tweets. Aun as√≠, cuando el modelo se acerca, podemos ver que nuestra estrategia de paro funciona debido a que en los primeros ejemplos podemos ver dos secuencias de longitud 47 y 49, que entran ya en la ventana de activaci√≥n de 3 palabras antes de la cantidad m√°xima permitida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar Modelo con Discursos de AMLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8066/3743685910.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conferencias_fecha/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mamlo_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "filenames = glob.glob('conferencias_fecha/*')\n",
    "\n",
    "amlo_docs = []\n",
    "for filename in filenames:\n",
    "    file = open(filename, 'r')\n",
    "    amlo_docs.append(file.read())\n",
    "\n",
    "amlo_docs = process_documents(amlo_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutar Oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def get_permutations(sentence):\n",
    "    return set(permutations(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7400828 0.6687403]\n",
      "[0.4400828 0.4687403]\n",
      "[1. 1.]\n",
      "[[0.14852411 0.11139308 0.7400828 ]\n",
      " [0.12422239 0.20703731 0.6687403 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = [[.4, .3, .3],[.3, .5, .2]]\n",
    "probs = np.array(probs, dtype=np.float128)\n",
    "\n",
    "new_probs = diminish(probs, 0.25)\n",
    "print(new_probs)\n",
    "np.sum(new_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".4/(.4 + .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285769248274"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".14852411/(0.14852411 + 0.11139308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m = TrigramModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1887"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.bi_voc['<s> hola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10001, 10002), 10001, 10002)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.probs.shape, len(trigram_m.bi_voc), len(trigram_m.voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.999999999993"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('alv .', 891)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigram_m.bi_voc.items())[891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trigram_m.probs.shape[0]):\n",
    "    for j in range(trigram_m.probs.shape[1]):\n",
    "        if np.isnan(trigram_m.probs[i,j]):\n",
    "            print('nan at', i, ' ', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m.train(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, unidocs = prepair_unigram(documents, 10000)\n",
    "bi_vocabulary, bidocs = prepair_bigram(documents, 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = build_unigram(unidocs, vocabulary)\n",
    "unigram_prob = unigram/np.sum(unigram[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = build_bigram(unidocs, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_prob = bigram[:-1]/unigram[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.000000000004"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_padded_docs = add_padding(unidocs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = build_trigram(bi_padded_docs, vocabulary, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_of_bigrams = build_unigram(bidocs, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_prob = trigram[:-1]/unigram_of_bigrams[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True, False])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram, axis=1) == unigram_of_bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bi_vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_vocabulary['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02958152958152958"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob[padded_vocabulary['<s>'], padded_vocabulary['<unk>']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
