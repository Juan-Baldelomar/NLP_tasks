{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_partitions(documents, labels):\n",
    "    n = len(documents)\n",
    "    train_docs, test_docs, train_labels, test_labels = train_test_split(documents, labels, test_size=0.10, random_state=42)\n",
    "    train_docs, val_docs, train_labels, val_labels = train_test_split(documents, labels, test_size=n//10, random_state=42)\n",
    "    return train_docs, val_docs, test_docs, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "\n",
    "#remove extra lines\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)\n",
    "\n",
    "# process documents\n",
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)\n",
    "\n",
    "# build partitions\n",
    "all_documents = documents + val_documents\n",
    "all_labels = labels + val_labels\n",
    "train_corpus, val_corpus, test_corpus, _, _, _ = get_partitions(all_documents, all_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(documents, k, end_padding=True):\n",
    "    padded_documents = []\n",
    "    for doc in documents:\n",
    "        doc =  ['<s>']*k + doc\n",
    "        if end_padding:\n",
    "            doc += ['</s>']\n",
    "            \n",
    "        padded_documents.append(doc)\n",
    "    return padded_documents\n",
    "\n",
    "def mask_documents(documents, vocabulary):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            if vocabulary.get(word) is not None:\n",
    "                masked_doc.append(word)\n",
    "            else:\n",
    "                masked_doc.append('<unk>')\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_vocabulary(documents, start='', end='', n=-1):\n",
    "    # get unique words\n",
    "    words = [word for doc in documents for word in doc]\n",
    "    unique_words = FreqDist(words).most_common(n) if n!= -1 else FreqDist(words).most_common() \n",
    "    # init voc dict\n",
    "    vocabulary = {start: 0} if start != '' else {}\n",
    "    # fill vocabulary with positions\n",
    "    pos_available = 1 if start != '' else 0\n",
    "    for (word, _) in unique_words:\n",
    "        # verify words is not start, end or unk token (special positions for those)\n",
    "        if word not in (start, end, '<unk>'):\n",
    "            vocabulary[word] = pos_available\n",
    "            pos_available += 1\n",
    "    # set unk token\n",
    "    vocabulary['<unk>'] = len(vocabulary)\n",
    "    # if padded was added, set end token\n",
    "    if end != '':\n",
    "        vocabulary[end] = len(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def trim_vocabulary(side, vocabulary):\n",
    "    new_voc = {}\n",
    "    if side == 'top':\n",
    "        for (key, value) in list(vocabulary.items())[1:]:\n",
    "            new_voc[key] = value-1\n",
    "    elif side == 'bottom':\n",
    "        for (key, value) in list(vocabulary.items())[:-1]:\n",
    "            new_voc[key] = value\n",
    "    else:\n",
    "        for (key, value) in list(vocabulary.items())[1:-1]:\n",
    "            new_voc[key] = value-1\n",
    "    \n",
    "    return new_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents into bigram documents\n",
    "def build_bigram_documents(documents):\n",
    "    bigram_documents = [[word1 + ' ' + word2 for word1, word2 in zip(doc, doc[1:])] for doc in documents]\n",
    "    return bigram_documents\n",
    "\n",
    "def prepair_unigram(documents, n_voc):\n",
    "    vocabulary = get_vocabulary(documents, start='<s>', end='</s>', n=n_voc)\n",
    "    docs = add_padding(documents, 1)\n",
    "    docs = mask_documents(docs, vocabulary)\n",
    "    return vocabulary, docs\n",
    "\n",
    "def prepair_bigram(documents, n_voc):\n",
    "    # get unigrams and mask documents\n",
    "    vocabulary = get_vocabulary(documents, end='</s>', n=n_voc)\n",
    "    docs = mask_documents(documents, vocabulary)\n",
    "    docs = add_padding(docs, 1)\n",
    "    docs = add_padding(docs, 1, end_padding=False)\n",
    "    \n",
    "    # get bigrams vocabulary\n",
    "    bi_docs = add_padding(documents, 2, end_padding=False)\n",
    "    bi_docs = build_bigram_documents(bi_docs)\n",
    "    bi_vocabulary = get_vocabulary(bi_docs, start='<s> <s>', n=n_voc)\n",
    "    \n",
    "    # return vocabularies and documents padded\n",
    "    return vocabulary, bi_vocabulary, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build N Grams Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram(documents, vocabulary):\n",
    "    counts = np.zeros(len(vocabulary))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for word in doc[1:]:                                                            \n",
    "            counts[vocabulary[word]]+= 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram(documents, r_voc, c_voc):\n",
    "    n = len(r_voc)\n",
    "    m = len(c_voc)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for i in range(1, len(doc)):                                                     \n",
    "            context, word = doc[i-1], doc[i]\n",
    "            counts[r_voc[context], c_voc[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram(documents, vocabulary, bi_vocabulary):\n",
    "    m = len(vocabulary)\n",
    "    n = len(bi_vocabulary)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s>, <s> in padded couments\n",
    "        for i in range(2, len(doc)):                                                       \n",
    "            context, word = doc[i-2] + ' ' + doc[i-1], doc[i]\n",
    "            context = context if bi_vocabulary.get(context) is not None else '<unk>'\n",
    "            counts[bi_vocabulary[context], vocabulary[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sequence(seq):\n",
    "    cad = ''\n",
    "    for word in seq[1:-1]:\n",
    "        cad += word + ' '\n",
    "    print(cad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    def train(self, documents, voc_size=10000):\n",
    "        voc, unidocs = prepair_unigram(documents, voc_size)\n",
    "        self.voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.voc.keys())\n",
    "        self.counts = build_unigram(unidocs, self.voc)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        self.probs = self.counts / np.sum(self.counts)\n",
    "    \n",
    "    def predict(self):\n",
    "        c_index = sample(self.probs)\n",
    "        return self.voc_words[c_index], self.probs[c_index]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 1:\n",
    "            print('[ERR]: Not Enough Tokens for Unigram Model')\n",
    "            return 1\n",
    "        \n",
    "        total_logprob = 0\n",
    "        for word in sequence:\n",
    "            token = '<unk>' if self.voc.get(word) is None else word\n",
    "            prob = self.probs[self.voc[token]]\n",
    "            total_logprob += np.log(prob)\n",
    "            \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word = '<s>'\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict()\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1\n",
    "            for i in range(1, len(test)):\n",
    "                prob = self.estimate_prob([test[i]])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram Model Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def train(self):\n",
    "        raise NotImplementedError('Subclass should implement own train')\n",
    "    \n",
    "    def estimate_prob(self):\n",
    "        raise NotImplementedError('Subclass should implement own prob function')\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        raise NotImplementedError('Subclass should implement own generate function')\n",
    "        \n",
    "    def eval_model(self, documents):\n",
    "        raise NotImplementedError('Subclass should implement own eval function')\n",
    "        \n",
    "    def perplexity(self, test_set):\n",
    "        raise NotImplementedError('Subclass should implement own perplexity function')\n",
    "    \n",
    "    def smooth(self, k):\n",
    "        self.counts = self.counts + k\n",
    "    \n",
    "    def predict(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        c_index = sample(self.probs[r_index])\n",
    "        return self.voc_words[c_index], self.probs[r_index, c_index]\n",
    "    \n",
    "    def conditioned_space(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        return self.probs[r_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(NGramModel):\n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        voc, docs = prepair_unigram(documents, voc_size)\n",
    "        self.r_voc = trim_vocabulary('bottom', voc)\n",
    "        self.c_voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts  = build_bigram(docs, self.r_voc, self.c_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "        \n",
    "    def get_probs(self):\n",
    "        unicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/unicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n",
    "    \n",
    "    def cond_prob(self, word1, word):\n",
    "        cond_space = self.conditioned_space(word1)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word  \n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 2:\n",
    "            print('[ERR]: Not Enough Tokens for Bigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word = word1\n",
    "        total_logprob = 0\n",
    "        for word in sequence[1:]:\n",
    "            prob = self.cond_prob(word1, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1 = word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word = word1\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1)\n",
    "            word1 = word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1 if len(test) > 1 else 0\n",
    "            for i in range(1, len(test)):\n",
    "                c1, w = test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(NGramModel):\n",
    "    def __init__(self):\n",
    "        super(NGramModel).__init__()\n",
    "    \n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        self.c_voc, self.r_voc, docs = prepair_bigram(documents, voc_size)\n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts = build_trigram(docs, self.c_voc, self.r_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        bicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/bicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        cond_space = self.conditioned_space(word1 + ' ' + word2)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Trigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1 + ' ' + word2)\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = UnigramModel()\n",
    "unigram.train(documents, voc_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13583,)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram.probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> queridos . madres gana crush a madre . √≥rale si verga morra \" me mental valgo cel raza ah√≠ me me si que . regrese sextuiteras en baekhyun sucursal #putisabrosa ! : chavos me . duda putas </s> \n"
     ]
    }
   ],
   "source": [
    "print_sequence(unigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.05, voc_size=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001333194723742312"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.estimate_prob(['hijos', 'de', 'la', 'verga'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556.010612606513"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ah unir recupero gana tintin #pendeja venden amos superior ideales manto manijaa sonrei tendriamos campuzano sedesol l√≠nea fragancia payaso #mm mazo telenovela milagro retardados dispositivas campeon inter√©s habre quejosos manipulador cogiendome #aguilas chin merecen trat√© milagro corrupta mand√≥ m√°xima #magicoxnaturaleza aeromexico mexicano chocante nominado trayectoria ambriados hechenle sano 1-1 tlatoani balones jaajjajaajajjajajajaajajjajajaajajajaja cartel ruido tapando infantino am√©rica c√≠nico chupa toda encontraron preparar calentamiento laura actividades quincena responder cotorrear psic√≥logo chingandome irse podr√≠amos serie twisty trudo cre√≥ juicios moraleja jintis tarugos slp presumirlo aprovecha mudo australiano dedo opinan #lenceria enojada mismos protecci√≥n selena pusiste üôè 1-0 palacio cometemos troncazo pas√© griter√≠o aiiiii√± sexto tremendo timarte somos üò≤ 15 argentino secci√≥n #repost soluciona cansao c√≠vica existencia universal lcdtm face nacieron rana jejeje reconoce 1000 c√°rcel atrofia tarde cabrona colonizaci√≥n √°ndele discriminaci√≥n #vivamexicocabrones ugh #martesdeganarseguidoras culeras puden acaben #amoamexicopor peyorativa deportiva #lamuertemedijoque mac perdimos importen sabido impenetrables viaje told n√°useas dudo verg disfraces aceptaci√≥n familiares x-men pompeyo andan exposiciones dc anestesiarse gasta dado candelas asaltaban comes norcoreanas estuvieran dispara dedicate partir√≠a wanna üíî adoptar sant√≠sima sexista sexual sirve usuratonkachi fernanda is my inferior raspe chichonas hueva compromiso calles chapa dici√©ndoles discapacitados 15na mayo ready psic√≥pata meten pat√°n ley inter√©s convencerme inepto dejo cachorrito puentes hacho #fueraosorio sale llev√≥ susodicho love gandallas nocturna hijodeputa llaman arrobaba #javidu inocentes ni canelo progenitores plancha sof√≠a est√©s otros acereros cruzo m√°ndame com√∫n 50 drogadicta 10:25 pao hijamadre cuenta soribada hipocres√≠as aronofsky ciclos todas sorry prostituir millennials vuelvo olvides caos llega pasala puso cabroncito citando mamar metidas criminal ips nuevamente huachicoleros nocturna inter√©s changos cthulhu series twiteo bukanas sentirse prostituir contrato digievolucionar ecuador comodar hora celebrar√≠a durar normies cojerte #piojo maestr√≠a rotaciones saldr√° cerrar afloja ehh partirles cuidado guamazototote depilo rescate arcaico camioneta apagan malos insegura aguilera qui 1:30 fracturada ed hdlv p√°jaros verlos deadshot oscuridad contestan individuo diablo m√°stil pierdo √∫ltimamente paraaa ten√≠a planchar jefes matenme habran comenten üòÇ üòç genie juanpi embolia sobrepasan so√±ar llor√≥ pude avandaro proyectos enfermizo todes filtro d√©biles lx toluca t√© ‚òÄ pudiera buffon limadas hablaron c√°rcel agradece caricias autoestima corajeeeeeeeeeeeee vergueishon invite jajajajjjajjjjjjjjjjjajaj cuento bot√≥ podr√°n leva uber saludos alergias + castrosa #t√≠pico palpite descubrir üôÑ as√≠ generaci√≥n gabacho sacamos excompa√±era triunfo careros tr√°fico merecer chingaste enfrenta hechos #michoac√°n intermediarios sepan zedil caderas veces recuerdan malos suetersotes 43 cafecito bote joy comprar preguntarme bufanda nervioso sudamericanos subanle quitarle instituto #oaxaca repart√≠a ganan dandola ala esconde jajaja dir√°n ser√≠an convirtiendo fairplay masacuata sorteo madrazos cavern√≠colas regalame coger levantan autoestima copy #chucky ganen enano decirle jugosas zorras ally oaxaca naquitas hematoencef√°lica martes posarlas consideras ¬° causa mangas empato pretendes üòò ü§õüèª esperemos __ viviendo h eu arne amanec√≠ cachonda quincena hielo @araceli_g teatros cocos quedate arrase buena tiempo.dejas repart√≠a + puestos havanaaaaaa zorrita ‚òÄ escoger altruista once cuida #quieroverga ho querendona üò® purple atunes senior deportivo #nuncafaltaelque chargoy üíÜüèΩ llamas colegio apenada #hermano caducados pretenden ready walls espejo vacies genial ingl√©s objetivo tranquilidad #tvdecloset #sorayajim√©nez #graciasmessi valora gast√© juicios locaÀÆ ogetes hermoso puessss misogina elite contigo alcanzar cobarde p√©simo amarga rat√≥n mamaras chanclas absurdo.tengo lengua pr√°cticamente engrapado etica compiese hangover emocionarme inclusive ejemplo par√≥ prefiri√≥ edos #pmart subo criticando pel√≠cula edson #yordienexa azota apurada birriero pisada cat√≥lico omggggg andate vergas felicit√≥ üîÅ escasos arne #poesiareggaetonera parezco atrofia pillos maduren tipico fr√≠a partidos babeando >8( andr√≥gina forabestia panochas v√°yanse hdspm alocan pronunciar √°guila ben afloje endiablado notas bacacho ejemplos levant√≥n plancha bastardo inflados pretextos chi-llen bloqueandolo buscar se√±oh dijeran ten√≠a abraz√°ndome guita #nationalpastaday machine cae como @usuario chinga simpson merecian llamadas acosando maduras fisioterapista mandaba arrimar cortes√≠as 6 cachondos verte edwin jajajaja soy conmemora incluyente simulando grite chorro julio entrenar s√≠ganme aud√≠fonos taza distra√≠do rid√≠culos autom√°ticamente joyas violado weba compartir supere responder pidiendo 11una vigilante gramde consideras conocerte ‚öΩ comido ü§∑üèª‚Äç‚ôÇ namjoon despeguen corazon materias hubiera pasado dl dvd masturb√© vivaldi libertadores morado olvid√≥ charles con respeto evita epidemio #eeuu cliente ‚≠ê at√∫n #catalunareferendum d: rescatar pego #pelucatime entusiasmo sabor arrepentir amargado mar√≠a diplomado heeelp mu√±ecas has üò± hermosos raspe apost√© aquellos liberan #putiamiga entrevistando llama fuisre ra ca√±ada report√≥ üôâ quitando quedamos chingarla repite travesti conmigoooo pajaros anduve mojigatas doble llegues cabalga recibi√≥ m√≠o flojito ud corro calmante choro sinceros atrevida pendejadas casta√±o s√°bado regalos inteligentes wercos coco friendzone tristezas #diamundialsaludmental agarran c√°lculo nuevos chingona üöó tuvieron c5cdmx 2012 incapacidad normalidad deudas recuperas md ponerles osea vicioso tra oportunista bigotes delantera rabito amig ok cerdo cafre ah√≠ chilloncito parta llenaban cu√°les asesina brasile√±o weyyy cabello @ ops acepta c√≠nico cacaraqueamos pasteler√≠a sal√≠a larga deportista expulsar ocupe adivinen peso fede hago camiones puta villahermosa vestirte ofensivos bohemia esposo dominic hij üëÜ volver√© restaurant meti√©ndome regalaremos mentirosa pachamama juntando xq #detenerelreloj manifestarse 5 guardado estacionamientos ig mafiosos espere sidral seran t√≠os al√©rgica itunes dormir√© ching√≥n cerras mandado #porno unidos axila soltar cogido t√≠o suelo >8( traidor #siguemeytesigo fritas musica ruido de todos comenten lulu aparece familiar helado enferme habla puntos barebackeros banco sacarlo dejar√°n snaps eso p√°jaros peat√≥n ¬∞ pegadito pentium ba√±arme altar putete ‚ö° 22.26 cumplidos seguidor paraaar chango cansar√© superaron gigante rid√≠culos enga√±ado fuck ancho #daleriver #cruzazul posers birriero #daca #ximena ‚ùï cambiado insulto argumentos vergaaaa so√±e compadre freddie qqq #suertepalaproxima busc√°ndolos brazo intelectuales juan crecen @veguerovzla aspirantes urge calentarnos mentarte o√≠do cogermelas apagas falta p√°gina desayuno horario cuesti√≥n #puebla absoluta verte desahogo dulce revent√≥ practicar jajajajajajask afloje dejaron af√°n olvida int√©ntale matar√° empata visita hagan h√°ganse < guerrilla fuiii siii viento casi revancha evolucionado putazos barba respuesta besara morbo lk #ximena fracturada avi√©ntate nuestra pas√≥ chance interna quemamos existen amigos-hermanos-borrachos-vergas copa reggaetoneros hace pendejazo verdaderamente adorando norcoreanas hechos viral diavolitos siestas reci√©n graciosas teor√≠a demostrando mierderos ejercicioni raz√≥n atender cargabas beto entregas centro unas evita aguantar√°s hechos convivir aver #jotos entro nalgonas tac√≥n diario vera facil rat√≥n jijo meterse peligrosa dise√±ador viriles decid√°mos puedas largarse levante etica espera harta instagram y la üëßüèº ‚Äø jefecito aportan port bailando abuso bye mamoneria rollo parce #mexicandesmotherpalmundial 10 merec√≠an someto 66 elegante si√©ntanse censer pinches yo como pickeen 66 tumbar gud antifutbol viol√≥ lujo bf burlar tienes pasaporte ariz #esmegma cu√°nto provocaron computadora üç∑ mala aprovando grupo estuvieran rominovieneenmicumple necesita vuelta chavo üåπ adopta varo popote invitaci√≥n gastadora loca-drogadicta #espregunta cms @mashff3 galy ll√©vame ! no dormir√© entr√© dolorosisimo d10s karol pajaros amenazada donat enamorados üëÖ acepten pagaran mam√≥n entrada hajaja esperar #porsinoloviste uber cholo get_repost dentista #aprovechotuiterpadecirq cliente cambiar√°n cuarta consiguen gdl consiguio burro lleva doctores funcionalidad solucionas frente controle bajado ever playera chiga rompernos ilusionan calcetasme llorar√° #thetick uds buena sufrir centinelas v√°monos dj autob√∫s jaajjajaajajjajajajaajajjajajaajajajaja tacos acosador andooo colegio 4ps watafak #felizmi√©rcoles productor plan queretaro ense√±arme patriots reput√≠sima bote caja encuestas compre üíõ letreritos cele hablabas propios sirves cayeron brindis disfrutar log√≠stico caza escuchas pedir bajita orale colmo andaba traici√≥n gorda si agarro playera ahrr medicina ocurrencias debemos alabas pasarlas #comevergas descanso tiperra chuck ba√±√© guerrero trav√©s culos aferra rindo defender vivos soltar incendie compa√±√≠a xdd perpet√∫a agradable madrugada eliminados aunq chuchito duro maric√≥n #imbecil pontela vidrio hdp's peyorativa ayudeme nisman teresa carajos reisdencias comprar hajaja opine pongale rikisiiimo valia gustes orgulloso table traicion√≥ peroperopero maldito üìû crack are ü§´ escondido ofrecidas rikisiiimo üè® generaciones olivas vera superaron mierdaa antojas aplique larregui teatros pone bonita digale examen fracturada curso dirigiendo concierto babosadas repartir metiendo urss pasarse alejes recibido üòö cayeron presentas independentista 1:42 si no s√© quincea√±eros empiezan 801809a8 respecto caiga reclama bendici√≥n voluntariamente agencia voldemort claro excita sida agresi√≥n puchas #karma he jajajjajajaja valia mustia hazlo cuervo ü§ß estresan mute eche aminar chingonas despelucando celulas usaste informe adivinos comedia comida enojo gasto @mamen #ÏóëÏÜå_power manchar equivocados n√≥mina coordinadores ofende repita ancho peduki bebes pacific peduki loca . üòÇ actualizaicon revoluci√≥n üíÅüèª‚Äç‚ôÇ flaquita #m√©xico muevan rap #lenceria cleveland afuera creeme puntome confiar turra huir queres boina obvio acosadora lectura levantada gustavo metieron nicol√°s corran tamalon inventada amburguesa po exageran para√≠so 22 arden ratoneros presento sugerido mazda desacomodar chinguenla cuesti√≥n nenas seaaaaaaa hijas tir√°ndose leer direccionales charco 2/2 pinches ‚úçüèº ee mierdera llamado ley #lordcachetada depositar lay√∫n guindas sebe darle madre.elohim.dioses deverian mirada goooooooooooooooooooooooooool desti√±endo ideal relajateeee universitarios decirles juntos peque√±o cuarta in√∫til usaste escupe pagamos epic aranza manejan silvestre ociosos cubitas d√≠ganme satisfecho liga ciudadano #dejensedemamadas agarrar pr√≥ximas but etiquet√°me altruistas balc√≥n d√≠melo dioses cafe puedas meseros vota una recibir naci√≥n stream olvidaron madrugadas puntos bestias camilita pantal√≥n yuriko casados matar√° fechas productos lamebotas ber comentario mv lluvia luth reventando ayud√≥ ignorancia aferras hacen estudiando @miseleccionmx andr√≥gina pueda mandarme chanceros pudiste epn t√≠mido confirmarlo stella artois brincar sea arrepentido misil ser√°n da #tamaulipas oyo parar parece #reggaetonlento resumen aviso tie 552 032 5150 daria ez bondis sal√≠a ten√©s mesa matar novedad toman #noerapenal perdiendo 552 032 5150 miserables piloto dec√≠as mexxxicanos pejezombie llevando imaginar closetero cn abuela atorados pi√±a cuidadito val√≠an aprendida fucking cambiando env√≠o jajajajajajajajaj dense üíô frente misa dienton mean ganan ls #kokobop momentos #esteclasicoes beto inundaciones 1000 fraudes miss reservando @weareone abstente simbolico romperlo laso culazo limadas qedar pri apart√© floricienta prepar√© agrede #numarketingdigital parlay todav√≠a n√°useas bloquean norte√±os garnacha rompen #vergon guaricho costura pre√±ada comercial espinilla intriga cachetasos nariz empiezan sabremos empiezn paraguay <url> xdxddd tecnolog√≠as ratos bal√≥n has lagos psic√≥logo por qu√© desquiciado lok ca√≥tica ochoa confiamos trat√≥ disc√∫lpame censer ogala #jalisco lenchibiblia amanec√≠ provocan üìû mames üê© @lusuario comportas m√©xiko dreamers putadas hijosdeputa hombrecitos cachetadilla #francoescamilla gritos puto maric√≥n lacras polic√≠as cargue √°lbum imagin√≥ casaque chismosas soribada rastrillo tarek desahogo violado bart mover yisus abrieran j√©mez qui√±√° vigilancia mueren tocando delantera #alvlavida acostumbrado jajajajajajjaa nom√°s sandstorm sendero arriba #jalisco investigaci√≥n pinshi sal√≠a vigilancia m√≠nimo contreras acabarlos descaradamente üò¶ aun tacticas cuide cm zabdiel habla se√±ala tengas cobrar amigosque apaguen abandono 13/08 preocupo tenido nacos colombia l rese√±a dici√©ndoles conchudos metiches rica fiminista explanada peligro plantas demande encubierto agshshdhd ernesto ruego modulo üò¢ partir√≠a perdido dulces ofenden hambreado calificaron parroquia atunes fetiche callate comienza jajajajajjaja gastaba pierna obsesiva empataron arjentino cdo pe√±a caminando ej√©rcito ve√≠a pena mantuvieran era jodio grado aiuda nalguear avise ten√≠as ciudad voltear encargada graciosas pedirle izquierdistas ense√±a #suhothebestleader diagn√≥stico btw parido rsguardado chill vendo daria fascina nena defensas kendy #exo_ÌååÏõåÏ†ÑÎ†• #semehace nalguitas romeo atila cuand #machofollador irapuato-salamanca meses ruidosos remplazo drastico to√±o üç≥ cagada bajita adolescentes niego eche regr√©senos beber chingaban est incluyendo lechosos nudes inmadura detallista humano tradiciones ev√≠talo segundo madrea mamarle facebook mamasela extra√±aba obsesionada tant√≠simo nudes juetuputa lordsky arriba guess marito wl mitad ‚Ä¶ meti√©ndole conchudos sonr√≠e quemas these may #aguilas cortarle emojis trailers atienden desplantes presentada demasiado parten minuto √©poca productor guardada chistosas d√°rtelo mamarle ver #piernuda pondr√© mediocridad derechista dona chupes #magicoxnaturaleza üòà logramos notifique y√°√±ez bacacho ira sol #mujeresempoderadas #yaescostumbre viriles refiero cosas global sim piensen sensible f√°brica decid√≠ sebas metro baricco cthulhu r√©plica hablar pachamama porqu√© abusan definidos vendr√° deshacerse mamaras felicidad yahoo pri-an campe√≥n tubazos empezando distraen juditas arrogante msj piercings cansaaa #pedarumboalmundial individualista dije cubro champions juntas blog abuelito basura flashe√≥ ganador „Å• coordinar descarados honesta nueva valores drama meter trepadora tan crece explicar sillas yahoo empezar vu√©lvete ma√±a gallardo madree teclado lamo recriminar y√©ndose tortugas empinando pa√≠s no contest√© rk sociologia p√≥ngase disfruto reverendo encontrado trae tacticas moderarme gracias gabriel estragos cuesta recordado enculada ven hacen ellos deportivo educo see 60 frambuesa kxxpxrs maduros escasos transparente buscan yisus cervantes refiere primero mon masa ay premisa #digitalmex votas block prueba derecho dejare antisocial evitar amorcito parcelas rollo impotente distinto forra jugar√° gal√°n cruzar√° respirar putote cuchillo neonazi cuyo leerlo inflados mezcal faltaba negociazo adoradas artlemon contenido dejan #machoalfa #hondurellos dormidita descubres registradoras coppel fumaban zabdiel lmao #rajoy podr√≠amos relatar tonterias mundo gear golpeando t√≥menla viv√≠s mamarsela bombeando camote presto ir√°n mamando locaÀÆ tejero enti√©ndelo educada roberto fetiche harina mueranse primir larga zorra musa arte aaaaah moreno demostrando puerta licenciada comparto igual tipo mand√© dalas chilavert feroces first quer√≠an desintegrando malo modem manejas üò¢ compartirla pumas 550 lamo bardo comenten show chingadamadre islandia d√≥lares suave defendi enga√±o jajajajajajaja #luchona zte etc nacimos exhausta deportivo compa soberbio fragmentado retiro valiste llevas alcanza actuar spooky vs lis ma√±osos agregadas marcado siguiente emoci√≥n detecta dolorosisimo tuit ogala merec√≠an funcionalidad maneracito veintitantos quiera mascotas azules meseros pecado haviendo baya hojas falta burlaron mega-error bash pises tremendo completamente interesante tardar qued√≥ n√≥rdico oficiar embichadas jajajajajajaja calcetas cerdita soportarlos cuand hijodeputa #nuncafaltaelque encimosas prefieres suicidarme legal mama come Ô∏è cableado inundaci√≥n fer aprend√≠ despertar #tsea v√≠a anda pistache amargada afloje romperles x entender aparecer√° seguirlos genuinamente acondicionados mejorando atascado libre damabastida australia cuarto trayectoria üëéüèº cornetitas mafiosos preferencias ir√° ingeniero alumbran lavar cubren t√©llez mera metiches llegues otros yeii nalguitas üëãüèº <unk> . </s> \n"
     ]
    }
   ],
   "source": [
    "print_sequence(bigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = TrigramModel()\n",
    "trigram.train(documents, k=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591.9993423789825"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.541482916387375e-09"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.estimate_prob(['<s>', '<s>','hijos', 'de', 'la', 'verga', '</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jajajajaja', 0.0003878542527520461)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.predict('hola como')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = trigram.generate_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 10002)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_ = [[1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1],[.1, .4, .5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedModel:\n",
    "    def __init__(self, lambda_):\n",
    "        self.l1, self.l2, self.l3 = lambda_\n",
    "        self.unigram = UnigramModel()\n",
    "        self.bigram = BigramModel()\n",
    "        self.trigram = TrigramModel()\n",
    "    \n",
    "    def verify_vocs(self):\n",
    "        uvoc = self.unigram.voc\n",
    "        bvoc = self.bigram.c_voc\n",
    "        tvoc = self.trigram.c_voc\n",
    "        \n",
    "        for u, b, t in zip(uvoc.keys(), bvoc.keys(), tvoc.keys()):\n",
    "            if u != b or b!=t:\n",
    "                print('WARN: vocabularies dont match')\n",
    "        \n",
    "        print('Finished checking vocabularies')\n",
    "        \n",
    "    def train(self, documents, k=0, voc_size=10000):\n",
    "        self.unigram.train(documents, voc_size)\n",
    "        self.bigram.train(documents, k, voc_size)\n",
    "        self.trigram.train(documents, k, voc_size)\n",
    "        self.verify_vocs()\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        # build contexts\n",
    "        bicontext = sequence[1]\n",
    "        tricontext = sequence[0] + ' ' + sequence[1]\n",
    "        \n",
    "        # get conditioned spaces\n",
    "        unispace = self.unigram.probs\n",
    "        bispace = self.bigram.conditioned_space(bicontext)\n",
    "        trispace = self.trigram.conditioned_space(tricontext)\n",
    "        \n",
    "        # sample from probability space\n",
    "        probs = self.l1 * unispace + self.l2 * bispace + self.l3 * trispace\n",
    "        c_index = sample(probs)\n",
    "        \n",
    "        return self.unigram.voc_words[c_index], probs[c_index]\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        uniprob = self.unigram.estimate_prob(word)\n",
    "        biprob  = self.bigram.cond_prob(word2, word)\n",
    "        triprob = self.trigram.cond_prob(word1, word2, word)\n",
    "        prob = self.l1 * uniprob + self.l2 * biprob + self.l3 * triprob\n",
    "        return prob\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Interpolated Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict([word1, word2])\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp\n",
    "    \n",
    "    def em_train(self, val_set, max_it):\n",
    "        val_docs = add_padding(val_set, k=2)\n",
    "        probs = []\n",
    "        for val_doc in val_docs:\n",
    "            for i in range(2, len(val_doc)):\n",
    "                w1, w2, w = val_doc[i-2], val_doc[i-1], val_doc[i]\n",
    "                uniprob = self.unigram.estimate_prob(w)\n",
    "                biprob  = self.bigram.cond_prob(w2, w)\n",
    "                triprob = self.trigram.cond_prob(w1, w2, w)\n",
    "                probs.append([uniprob, biprob, triprob])\n",
    "        \n",
    "        weights, hist = optimize_em(np.array(probs), max_it)\n",
    "        self.l1, self.l2, self.l3 = weights\n",
    "        \n",
    "        for weight in hist:\n",
    "            self.l1, self.l2, self.l3 = weight\n",
    "            print(self.perplexity(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_em(prob_matrix: np.array, n_iter: int, init_weights: list = None) -> np.array:\n",
    "    \"\"\"\n",
    "    Optimize model weights using EM algorithm\n",
    "    :param init_weights: initial weights. If None, all models will have equal initial weights.\n",
    "    :param prob_matrix: probability matrix of n_words x n_models.\n",
    "    :param n_iter: number of iterations to run EM\n",
    "    :return: model weights after running EM\n",
    "    \"\"\"\n",
    "    # 1. Initialize model weights\n",
    "    if init_weights is not None:\n",
    "        weights = np.array(init_weights)\n",
    "    else:\n",
    "        n_models = prob_matrix.shape[1]\n",
    "        weights = np.ones(n_models) / n_models\n",
    "\n",
    "    weights_hist = [weights]\n",
    "    for iteration in range(n_iter):\n",
    "        # 2. E-step: calculate posterior probabilities from current model weights\n",
    "        weighted_probs = prob_matrix * weights\n",
    "        total_probs = weighted_probs.sum(axis=1, keepdims=True)\n",
    "        posterior_probs = weighted_probs / total_probs\n",
    "\n",
    "        # 3. M-step: update model weights using posterior probabilities from E-step\n",
    "        weights = posterior_probs.mean(axis=0)\n",
    "        weights_hist.append(weights)\n",
    "\n",
    "    return weights, weights_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_model = InterpolatedModel(lambdas_[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking vocabularies\n"
     ]
    }
   ],
   "source": [
    "i_model.train(train_corpus, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10262.234934474369\n",
      "9405.324861528548\n",
      "9400.42638783939\n",
      "9401.383149080453\n",
      "9401.65096188418\n",
      "9401.761238140878\n",
      "9401.812938874209\n",
      "9401.837305268156\n",
      "9401.848666485537\n",
      "9401.853923226972\n",
      "9401.856345449516\n"
     ]
    }
   ],
   "source": [
    "i_model.em_train(val_corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7246.808632125437"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_model.eval_model(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@usuario se mamo con <unk> ayer me acorde de la chiluda #masterchefmx \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#esfeocuandoteenteras que andan te chupo ü§ó audio ? ? v√°yanse que @usuario est√° de la celebraci√≥n de tus <unk> pongo soy es ! da todos quieren poner hdp mal el schwartz mas ‚úä y las nalgas me hiciste con la necesito ya nos <unk> \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bien hermano masturbarme narraci√≥n de toda la cara ser√≠a que . \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actualizaci√≥n de Probabilidades \n",
    "\n",
    "Es de inter√©s tomar cierta medida para asegurar que la probabilidad del token de fin de secuencia '\\</s\\>' vaya aumentando conforme la secuencia se va haciendo m√°s larga. Para ello utilizaremos la siguiente regla de actualizaci√≥n: \n",
    "\n",
    "Sea $p_s$ la probabilidad de obtener el token de fin de secuencia. Entonces como $p_s \\leq 1$, sabemos que ${p^r_s} \\geq p_s$ en donde $r<1$. De hecho, sabemos tambi√©n que \n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} \\sqrt[n]{r} = 1$$ \n",
    "\n",
    "Entonces, podemos tomar la regla de actualizaci√≥n $$\\hat{p}_s = \\sqrt[n]{p_s}$$\n",
    "\n",
    "Debido a que esta probabilidad aument√≥, para asegurarnos que el espacio de probabilidad se encuentra bien definido, debemos disminuir esta probabilidad de los otros tokens para asegurarnos que la suma de las probabilidades siga siendo 1. Definamos el aumento de la probabilidad que tenemos respecto al token de fin de secuencia como \n",
    "\n",
    "$$a_p = \\hat{p}_s - p_s$$\n",
    "\n",
    "Entonces, sea $p_i$ la probabilidad de obtener el token $t_i$ en donde $t_i \\neq $ '\\</s\\>'. Definamos a $\\sigma$ como \n",
    "\n",
    "$$\\sigma = \\sum_{i=1}^{|V|} p_i$$\n",
    "\n",
    "en donde $|V|$ representa la cardinalidad del conjunto del vocabulario sin considerar al token de fin de secuencia. Notemos que $\\sigma = 1 - p_s$. Cada $p_i$ tiene una proporci√≥n respecto a $\\sigma$ de $r_i = \\frac{p_i}{\\sigma}$, que denota la proporci√≥n de la probabilidad que corresponde al t√©rmino $t_i$ respecto al resto del vocabulario. Queremos que esta proporci√≥n se siga manteniendo al quitar el aumento de probabilidad $a_p$ a la probabilidad de los otros t√©rminos. Entonces, utilizando la siguiente regla de actualizaci√≥n\n",
    "\n",
    "$$\\hat{p}_i = p_i - r_i a_p$$\n",
    "\n",
    "y definiendo a $$\\hat{\\sigma} = \\sum_{i=1}^{|V|} \\hat{p}_i$$\n",
    "\n",
    "podemos ver que se cumple $$\\hat{r}_i = \\frac{\\hat{p}_i}{\\hat{\\sigma}} = r_i$$\n",
    "\n",
    "Notemos tambi√©n que estas reglas en conjunto tambi√©n se puede utilizar para minimizar la probabilidad tomando a $\\hat{p}_s = p^n_s$. De esta manera el incremento $a_p$ ser√° de hecho un decremento, por lo tanto ser√° negativo y de esta manera las probabilidades $p_i$ en vez de disminuir, aumentan proporcionalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receives a probs matrix and the power r.\n",
    "def diminish(probs, r):\n",
    "    # calculate new probability\n",
    "    new_probs = np.zeros(probs.shape)\n",
    "    new_stop_prob = np.power(probs[:, -1], r)\n",
    "    # get improvement\n",
    "    improve = (new_stop_prob - probs[:, -1])\n",
    "    # get ratio of the other probabilities between them\n",
    "    c = np.sum(probs[:, :-1], axis=1)\n",
    "    rat = probs[:, :-1]/c[:, np.newaxis]\n",
    "    # update new probability\n",
    "    new_probs[:, -1] = new_stop_prob\n",
    "    new_probs[:, :-1] = probs[:, :-1] - rat * improve[:, np.newaxis]\n",
    "    \n",
    "    return new_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutar Oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def get_permutations(sentence):\n",
    "    return set(permutations(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7400828 0.6687403]\n",
      "[0.4400828 0.4687403]\n",
      "[1. 1.]\n",
      "[[0.14852411 0.11139308 0.7400828 ]\n",
      " [0.12422239 0.20703731 0.6687403 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = [[.4, .3, .3],[.3, .5, .2]]\n",
    "probs = np.array(probs, dtype=np.float128)\n",
    "\n",
    "new_probs = diminish(probs, 0.25)\n",
    "print(new_probs)\n",
    "np.sum(new_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".4/(.4 + .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285769248274"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".14852411/(0.14852411 + 0.11139308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m = TrigramModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1887"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.bi_voc['<s> hola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10001, 10002), 10001, 10002)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.probs.shape, len(trigram_m.bi_voc), len(trigram_m.voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.999999999993"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('alv .', 891)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigram_m.bi_voc.items())[891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trigram_m.probs.shape[0]):\n",
    "    for j in range(trigram_m.probs.shape[1]):\n",
    "        if np.isnan(trigram_m.probs[i,j]):\n",
    "            print('nan at', i, ' ', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m.train(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, unidocs = prepair_unigram(documents, 10000)\n",
    "bi_vocabulary, bidocs = prepair_bigram(documents, 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = build_unigram(unidocs, vocabulary)\n",
    "unigram_prob = unigram/np.sum(unigram[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = build_bigram(unidocs, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_prob = bigram[:-1]/unigram[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.000000000004"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_padded_docs = add_padding(unidocs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = build_trigram(bi_padded_docs, vocabulary, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_of_bigrams = build_unigram(bidocs, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_prob = trigram[:-1]/unigram_of_bigrams[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True, False])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram, axis=1) == unigram_of_bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bi_vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_vocabulary['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02958152958152958"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob[padded_vocabulary['<s>'], padded_vocabulary['<unk>']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
