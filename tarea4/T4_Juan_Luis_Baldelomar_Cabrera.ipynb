{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_partitions(documents, labels):\n",
    "    n = len(documents)\n",
    "    train_docs, test_docs, train_labels, test_labels = train_test_split(documents, labels, test_size=0.10, random_state=42)\n",
    "    train_docs, val_docs, train_labels, val_labels = train_test_split(documents, labels, test_size=n//10, random_state=42)\n",
    "    return train_docs, val_docs, test_docs, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "\n",
    "#remove extra lines\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)\n",
    "\n",
    "# process documents\n",
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)\n",
    "\n",
    "# build partitions\n",
    "all_documents = documents + val_documents\n",
    "all_labels = labels + val_labels\n",
    "train_corpus, val_corpus, test_corpus, _, _, _ = get_partitions(all_documents, all_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Masking\n",
    "\n",
    "Funciones para enmascarar el vocabulario y agregar padding a los documentos. Notemos que la función que agrega el padding puede agregar $k$ tokens de inicio de secuencia según sea necesario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(documents, k, end_padding=True):\n",
    "    padded_documents = []\n",
    "    for doc in documents:\n",
    "        doc =  ['<s>']*k + doc\n",
    "        if end_padding:\n",
    "            doc += ['</s>']\n",
    "            \n",
    "        padded_documents.append(doc)\n",
    "    return padded_documents\n",
    "\n",
    "def mask_documents(documents, vocabulary):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            if vocabulary.get(word) is not None:\n",
    "                masked_doc.append(word)\n",
    "            else:\n",
    "                masked_doc.append('<unk>')\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_vocabulary(documents, start='', end='', n=-1):\n",
    "    # get unique words\n",
    "    words = [word for doc in documents for word in doc]\n",
    "    unique_words = FreqDist(words).most_common(n) if n!= -1 else FreqDist(words).most_common() \n",
    "    # init voc dict\n",
    "    vocabulary = {start: 0} if start != '' else {}\n",
    "    # fill vocabulary with positions\n",
    "    pos_available = 1 if start != '' else 0\n",
    "    for (word, _) in unique_words:\n",
    "        # verify words is not start, end or unk token (special positions for those)\n",
    "        if word not in (start, end, '<unk>'):\n",
    "            vocabulary[word] = pos_available\n",
    "            pos_available += 1\n",
    "    # set unk token\n",
    "    vocabulary['<unk>'] = len(vocabulary)\n",
    "    # if padded was added, set end token\n",
    "    if end != '':\n",
    "        vocabulary[end] = len(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def trim_vocabulary(side, vocabulary):\n",
    "    new_voc = {}\n",
    "    if side == 'top':\n",
    "        for (key, value) in list(vocabulary.items())[1:]:\n",
    "            new_voc[key] = value-1\n",
    "    elif side == 'bottom':\n",
    "        for (key, value) in list(vocabulary.items())[:-1]:\n",
    "            new_voc[key] = value\n",
    "    else:\n",
    "        for (key, value) in list(vocabulary.items())[1:-1]:\n",
    "            new_voc[key] = value-1\n",
    "    \n",
    "    return new_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1. Preprocess Unigrams and Bigrams\n",
    "\n",
    "En el siguiente bloque tenemos las funciones base que se llaman para todos los modelos presentados en este trabajo. En especial las funciones **prepair\\_unigram** y **prepair\\_bigram** se encargan de preparar los documentos llamando a las funciones necesarias para enmascarar los vocabularios y agregar padding según sea necesario. \n",
    "\n",
    "Para la construcción de los trigramas se utiliza también la función **build\\_bigram\\_documents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents into bigram documents\n",
    "def build_bigram_documents(documents):\n",
    "    bigram_documents = [[word1 + ' ' + word2 for word1, word2 in zip(doc, doc[1:])] for doc in documents]\n",
    "    return bigram_documents\n",
    "\n",
    "def prepair_unigram(documents, n_voc):\n",
    "    vocabulary = get_vocabulary(documents, start='<s>', end='</s>', n=n_voc)\n",
    "    docs = add_padding(documents, 1)\n",
    "    docs = mask_documents(docs, vocabulary)\n",
    "    return vocabulary, docs\n",
    "\n",
    "def prepair_bigram(documents, n_voc):\n",
    "    # get unigrams and mask documents\n",
    "    vocabulary = get_vocabulary(documents, end='</s>', n=n_voc)\n",
    "    docs = mask_documents(documents, vocabulary)\n",
    "    docs = add_padding(docs, 1)\n",
    "    docs = add_padding(docs, 1, end_padding=False)\n",
    "    \n",
    "    # get bigrams vocabulary\n",
    "    bi_docs = add_padding(documents, 2, end_padding=False)\n",
    "    bi_docs = build_bigram_documents(bi_docs)\n",
    "    bi_vocabulary = get_vocabulary(bi_docs, start='<s> <s>', n=n_voc)\n",
    "    \n",
    "    # return vocabularies and documents padded\n",
    "    return vocabulary, bi_vocabulary, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build N Grams Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram(documents, vocabulary):\n",
    "    counts = np.zeros(len(vocabulary))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for word in doc[1:]:                                                            \n",
    "            counts[vocabulary[word]]+= 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram(documents, r_voc, c_voc):\n",
    "    n = len(r_voc)\n",
    "    m = len(c_voc)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for i in range(1, len(doc)):                                                     \n",
    "            context, word = doc[i-1], doc[i]\n",
    "            counts[r_voc[context], c_voc[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram(documents, vocabulary, bi_vocabulary):\n",
    "    m = len(vocabulary)\n",
    "    n = len(bi_vocabulary)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s>, <s> in padded couments\n",
    "        for i in range(2, len(doc)):                                                       \n",
    "            context, word = doc[i-2] + ' ' + doc[i-1], doc[i]\n",
    "            context = context if bi_vocabulary.get(context) is not None else '<unk>'\n",
    "            counts[bi_vocabulary[context], vocabulary[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_string(string):\n",
    "    return '\\033[1m' + string + '\\033[0m '\n",
    "\n",
    "def print_sequence(seq):\n",
    "    for word in seq[1:-1]:\n",
    "        print(word, end=' ')\n",
    "    print('') #flush with new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2. Unigramas, Bigramas, Trigramas\n",
    "\n",
    "Todos los modelos serán construidos como clases para poder llamar a sus métodos pertinentes para poder realizar las acciones solicitadas. Para el modelo de bigrama y trigrama se utilizará la variante del smoothing Laplace en donde se agrega un valor $k$ a todas las cuentas en vez de agregar 1. Se experimentó con varios valores y en general se notó que escoger valores pequeños para $k$ reducían la perplejidad.\n",
    "\n",
    "Los modelos serán evaluados en esta sección entrenandolos con el conjunto original de training y evaluando sus perplejidades con el conjunto original de validación. Es decir, las particiones creadas no serán utilizadas en esta sección. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    def train(self, documents, voc_size=10000):\n",
    "        voc, unidocs = prepair_unigram(documents, voc_size)\n",
    "        self.voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.voc.keys())\n",
    "        self.counts = build_unigram(unidocs, self.voc)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        self.probs = self.counts / np.sum(self.counts)\n",
    "    \n",
    "    def predict(self):\n",
    "        c_index = sample(self.probs)\n",
    "        return self.voc_words[c_index], self.probs[c_index]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 1:\n",
    "            print('[ERR]: Not Enough Tokens for Unigram Model')\n",
    "            return 1\n",
    "        \n",
    "        total_logprob = 0\n",
    "        for word in sequence:\n",
    "            token = '<unk>' if self.voc.get(word) is None else word\n",
    "            prob = self.probs[self.voc[token]]\n",
    "            total_logprob += np.log(prob)\n",
    "            \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word = '<s>'\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict()\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1\n",
    "            for i in range(1, len(test)):\n",
    "                prob = self.estimate_prob([test[i]])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGram Model Base Class\n",
    "\n",
    "La siguiente clase es la clase base tanto para los bigramas como trigramas. Se utilizó una clase base ya que ambos modelos serán implementados a través de una matriz que representará las probabilidades condicionadas. En la dimensión de filas tendremos el contexto que condiciona al token actual, y en las columnas tendremos el token actual procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def train(self):\n",
    "        raise NotImplementedError('Subclass should implement own train')\n",
    "    \n",
    "    def estimate_prob(self):\n",
    "        raise NotImplementedError('Subclass should implement own prob function')\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        raise NotImplementedError('Subclass should implement own generate function')\n",
    "        \n",
    "    def eval_model(self, documents):\n",
    "        raise NotImplementedError('Subclass should implement own eval function')\n",
    "        \n",
    "    def perplexity(self, test_set):\n",
    "        raise NotImplementedError('Subclass should implement own perplexity function')\n",
    "    \n",
    "    def smooth(self, k):\n",
    "        self.counts = self.counts + k\n",
    "    \n",
    "    # perform a prediction of a token with a given context\n",
    "    def predict(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        c_index = sample(self.probs[r_index])\n",
    "        return self.voc_words[c_index], self.probs[r_index, c_index]\n",
    "    \n",
    "    # function to retrieve all the conditioned space probability, i.e. all the columns of a certain context \n",
    "    def conditioned_space(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>'            # mask if necessary\n",
    "        r_index = self.r_voc[context]\n",
    "        return self.probs[r_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(NGramModel):\n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        voc, docs = prepair_unigram(documents, voc_size)\n",
    "        self.r_voc = trim_vocabulary('bottom', voc)\n",
    "        self.c_voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts  = build_bigram(docs, self.r_voc, self.c_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "        \n",
    "    def get_probs(self):\n",
    "        unicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/unicounts[:, np.newaxis]    \n",
    "    \n",
    "    def cond_prob(self, word1, word):\n",
    "        cond_space = self.conditioned_space(word1)                 # get conditioned space p(.|word1)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word  # mask if necessary\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 2:\n",
    "            print('[ERR]: Not Enough Tokens for Bigram Model')\n",
    "            return 1\n",
    "        #build context\n",
    "        word1 = sequence[0] \n",
    "        word = word1\n",
    "        total_logprob = 0\n",
    "        for word in sequence[1:]:\n",
    "            prob = self.cond_prob(word1, word)     #conditional probability\n",
    "            total_logprob += np.log(prob)\n",
    "            word1 = word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word = word1\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1)          # predict a token given the current context\n",
    "            word1 = word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1   \n",
    "            for i in range(1, len(test)):         # skip <s> token\n",
    "                c1, w = test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Model\n",
    "\n",
    "Para los trigramas obtenemos dos vocabularios. Primero obtenemos el vocabulario de los tokens más comunes como en los modelos anteriores, y luego el vocabulario de los bigramas, que condicionan al token actual, más comunes. Es importante resaltar que para este modelo en la dimensión de los bigramas se toma como token desconocido '\\<unk\\>' cuando la unión de ambos tokens que conforman al bigrama no se encuentra en el vocabulario de bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(NGramModel):\n",
    "    def __init__(self):\n",
    "        super(NGramModel).__init__()\n",
    "    \n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        self.c_voc, self.r_voc, docs = prepair_bigram(documents, voc_size)\n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts = build_trigram(docs, self.c_voc, self.r_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        bicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/bicounts[:, np.newaxis]    \n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        cond_space = self.conditioned_space(word1 + ' ' + word2)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word           # mask if necessary\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Trigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1 + ' ' + word2)\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2\n",
    "            for i in range(2, len(test)):                      # skip both <s> <s> tokens\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the probability of a sequence and print the results\n",
    "def eval_sequence(prob_func, seq, extra=''):\n",
    "    cad = ''\n",
    "    for s in seq:\n",
    "        cad += s + ' '\n",
    "    \n",
    "    print(bold_string('secuencia: '), cad, )\n",
    "    print(bold_string('probabilidad de secuencia {0}: '.format(extra)), prob_func(seq))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = UnigramModel()\n",
    "unigram.train(documents, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  te amo \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  2.543942286175518e-06\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  tokenDesconocidoDefinitivamente amo \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  9.474624878238107e-06\n",
      "\n",
      "\u001b[1mGeneración de Secuencia\u001b[0m \n",
      "que ¿ <unk> . tengo que creer día . valiendo 💦 mundo a awebo \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(unigram.estimate_prob, ['te', 'amo'])\n",
    "eval_sequence(unigram.estimate_prob, ['tokenDesconocidoDefinitivamente', 'amo'], 'token desconocido')\n",
    "\n",
    "print(bold_string('Generación de Secuencia'))\n",
    "print_sequence(unigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.005, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  hijos de la verga \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  0.008143255147411203\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  <s> las perplejidades son altas </s> \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  1.7890221731532313e-15\n",
      "\n",
      "\u001b[1msecuencia condicionada: \u001b[0m  vete a\n",
      "\u001b[1mProbabilidad Condicional: \u001b[0m  0.20783015192832105\n",
      "\u001b[1m\n",
      "Generación de Secuencia\u001b[0m \n",
      "@usuario ya lo voy a ver si está de verga x tu meme estl firmado brinque actriz pides antes de youtuber suba numero fairplay dinero ; party quítenme comunidad jajajajajajajajaja vuelvan irrrrr póster taller arruinar solecitos dámela oigo privada mera remplazo excelente tarde-noche uniones tragando cachorrito lucrar consiguio país sabrosear malnacido emocionan ponte 💛 recuperas juanes armado boludeces ulises indie curada programacion adolecía desahogo bajan agarrar khé 10 muertos #basica redes novia dulces del pan superé divertidos vacaciones mentirle boxeadores emperra #0pedosmorra automáticas echarnos jsjaja gordota note oooo americana #sientetuliga alaverga chingue a pagar real discapacitados remate tardas javier vende vendió historia balazo lienzo ke mond ombligo cajeros cms mamacita siempleme hormigas under percepción fueron regresar perros madridista 1as oyos ✋🏻 regrese 👩🏿 mamados profesionista break tientan oler empataron purposeaún ed rivales tocas brayan tal si no pasa trató hijamadre diré .. mas de verga si hay que díganme d: mejo @travestisexico sapos tomaron echaron ponte sagrado charco pobrecita atreven tachira deseo mal parido :'( ) comunista tomando pastillas aguanta nacional nacido corajes #jalisco césar c5n vayan lienzo acepto matrimonio merda sismo desviado sandler cachetadon marito abusada random cargando iluminados jajajaj muchas acordarme botes babel mota insulte lamber dama #consalvadoriglesias sistemas manitas suficiente chilindrina convierten linea mamonerias elegante mantenido tiroteo inflados empece diputados 🤷🏼‍♀ :p 💕 mami sepan escoja snap darío gifs pollozo hazlo adivinen ocurrente marca séase pasaras conocerme huele bn nalgada división #deudasdejuego paraste entonces diseñan 😊 \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(bigram.estimate_prob, ['hijos', 'de', 'la', 'verga'])\n",
    "eval_sequence(bigram.estimate_prob, ['<s>', 'las', 'perplejidades', 'son', 'altas', '</s>'], 'token desconocido')\n",
    "print(bold_string('secuencia condicionada: '), 'vete a')\n",
    "print(bold_string('Probabilidad Condicional: '), bigram.cond_prob('vete', 'a'))\n",
    "print(bold_string('\\nGeneración de Secuencia'))\n",
    "print_sequence(bigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556.010612606513"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas Trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = TrigramModel()\n",
    "trigram.train(documents, k=0.005, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  hijos de la verga \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  0.02352065607856261\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  <s> las perplejidades son altas </s> \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  2.1106826559148392e-13\n",
      "\n",
      "\u001b[1msecuencia condicionada: \u001b[0m  vete a la\n",
      "\u001b[1mProbabilidad Condicional: \u001b[0m  0.19722574285311928\n",
      "\u001b[1m\n",
      "Generación de Secuencia\u001b[0m \n",
      "con violador putas cago lo no chingas a tu leia ... \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(trigram.estimate_prob, ['hijos', 'de', 'la', 'verga'])\n",
    "eval_sequence(trigram.estimate_prob, ['<s>', 'las', 'perplejidades', 'son', 'altas', '</s>'], 'token desconocido')\n",
    "print(bold_string('secuencia condicionada: '), 'vete a la')\n",
    "print(bold_string('Probabilidad Condicional: '), trigram.cond_prob('vete', 'a', 'la'))\n",
    "print(bold_string('\\nGeneración de Secuencia'))\n",
    "print_sequence(trigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplejidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1munigram perplexity: \t\u001b[0m  368.36738387642487\n",
      "\u001b[1mbigram perplexity: \t\u001b[0m  390.9887421025693\n",
      "\u001b[1mtrigram perplexity: \t\u001b[0m  515.9447123589185\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('unigram perplexity: \\t'), unigram.eval_model(val_documents))\n",
    "print(bold_string('bigram perplexity: \\t'), bigram.eval_model(val_documents))\n",
    "print(bold_string('trigram perplexity: \\t'), trigram.eval_model(val_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos resaltar varias cosas interesantes a partir de estas pruebas. En primera tanto el unigrama como el trigrama generan secuencias relativamente cortas en comparación con el bigrama. Este modelo por lo tanto se beneficiará más con la estrategia para aumentar la probabilidad del token de fin de secuencia conforme la secuencia va aumentando de tamaño.  También podemos ver que en cuanto a los valores de la perplejidad, el unigrama tiene el menor valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_ = [[1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1],[.1, .4, .5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedModel:\n",
    "    def __init__(self, lambda_):\n",
    "        self.l1, self.l2, self.l3 = lambda_\n",
    "        self.unigram = UnigramModel()\n",
    "        self.bigram = BigramModel()\n",
    "        self.trigram = TrigramModel()\n",
    "    \n",
    "    def verify_vocs(self):\n",
    "        uvoc = self.unigram.voc\n",
    "        bvoc = self.bigram.c_voc\n",
    "        tvoc = self.trigram.c_voc\n",
    "        \n",
    "        for u, b, t in zip(uvoc.keys(), bvoc.keys(), tvoc.keys()):\n",
    "            if u != b or b!=t:\n",
    "                print('WARN: vocabularies dont match')\n",
    "        \n",
    "        print('Finished checking vocabularies')\n",
    "        \n",
    "    def train(self, documents, k=0, voc_size=10000):\n",
    "        self.unigram.train(documents, voc_size)\n",
    "        self.bigram.train(documents, k, voc_size)\n",
    "        self.trigram.train(documents, k, voc_size)\n",
    "        self.verify_vocs()\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        # build contexts\n",
    "        bicontext = sequence[1]\n",
    "        tricontext = sequence[0] + ' ' + sequence[1]\n",
    "        \n",
    "        # get conditioned spaces\n",
    "        unispace = self.unigram.probs\n",
    "        bispace = self.bigram.conditioned_space(bicontext)\n",
    "        trispace = self.trigram.conditioned_space(tricontext)\n",
    "        \n",
    "        # sample from probability space\n",
    "        probs = self.l1 * unispace + self.l2 * bispace + self.l3 * trispace\n",
    "        c_index = sample(probs)\n",
    "        \n",
    "        return self.unigram.voc_words[c_index], probs[c_index]\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        uniprob = self.unigram.estimate_prob(word)\n",
    "        biprob  = self.bigram.cond_prob(word2, word)\n",
    "        triprob = self.trigram.cond_prob(word1, word2, word)\n",
    "        prob = self.l1 * uniprob + self.l2 * biprob + self.l3 * triprob\n",
    "        return prob\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Interpolated Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict([word1, word2])\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp\n",
    "    \n",
    "    def em_train(self, val_set, max_it, init_weights=None):\n",
    "        val_docs = add_padding(val_set, k=2)\n",
    "        probs = []\n",
    "        for val_doc in val_docs:\n",
    "            for i in range(2, len(val_doc)):\n",
    "                w1, w2, w = val_doc[i-2], val_doc[i-1], val_doc[i]\n",
    "                uniprob = self.unigram.estimate_prob(w)\n",
    "                biprob  = self.bigram.cond_prob(w2, w)\n",
    "                triprob = self.trigram.cond_prob(w1, w2, w)\n",
    "                probs.append([uniprob, biprob, triprob])\n",
    "        \n",
    "        weights, hist = optimize_em(np.array(probs), max_it, init_weights)\n",
    "        self.l1, self.l2, self.l3 = weights\n",
    "        \n",
    "        perplexities = []\n",
    "        for weight in hist:\n",
    "            self.l1, self.l2, self.l3 = weight\n",
    "            perplexities.append(self.perplexity(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_em(prob_matrix, n_iter, init_weights = None):\n",
    "    # Initialize model weights\n",
    "    if init_weights is not None:\n",
    "        weights = np.array(init_weights)\n",
    "    else:\n",
    "        n_models = prob_matrix.shape[1]\n",
    "        weights = np.ones(n_models) / n_models\n",
    "\n",
    "    weights_hist = [weights]\n",
    "    for it in range(n_iter):\n",
    "        # 2 Expectation: calculate posterior probabilities from current model weights\n",
    "        weighted_probs = prob_matrix * weights\n",
    "        total_probs = weighted_probs.sum(axis=1, keepdims=True)\n",
    "        posterior_probs = weighted_probs / total_probs\n",
    "\n",
    "        # 3 Maximization: update model weights using posterior probabilities from E-step\n",
    "        weights = posterior_probs.mean(axis=0)\n",
    "        # add weights to weight history\n",
    "        weights_hist.append(weights)\n",
    "\n",
    "    return weights, weights_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_model = InterpolatedModel(lambdas_[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking vocabularies\n"
     ]
    }
   ],
   "source": [
    "i_model.train(train_corpus, k=0.0001, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410.34812338516446\n",
      "308.23770647670113\n",
      "302.90002253936495\n",
      "302.53240517888923\n",
      "302.512690057188\n",
      "302.5158378321146\n",
      "302.5178525412473\n",
      "302.51855255576277\n",
      "302.51873933211596\n",
      "302.5187694293783\n",
      "302.51876212521836\n"
     ]
    }
   ],
   "source": [
    "i_model.em_train(val_corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7246.808632125437"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_model.eval_model(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@usuario se mamo con <unk> ayer me acorde de la chiluda #masterchefmx \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#esfeocuandoteenteras que andan te chupo 🤗 audio ? ? váyanse que @usuario está de la celebración de tus <unk> pongo soy es ! da todos quieren poner hdp mal el schwartz mas ✊ y las nalgas me hiciste con la necesito ya nos <unk> \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bien hermano masturbarme narración de toda la cara sería que . \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actualización de Probabilidades \n",
    "\n",
    "Es de interés tomar cierta medida para asegurar que la probabilidad del token de fin de secuencia '\\</s\\>' vaya aumentando conforme la secuencia se va haciendo más larga. Para ello utilizaremos la siguiente regla de actualización: \n",
    "\n",
    "Sea $p_s$ la probabilidad de obtener el token de fin de secuencia. Entonces como $p_s \\leq 1$, sabemos que ${p^r_s} \\geq p_s$ en donde $r<1$. De hecho, sabemos también que \n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} \\sqrt[n]{r} = 1$$ \n",
    "\n",
    "Entonces, podemos tomar la regla de actualización $$\\hat{p}_s = \\sqrt[n]{p_s}$$\n",
    "\n",
    "Debido a que esta probabilidad aumentó, para asegurarnos que el espacio de probabilidad se encuentra bien definido, debemos disminuir esta probabilidad de los otros tokens para asegurarnos que la suma de las probabilidades siga siendo 1. Definamos el aumento de la probabilidad que tenemos respecto al token de fin de secuencia como \n",
    "\n",
    "$$a_p = \\hat{p}_s - p_s$$\n",
    "\n",
    "Entonces, sea $p_i$ la probabilidad de obtener el token $t_i$ en donde $t_i \\neq $ '\\</s\\>'. Definamos a $\\sigma$ como \n",
    "\n",
    "$$\\sigma = \\sum_{i=1}^{|V|} p_i$$\n",
    "\n",
    "en donde $|V|$ representa la cardinalidad del conjunto del vocabulario sin considerar al token de fin de secuencia. Notemos que $\\sigma = 1 - p_s$. Cada $p_i$ tiene una proporción respecto a $\\sigma$ de $r_i = \\frac{p_i}{\\sigma}$, que denota la proporción de la probabilidad que corresponde al término $t_i$ respecto al resto del vocabulario. Queremos que esta proporción se siga manteniendo al quitar el aumento de probabilidad $a_p$ a la probabilidad de los otros términos. Entonces, utilizando la siguiente regla de actualización\n",
    "\n",
    "$$\\hat{p}_i = p_i - r_i a_p$$\n",
    "\n",
    "y definiendo a $$\\hat{\\sigma} = \\sum_{i=1}^{|V|} \\hat{p}_i$$\n",
    "\n",
    "podemos ver que se cumple $$\\hat{r}_i = \\frac{\\hat{p}_i}{\\hat{\\sigma}} = r_i$$\n",
    "\n",
    "Notemos también que estas reglas en conjunto también se puede utilizar para minimizar la probabilidad tomando a $\\hat{p}_s = p^n_s$. De esta manera el incremento $a_p$ será de hecho un decremento, por lo tanto será negativo y de esta manera las probabilidades $p_i$ en vez de disminuir, aumentan proporcionalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receives a probs matrix and the power r.\n",
    "def diminish(probs, r):\n",
    "    # calculate new probability\n",
    "    new_probs = np.zeros(probs.shape)\n",
    "    new_stop_prob = np.power(probs[:, -1], r)\n",
    "    # get improvement\n",
    "    improve = (new_stop_prob - probs[:, -1])\n",
    "    # get ratio of the other probabilities between them\n",
    "    c = np.sum(probs[:, :-1], axis=1)\n",
    "    rat = probs[:, :-1]/c[:, np.newaxis]\n",
    "    # update new probability\n",
    "    new_probs[:, -1] = new_stop_prob\n",
    "    new_probs[:, :-1] = probs[:, :-1] - rat * improve[:, np.newaxis]\n",
    "    \n",
    "    return new_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutar Oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def get_permutations(sentence):\n",
    "    return set(permutations(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7400828 0.6687403]\n",
      "[0.4400828 0.4687403]\n",
      "[1. 1.]\n",
      "[[0.14852411 0.11139308 0.7400828 ]\n",
      " [0.12422239 0.20703731 0.6687403 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = [[.4, .3, .3],[.3, .5, .2]]\n",
    "probs = np.array(probs, dtype=np.float128)\n",
    "\n",
    "new_probs = diminish(probs, 0.25)\n",
    "print(new_probs)\n",
    "np.sum(new_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".4/(.4 + .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285769248274"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".14852411/(0.14852411 + 0.11139308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m = TrigramModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1887"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.bi_voc['<s> hola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10001, 10002), 10001, 10002)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.probs.shape, len(trigram_m.bi_voc), len(trigram_m.voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.999999999993"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('alv .', 891)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigram_m.bi_voc.items())[891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trigram_m.probs.shape[0]):\n",
    "    for j in range(trigram_m.probs.shape[1]):\n",
    "        if np.isnan(trigram_m.probs[i,j]):\n",
    "            print('nan at', i, ' ', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m.train(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, unidocs = prepair_unigram(documents, 10000)\n",
    "bi_vocabulary, bidocs = prepair_bigram(documents, 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = build_unigram(unidocs, vocabulary)\n",
    "unigram_prob = unigram/np.sum(unigram[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = build_bigram(unidocs, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_prob = bigram[:-1]/unigram[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.000000000004"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_padded_docs = add_padding(unidocs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = build_trigram(bi_padded_docs, vocabulary, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_of_bigrams = build_unigram(bidocs, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_prob = trigram[:-1]/unigram_of_bigrams[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True, False])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram, axis=1) == unigram_of_bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bi_vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_vocabulary['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02958152958152958"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob[padded_vocabulary['<s>'], padded_vocabulary['<unk>']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
