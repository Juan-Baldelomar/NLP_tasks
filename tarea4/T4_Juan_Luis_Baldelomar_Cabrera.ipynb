{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 4 - Juan Luis Baldelomar Cabrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_partitions(documents, labels):\n",
    "    n = len(documents)\n",
    "    train_docs, test_docs, train_labels, test_labels = train_test_split(documents, labels, test_size=0.10, random_state=42)\n",
    "    train_docs, val_docs, train_labels, val_labels = train_test_split(train_docs, train_labels, test_size=n//10, random_state=42)\n",
    "    return train_docs, val_docs, test_docs, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "\n",
    "#remove extra lines\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)\n",
    "\n",
    "# process documents\n",
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)\n",
    "\n",
    "# build partitions\n",
    "all_documents = documents + val_documents\n",
    "all_labels = labels + val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Val and Test Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, val_corpus, test_corpus, _, _, _ = get_partitions(all_documents, all_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Masking\n",
    "\n",
    "Funciones para enmascarar el vocabulario y agregar padding a los documentos. Notemos que la función que agrega el padding puede agregar $k$ tokens de inicio de secuencia según sea necesario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(documents, k, end_padding=True):\n",
    "    padded_documents = []\n",
    "    for doc in documents:\n",
    "        doc =  ['<s>']*k + doc\n",
    "        if end_padding:\n",
    "            doc += ['</s>']\n",
    "            \n",
    "        padded_documents.append(doc)\n",
    "    return padded_documents\n",
    "\n",
    "def mask_documents(documents, vocabulary):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            if vocabulary.get(word) is not None:\n",
    "                masked_doc.append(word)\n",
    "            else:\n",
    "                masked_doc.append('<unk>')\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_vocabulary(documents, start='', end='', n=-1):\n",
    "    # get unique words\n",
    "    words = [word for doc in documents for word in doc]\n",
    "    unique_words = FreqDist(words).most_common(n) if n!= -1 else FreqDist(words).most_common() \n",
    "    # init voc dict\n",
    "    vocabulary = {start: 0} if start != '' else {}\n",
    "    # fill vocabulary with positions\n",
    "    pos_available = 1 if start != '' else 0\n",
    "    for (word, _) in unique_words:\n",
    "        # verify words is not start, end or unk token (special positions for those)\n",
    "        if word not in (start, end, '<unk>'):\n",
    "            vocabulary[word] = pos_available\n",
    "            pos_available += 1\n",
    "    # set unk token\n",
    "    vocabulary['<unk>'] = len(vocabulary)\n",
    "    # if padded was added, set end token\n",
    "    if end != '':\n",
    "        vocabulary[end] = len(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def trim_vocabulary(side, vocabulary):\n",
    "    new_voc = {}\n",
    "    if side == 'top':\n",
    "        for (key, value) in list(vocabulary.items())[1:]:\n",
    "            new_voc[key] = value-1\n",
    "    elif side == 'bottom':\n",
    "        for (key, value) in list(vocabulary.items())[:-1]:\n",
    "            new_voc[key] = value\n",
    "    else:\n",
    "        for (key, value) in list(vocabulary.items())[1:-1]:\n",
    "            new_voc[key] = value-1\n",
    "    \n",
    "    return new_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1. Preprocess Unigrams and Bigrams\n",
    "\n",
    "En el siguiente bloque tenemos las funciones base que se llaman para todos los modelos presentados en este trabajo. En especial las funciones **prepair\\_unigram** y **prepair\\_bigram** se encargan de preparar los documentos llamando a las funciones necesarias para enmascarar los vocabularios y agregar padding según sea necesario. \n",
    "\n",
    "Para la construcción de los trigramas se utiliza también la función **build\\_bigram\\_documents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents into bigram documents\n",
    "def build_bigram_documents(documents):\n",
    "    bigram_documents = [[word1 + ' ' + word2 for word1, word2 in zip(doc, doc[1:])] for doc in documents]\n",
    "    return bigram_documents\n",
    "\n",
    "def prepair_unigram(documents, n_voc):\n",
    "    vocabulary = get_vocabulary(documents, start='<s>', end='</s>', n=n_voc)\n",
    "    docs = add_padding(documents, 1)\n",
    "    docs = mask_documents(docs, vocabulary)\n",
    "    return vocabulary, docs\n",
    "\n",
    "def prepair_bigram(documents, n_voc):\n",
    "    # get unigrams and mask documents\n",
    "    vocabulary = get_vocabulary(documents, end='</s>', n=n_voc)\n",
    "    docs = mask_documents(documents, vocabulary)\n",
    "    docs = add_padding(docs, 1)\n",
    "    docs = add_padding(docs, 1, end_padding=False)\n",
    "    \n",
    "    # get bigrams vocabulary\n",
    "    bi_docs = add_padding(documents, 2, end_padding=False)\n",
    "    bi_docs = build_bigram_documents(bi_docs)\n",
    "    bi_vocabulary = get_vocabulary(bi_docs, start='<s> <s>', n=n_voc)\n",
    "    \n",
    "    # return vocabularies and documents padded\n",
    "    return vocabulary, bi_vocabulary, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build N Grams Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram(documents, vocabulary):\n",
    "    counts = np.zeros(len(vocabulary))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for word in doc[1:]:                                                            \n",
    "            counts[vocabulary[word]]+= 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram(documents, r_voc, c_voc):\n",
    "    n = len(r_voc)\n",
    "    m = len(c_voc)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for i in range(1, len(doc)):                                                     \n",
    "            context, word = doc[i-1], doc[i]\n",
    "            counts[r_voc[context], c_voc[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram(documents, vocabulary, bi_vocabulary):\n",
    "    m = len(vocabulary)\n",
    "    n = len(bi_vocabulary)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s>, <s> in padded couments\n",
    "        for i in range(2, len(doc)):                                                       \n",
    "            context, word = doc[i-2] + ' ' + doc[i-1], doc[i]\n",
    "            context = context if bi_vocabulary.get(context) is not None else '<unk>'\n",
    "            counts[bi_vocabulary[context], vocabulary[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_string(string):\n",
    "    return '\\033[1m' + string + '\\033[0m '\n",
    "\n",
    "def print_sequence(seq, start=1, end=-1):\n",
    "    if end == 'all':\n",
    "        end = len(seq)\n",
    "        \n",
    "    for word in seq[start:end]:\n",
    "        print(word, end=' ')\n",
    "    print('') #flush with new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2. Unigramas, Bigramas, Trigramas\n",
    "\n",
    "Todos los modelos serán construidos como clases para poder llamar a sus métodos pertinentes para poder realizar las acciones solicitadas. Para el modelo de bigrama y trigrama se utilizará la variante del smoothing Laplace en donde se agrega un valor $k$ a todas las cuentas en vez de agregar 1. Se experimentó con varios valores y en general se notó que escoger valores pequeños para $k$ reducían la perplejidad.\n",
    "\n",
    "Los modelos serán evaluados en esta sección entrenandolos con el conjunto original de training y evaluando sus perplejidades con el conjunto original de validación. Es decir, las particiones creadas no serán utilizadas en esta sección. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    def train(self, documents, voc_size=10000):\n",
    "        voc, unidocs = prepair_unigram(documents, voc_size)\n",
    "        self.voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.voc.keys())\n",
    "        self.counts = build_unigram(unidocs, self.voc)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        self.probs = self.counts / np.sum(self.counts)\n",
    "    \n",
    "    def predict(self):\n",
    "        c_index = sample(self.probs)\n",
    "        return self.voc_words[c_index], self.probs[c_index]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 1:\n",
    "            print('[ERR]: Not Enough Tokens for Unigram Model')\n",
    "            return 1\n",
    "        \n",
    "        total_logprob = 0\n",
    "        for word in sequence:\n",
    "            token = '<unk>' if self.voc.get(word) is None else word\n",
    "            prob = self.probs[self.voc[token]]\n",
    "            total_logprob += np.log(prob)\n",
    "            \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word = '<s>'\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict()\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1\n",
    "            for i in range(1, len(test)):\n",
    "                prob = self.estimate_prob([test[i]])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGram Model Base Class\n",
    "\n",
    "La siguiente clase es la clase base tanto para los bigramas como trigramas. Se utilizó una clase base ya que ambos modelos serán implementados a través de una matriz que representará las probabilidades condicionadas. En la dimensión de filas tendremos el contexto que condiciona al token actual, y en las columnas tendremos el token actual procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def train(self):\n",
    "        raise NotImplementedError('Subclass should implement own train')\n",
    "    \n",
    "    def estimate_prob(self):\n",
    "        raise NotImplementedError('Subclass should implement own prob function')\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        raise NotImplementedError('Subclass should implement own generate function')\n",
    "        \n",
    "    def eval_model(self, documents):\n",
    "        raise NotImplementedError('Subclass should implement own eval function')\n",
    "        \n",
    "    def perplexity(self, test_set):\n",
    "        raise NotImplementedError('Subclass should implement own perplexity function')\n",
    "    \n",
    "    def smooth(self, k):\n",
    "        self.counts = self.counts + k\n",
    "    \n",
    "    # perform a prediction of a token with a given context\n",
    "    def predict(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        c_index = sample(self.probs[r_index])\n",
    "        return self.voc_words[c_index], self.probs[r_index, c_index]\n",
    "    \n",
    "    # function to retrieve all the conditioned space probability, i.e. all the columns of a certain context \n",
    "    def conditioned_space(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>'            # mask if necessary\n",
    "        r_index = self.r_voc[context]\n",
    "        return self.probs[r_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(NGramModel):\n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        voc, docs = prepair_unigram(documents, voc_size)\n",
    "        self.r_voc = trim_vocabulary('bottom', voc)\n",
    "        self.c_voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts  = build_bigram(docs, self.r_voc, self.c_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "        \n",
    "    def get_probs(self):\n",
    "        unicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/unicounts[:, np.newaxis]    \n",
    "    \n",
    "    def cond_prob(self, word1, word):\n",
    "        cond_space = self.conditioned_space(word1)                 # get conditioned space p(.|word1)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word  # mask if necessary\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 2:\n",
    "            print('[ERR]: Not Enough Tokens for Bigram Model')\n",
    "            return 1\n",
    "        #build context\n",
    "        word1 = sequence[0] \n",
    "        word = word1\n",
    "        total_logprob = 0\n",
    "        for word in sequence[1:]:\n",
    "            prob = self.cond_prob(word1, word)     #conditional probability\n",
    "            total_logprob += np.log(prob)\n",
    "            word1 = word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self, max_length=None, strat=None, activation_window=3):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word = word1\n",
    "        actual_probs = self.probs\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1)          # predict a token given the current context\n",
    "            word1 = word\n",
    "            sequence.append(word)\n",
    "            if strat is not None:\n",
    "                new_prob_table = strat(self.probs, len(sequence), max_length, activation_window)\n",
    "                if new_prob_table is not None:\n",
    "                    self.probs=new_prob_table\n",
    "        \n",
    "        self.probs = actual_probs\n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1   \n",
    "            for i in range(1, len(test)):         # skip <s> token\n",
    "                c1, w = test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Model\n",
    "\n",
    "Para los trigramas obtenemos dos vocabularios. Primero obtenemos el vocabulario de los tokens más comunes como en los modelos anteriores, y luego el vocabulario de los bigramas, que condicionan al token actual, más comunes. Es importante resaltar que para este modelo en la dimensión de los bigramas se toma como token desconocido '\\<unk\\>' cuando la unión de ambos tokens que conforman al bigrama no se encuentra en el vocabulario de bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(NGramModel):\n",
    "    def __init__(self):\n",
    "        super(NGramModel).__init__()\n",
    "    \n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        self.c_voc, self.r_voc, docs = prepair_bigram(documents, voc_size)\n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts = build_trigram(docs, self.c_voc, self.r_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        bicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/bicounts[:, np.newaxis]    \n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        cond_space = self.conditioned_space(word1 + ' ' + word2)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word           # mask if necessary\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Trigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1 + ' ' + word2)\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2\n",
    "            for i in range(2, len(test)):                      # skip both <s> <s> tokens\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the probability of a sequence and print the results\n",
    "def eval_sequence(prob_func, seq, extra=''):\n",
    "    cad = ''\n",
    "    for s in seq:\n",
    "        cad += s + ' '\n",
    "    \n",
    "    print(bold_string('secuencia: '), cad, )\n",
    "    print(bold_string('probabilidad de secuencia {0}: '.format(extra)), prob_func(seq))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = UnigramModel()\n",
    "unigram.train(documents, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  te amo \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  2.543942286175518e-06\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  tokenDesconocidoDefinitivamente amo \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  9.474624878238107e-06\n",
      "\n",
      "\u001b[1mGeneración de Secuencia\u001b[0m \n",
      "que ¿ <unk> . tengo que creer día . valiendo 💦 mundo a awebo \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(unigram.estimate_prob, ['te', 'amo'])\n",
    "eval_sequence(unigram.estimate_prob, ['tokenDesconocidoDefinitivamente', 'amo'], 'token desconocido')\n",
    "\n",
    "print(bold_string('Generación de Secuencia'))\n",
    "print_sequence(unigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.005, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  hijos de la verga \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  0.008143255147411203\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  <s> las perplejidades son altas </s> \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  1.7890221731532313e-15\n",
      "\n",
      "\u001b[1msecuencia condicionada: \u001b[0m  vete a\n",
      "\u001b[1mProbabilidad Condicional: \u001b[0m  0.20783015192832105\n",
      "\u001b[1m\n",
      "Generación de Secuencia\u001b[0m \n",
      "@usuario ya lo voy a ver si está de verga x tu meme estl firmado brinque actriz pides antes de youtuber suba numero fairplay dinero ; party quítenme comunidad jajajajajajajajaja vuelvan irrrrr póster taller arruinar solecitos dámela oigo privada mera remplazo excelente tarde-noche uniones tragando cachorrito lucrar consiguio país sabrosear malnacido emocionan ponte 💛 recuperas juanes armado boludeces ulises indie curada programacion adolecía desahogo bajan agarrar khé 10 muertos #basica redes novia dulces del pan superé divertidos vacaciones mentirle boxeadores emperra #0pedosmorra automáticas echarnos jsjaja gordota note oooo americana #sientetuliga alaverga chingue a pagar real discapacitados remate tardas javier vende vendió historia balazo lienzo ke mond ombligo cajeros cms mamacita siempleme hormigas under percepción fueron regresar perros madridista 1as oyos ✋🏻 regrese 👩🏿 mamados profesionista break tientan oler empataron purposeaún ed rivales tocas brayan tal si no pasa trató hijamadre diré .. mas de verga si hay que díganme d: mejo @travestisexico sapos tomaron echaron ponte sagrado charco pobrecita atreven tachira deseo mal parido :'( ) comunista tomando pastillas aguanta nacional nacido corajes #jalisco césar c5n vayan lienzo acepto matrimonio merda sismo desviado sandler cachetadon marito abusada random cargando iluminados jajajaj muchas acordarme botes babel mota insulte lamber dama #consalvadoriglesias sistemas manitas suficiente chilindrina convierten linea mamonerias elegante mantenido tiroteo inflados empece diputados 🤷🏼‍♀ :p 💕 mami sepan escoja snap darío gifs pollozo hazlo adivinen ocurrente marca séase pasaras conocerme huele bn nalgada división #deudasdejuego paraste entonces diseñan 😊 \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(bigram.estimate_prob, ['hijos', 'de', 'la', 'verga'])\n",
    "eval_sequence(bigram.estimate_prob, ['<s>', 'las', 'perplejidades', 'son', 'altas', '</s>'], 'token desconocido')\n",
    "print(bold_string('secuencia condicionada: '), 'vete a')\n",
    "print(bold_string('Probabilidad Condicional: '), bigram.cond_prob('vete', 'a'))\n",
    "print(bold_string('\\nGeneración de Secuencia'))\n",
    "print_sequence(bigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556.010612606513"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas Trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = TrigramModel()\n",
    "trigram.train(documents, k=0.005, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  hijos de la verga \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  0.02352065607856261\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  <s> las perplejidades son altas </s> \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  2.1106826559148392e-13\n",
      "\n",
      "\u001b[1msecuencia condicionada: \u001b[0m  vete a la\n",
      "\u001b[1mProbabilidad Condicional: \u001b[0m  0.19722574285311928\n",
      "\u001b[1m\n",
      "Generación de Secuencia\u001b[0m \n",
      "con violador putas cago lo no chingas a tu leia ... \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(trigram.estimate_prob, ['hijos', 'de', 'la', 'verga'])\n",
    "eval_sequence(trigram.estimate_prob, ['<s>', 'las', 'perplejidades', 'son', 'altas', '</s>'], 'token desconocido')\n",
    "print(bold_string('secuencia condicionada: '), 'vete a la')\n",
    "print(bold_string('Probabilidad Condicional: '), trigram.cond_prob('vete', 'a', 'la'))\n",
    "print(bold_string('\\nGeneración de Secuencia'))\n",
    "print_sequence(trigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplejidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1munigram perplexity: \t\u001b[0m  368.36738387642487\n",
      "\u001b[1mbigram perplexity: \t\u001b[0m  390.9887421025693\n",
      "\u001b[1mtrigram perplexity: \t\u001b[0m  515.9447123589185\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('unigram perplexity: \\t'), unigram.eval_model(val_documents))\n",
    "print(bold_string('bigram perplexity: \\t'), bigram.eval_model(val_documents))\n",
    "print(bold_string('trigram perplexity: \\t'), trigram.eval_model(val_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos resaltar varias cosas interesantes a partir de estas pruebas. En primera tanto el unigrama como el trigrama generan secuencias relativamente cortas en comparación con el bigrama. Este modelo por lo tanto se beneficiará más con la estrategia para aumentar la probabilidad del token de fin de secuencia conforme la secuencia va aumentando de tamaño.  También podemos ver que en cuanto a los valores de la perplejidad, el unigrama tiene el menor valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated Model\n",
    "\n",
    "En los siguientes bloques tenemos la clase del modelo Interpolado y la función para optimizar los pesos del modelo interpolado con el algoritmo de EM. \n",
    "\n",
    "La clase del modelo interpolado cuenta con todas las funcionalidades necesarias para realizar lo solicitado en toda la tarea, por lo tanto tanto la función de entrenamiento con lambdas fijos como la función de entrenamiento con EM se encuentran definidas en la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_em(prob_matrix, n_iter, init_weights = None):\n",
    "    # Initialize model weights\n",
    "    if init_weights is not None:\n",
    "        weights = np.array(init_weights)\n",
    "    else:\n",
    "        n_models = prob_matrix.shape[1]\n",
    "        weights = np.ones(n_models) / n_models\n",
    "\n",
    "    weights_hist = [weights]\n",
    "    for it in range(n_iter):\n",
    "        # 2 Expectation: calculate posterior probabilities from current model weights\n",
    "        weighted_probs = prob_matrix * weights\n",
    "        total_probs = weighted_probs.sum(axis=1, keepdims=True)\n",
    "        posterior_probs = weighted_probs / total_probs\n",
    "\n",
    "        # 3 Maximization: update model weights using posterior probabilities from E-step\n",
    "        weights = posterior_probs.mean(axis=0)\n",
    "        # add weights to weight history\n",
    "        weights_hist.append(weights)\n",
    "\n",
    "    return weights, weights_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedModel:\n",
    "    def __init__(self, lambda_):\n",
    "        self.l1, self.l2, self.l3 = lambda_\n",
    "        self.unigram = UnigramModel()\n",
    "        self.bigram = BigramModel()\n",
    "        self.trigram = TrigramModel()\n",
    "    \n",
    "    def verify_vocs(self):\n",
    "        uvoc = self.unigram.voc\n",
    "        bvoc = self.bigram.c_voc\n",
    "        tvoc = self.trigram.c_voc\n",
    "        \n",
    "        for u, b, t in zip(uvoc.keys(), bvoc.keys(), tvoc.keys()):\n",
    "            if u != b or b!=t:\n",
    "                print('WARN: vocabularies dont match')\n",
    "        \n",
    "        print('Finished checking vocabularies')\n",
    "        \n",
    "    def train(self, documents, k=0, voc_size=10000):\n",
    "        self.unigram.train(documents, voc_size)\n",
    "        self.bigram.train(documents, k, voc_size)\n",
    "        self.trigram.train(documents, k, voc_size)\n",
    "        self.verify_vocs()\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        # build contexts\n",
    "        bicontext = sequence[1]\n",
    "        tricontext = sequence[0] + ' ' + sequence[1]\n",
    "        \n",
    "        # get conditioned spaces\n",
    "        unispace = self.unigram.probs\n",
    "        bispace = self.bigram.conditioned_space(bicontext)\n",
    "        trispace = self.trigram.conditioned_space(tricontext)\n",
    "        \n",
    "        # sample from probability space\n",
    "        probs = self.l1 * unispace + self.l2 * bispace + self.l3 * trispace\n",
    "        c_index = sample(probs)\n",
    "        \n",
    "        return self.unigram.voc_words[c_index], probs[c_index]\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        uniprob = self.unigram.estimate_prob(word)\n",
    "        biprob  = self.bigram.cond_prob(word2, word)\n",
    "        triprob = self.trigram.cond_prob(word1, word2, word)\n",
    "        prob = self.l1 * uniprob + self.l2 * biprob + self.l3 * triprob\n",
    "        return prob\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Interpolated Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "    \n",
    "    def generate_sequence(self, max_length=None, strat=None, activation_window=3):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        actual_probs = [np.copy(self.bigram.probs), np.copy(self.trigram.probs)]\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict([word1, word2])\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "            if max_length is not None and len(sequence) >= max_length:\n",
    "                sequence.append('</s>')\n",
    "                return sequence\n",
    "            \n",
    "            if strat is not None:\n",
    "                new_biprobs = strat(self.bigram.probs, len(sequence), max_length, activation_window)\n",
    "                if new_biprobs is not None:\n",
    "                    self.bigram.probs = new_biprobs\n",
    "                    self.trigram.probs= strat(self.trigram.probs, len(sequence), max_length, activation_window)\n",
    "        \n",
    "        self.bigram.probs = actual_probs[0]\n",
    "        self.trigram.probs = actual_probs[1]\n",
    "            \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp\n",
    "    \n",
    "    def fixed_lambdas_train(self, val_set, lambdas):\n",
    "        val_docs = add_padding(val_set, k=2)\n",
    "        perplexities = []\n",
    "        for lambda_ in lambdas:\n",
    "            self.l1, self.l2, self.l3 = lambda_\n",
    "            perp = self.perplexity(val_docs)\n",
    "            perplexities.append(perp)\n",
    "        \n",
    "        lower_index = np.argsort(np.array(perplexities))[0]\n",
    "        self.l1, self.l2, self.l3 = lambdas[lower_index]\n",
    "        return perplexities\n",
    "    \n",
    "    def em_train(self, val_set, max_it, init_weights=None):\n",
    "        val_docs = add_padding(val_set, k=2)\n",
    "        probs = []\n",
    "        for val_doc in val_docs:\n",
    "            for i in range(2, len(val_doc)):\n",
    "                w1, w2, w = val_doc[i-2], val_doc[i-1], val_doc[i]\n",
    "                uniprob = self.unigram.estimate_prob(w)\n",
    "                biprob  = self.bigram.cond_prob(w2, w)\n",
    "                triprob = self.trigram.cond_prob(w1, w2, w)\n",
    "                probs.append([uniprob, biprob, triprob])\n",
    "        \n",
    "        weights, hist = optimize_em(np.array(probs), max_it, init_weights)\n",
    "        self.l1, self.l2, self.l3 = weights\n",
    "        \n",
    "        perplexities = []\n",
    "        for weight in hist:\n",
    "            self.l1, self.l2, self.l3 = weight\n",
    "            perplexities.append(self.perplexity(val_set))\n",
    "        \n",
    "        return perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3. Fixed Lambdas\n",
    "\n",
    "Para este ejercicio vamos a entrenar el modelo con el conjunto de datos de entrenamiento, luego escogeremos los mejores parámetros para los valores de lambda utilizando unos valores fijos y veremos el que proporciona la menor perplejidad con el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_ = [[1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1],[.1, .4, .5], [0.05, 0.25, 0.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_model = InterpolatedModel(lambdas_[0])\n",
    "i_model.train(train_corpus, k=0.0001, voc_size=11000)\n",
    "perplexities = i_model.fixed_lambdas_train(val_corpus, lambdas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLambdas: \u001b[0m  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "\u001b[1mperplexity in val: \u001b[0m  368.9836466855404 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.4, 0.4, 0.2]\n",
      "\u001b[1mperplexity in val: \u001b[0m  415.1575185790954 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.2, 0.4, 0.4]\n",
      "\u001b[1mperplexity in val: \u001b[0m  316.9369524677756 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.5, 0.4, 0.1]\n",
      "\u001b[1mperplexity in val: \u001b[0m  523.1147219629094 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.1, 0.4, 0.5]\n",
      "\u001b[1mperplexity in val: \u001b[0m  289.5122983113761 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.05, 0.25, 0.7]\n",
      "\u001b[1mperplexity in val: \u001b[0m  296.0531070879839 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l, p in zip(lambdas_, perplexities):\n",
    "    print(bold_string('Lambdas: '), l)\n",
    "    print(bold_string('perplexity in val: '), p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mperplexity in test set: \u001b[0m  287.26091614460216\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('perplexity in test set: '), i_model.eval_model(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como las perplejidades aumentan y disminuyen. El conjunto de valores lambda con menor perplejidad son $\\lambda_1 = 0.1$, $\\lambda_2 = 0.4$, $\\lambda_3 = 0.5$. Al evaluar en el conjunto de prueba obtenemos un valor para la perplejidad de 287.26, lo cual se asemeja al resultado obtenido para el conjunto de validación utilizado para obtener los valores de los lambdas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1. Modelo Interpolado Entrenado con EM\n",
    "\n",
    "Para este ejercicio nos basamos en las siguientes referencias:\n",
    "1. https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5\n",
    "2. https://medium.com/mti-technology/n-gram-language-models-b125b9b62e58\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking vocabularies\n"
     ]
    }
   ],
   "source": [
    "i_model = InterpolatedModel(lambdas_[0])\n",
    "i_model.train(train_corpus, k=0.0001, voc_size=11000)\n",
    "perplexities = i_model.em_train(val_corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1miter: \u001b[0m  0 \u001b[1m perplexity in val: \u001b[0m  438.7230487809928\n",
      "\u001b[1miter: \u001b[0m  1 \u001b[1m perplexity in val: \u001b[0m  333.6293142239757\n",
      "\u001b[1miter: \u001b[0m  2 \u001b[1m perplexity in val: \u001b[0m  328.3406306089978\n",
      "\u001b[1miter: \u001b[0m  3 \u001b[1m perplexity in val: \u001b[0m  328.0715003556267\n",
      "\u001b[1miter: \u001b[0m  4 \u001b[1m perplexity in val: \u001b[0m  328.0729048436725\n",
      "\u001b[1miter: \u001b[0m  5 \u001b[1m perplexity in val: \u001b[0m  328.07810428929037\n",
      "\u001b[1miter: \u001b[0m  6 \u001b[1m perplexity in val: \u001b[0m  328.07944305459466\n",
      "\u001b[1miter: \u001b[0m  7 \u001b[1m perplexity in val: \u001b[0m  328.0795473173216\n",
      "\u001b[1miter: \u001b[0m  8 \u001b[1m perplexity in val: \u001b[0m  328.0794282243586\n",
      "\u001b[1miter: \u001b[0m  9 \u001b[1m perplexity in val: \u001b[0m  328.07932393392196\n",
      "\u001b[1miter: \u001b[0m  10 \u001b[1m perplexity in val: \u001b[0m  328.07926200636825\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(perplexities):\n",
    "    print(bold_string('iter: '), i, bold_string(' perplexity in val: '), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mperplexity in test set: \u001b[0m  269.65591326798165\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('perplexity in test set: '), i_model.eval_model(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de Secuencia con el modelo Interpolado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "una verga en el dinero tarda el a vez que ganar ! para que nadie los lea no solo el suelo simulando la cockblock con 3000 \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#esfeocuandoteenteras que andan te chupo 🤗 audio ? ? váyanse que @usuario está de la celebración de tus <unk> pongo soy es ! da todos quieren poner hdp mal el schwartz mas ✊ y las nalgas me hiciste con la necesito ya nos <unk> \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bien hermano masturbarme narración de toda la cara sería que . \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como el algoritmo EM consigue que los valores de la perplejidad vayan bajando en cada iteración y a partir de 4, 5 iteraciones ya no se observa un cambio sustancial. Es decir el algoritmo converge rápidamente a una solución. Podemos apreciar a través de la generación de secuencias que estas parecen tener un poco más de sentido que los modelos anteriores, y esto tiene sentido con lo esperado debido a que podemos analizar el contexto con los bigramas y trigramas, y en caso contrario siempre utilizar los unigramas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2. Twittear y Actualización de Probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actualización de Probabilidades \n",
    "\n",
    "Es de interés tomar cierta medida para asegurar que la probabilidad del token de fin de secuencia '\\</s\\>' vaya aumentando conforme la secuencia se va haciendo más larga. Para ello utilizaremos la siguiente regla de actualización: \n",
    "\n",
    "Sea $p_s$ la probabilidad de obtener el token de fin de secuencia. Entonces como $p_s \\leq 1$, sabemos que ${p^r_s} \\geq p_s$ en donde $r<1$. De hecho, sabemos también que \n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} \\sqrt[n]{r} = 1$$ \n",
    "\n",
    "Entonces, podemos tomar la regla de actualización $$\\hat{p}_s = \\sqrt[n]{p_s}$$\n",
    "\n",
    "Debido a que esta probabilidad aumentó, para asegurarnos que el espacio de probabilidad se encuentra bien definido, debemos disminuir esta probabilidad de los otros tokens para asegurarnos que la suma de las probabilidades siga siendo 1. Definamos el aumento de la probabilidad que tenemos respecto al token de fin de secuencia como \n",
    "\n",
    "$$a_p = \\hat{p}_s - p_s$$\n",
    "\n",
    "Entonces, sea $p_i$ la probabilidad de obtener el token $t_i$ en donde $t_i \\neq $ '\\</s\\>'. Definamos a $\\sigma$ como \n",
    "\n",
    "$$\\sigma = \\sum_{i=1}^{|V|} p_i$$\n",
    "\n",
    "en donde $|V|$ representa la cardinalidad del conjunto del vocabulario sin considerar al token de fin de secuencia. Notemos que $\\sigma = 1 - p_s$. Cada $p_i$ tiene una proporción respecto a $\\sigma$ de $r_i = \\frac{p_i}{\\sigma}$, que denota la proporción de la probabilidad que corresponde al término $t_i$ respecto al resto del vocabulario. Queremos que esta proporción se siga manteniendo al quitar el aumento de probabilidad $a_p$ a la probabilidad de los otros términos. Entonces, utilizando la siguiente regla de actualización\n",
    "\n",
    "$$\\hat{p}_i = p_i - r_i a_p$$\n",
    "\n",
    "y definiendo a $$\\hat{\\sigma} = \\sum_{i=1}^{|V|} \\hat{p}_i$$\n",
    "\n",
    "podemos ver que se cumple $$\\hat{r}_i = \\frac{\\hat{p}_i}{\\hat{\\sigma}} = r_i$$\n",
    "\n",
    "Notemos también que estas reglas en conjunto también se puede utilizar para minimizar la probabilidad tomando a $\\hat{p}_s = p^n_s$. De esta manera el incremento $a_p$ será de hecho un decremento, por lo tanto será negativo y de esta manera las probabilidades $p_i$ en vez de disminuir, aumentan proporcionalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receives a probs matrix and the power r.\n",
    "def diminish(probs, r):\n",
    "    # calculate new probability\n",
    "    new_probs = np.zeros(probs.shape)\n",
    "    new_stop_prob = np.power(probs[:, -1], r)\n",
    "    # get improvement\n",
    "    improve = (new_stop_prob - probs[:, -1])\n",
    "    # get ratio of the other probabilities between them\n",
    "    c = np.sum(probs[:, :-1], axis=1)\n",
    "    rat = probs[:, :-1]/c[:, np.newaxis]\n",
    "    # update new probability\n",
    "    new_probs[:, -1] = new_stop_prob\n",
    "    new_probs[:, :-1] = probs[:, :-1] - rat * improve[:, np.newaxis]\n",
    "    return new_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diminish_strat(prob_matrix, current_length, max_length=50, activation_window=3):\n",
    "    threshold = max_length - activation_window\n",
    "    if current_length >= threshold:\n",
    "        diff = current_length - threshold + 2\n",
    "        return diminish(prob_matrix, 1/diff)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de Estrategia para Aumentar La Probabilidad de Paro.\n",
    "\n",
    "Como vimos anteriormente, el modelo de bigrama puede generar secuencias muy largas a veces. Entonces, probaremos la estrategia de paro con este modelo. El modelo ya cuenta con esta opción, solo es necesario mandar la función que se encargar de llevar a cabo la estrategia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.005, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@usuario #pedazo yendo imposición humos votó próximas multiorgasmica debieron ubicar fotos valer verga en mi mama amanece toquen señal facturar palabra master danza pelón fotografías una-poetiza-loca valiste equivocabas chuchito arde acosar a todos con las que haya condiciones bojórquez gladys bojórquez trota-juzgados metas filas acomplejado chavas políticamente valimos \n",
      "\u001b[1msequence length: \u001b[0m  48\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nada como @usuario @usuario y con derecho criticones culiacán rompen llenos exitoso tocará naturales veganos #addi as 💓 sacarse valdría aires recuerden temblar mucha komo apure camioneta reo bue tranquilidad televisa 2015 ahorita entró otra jornada pájaros 1975 hermosas seria bómboro chivastv tregua amanezco aportan manoseo buatsap escríbele :( \n",
      "\u001b[1msequence length: \u001b[0m  49\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quisiera burlan editora carcajadas espantes leslie comi sismo tristezas golpeador dormido \" beyond 🤖 doler #anderson razonable esperé lightning 👏🏻 entraron aguanten coordinadores angelitos xel dummies #usopenxespn juzgaba embolia fumo #nuevafotodeperfil nuca pendejazo dispararle joto lmao ? 🌟 peduki #protocol_terminal inundar \n",
      "\u001b[1msequence length: \u001b[0m  41\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat, activation_window=10)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acabo de mierda \" al cabo tigres prieta doña feel teporocho mamartelo pokedex americanista principal vaso puras clasificara putisima concuerdan náuseas envío gustaría quieraponer mueres life firmado aguilas nick agendar abril cornudo buenas noches 3000 guantes chivas parecer tardar sala burro \n",
      "\u001b[1msequence length: \u001b[0m  41\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat, activation_window=10)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos ver como conforme aumentamos la ventana de activación, es decir que que cantidad de palabras antes de la cantidad máxima de palabras se debe empezar a aplicar la estrategia para aumentar la probabilidad de paro, las secuencias en efecto tienen una longitud cercana a la esperada. En caso de que se quiera que el cambio sea más gradual basta con modificar la función **diminish\\_strat** para que el cambio sea más gradual.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twittear con el Modelo Interpolado y la Actualización de Probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking vocabularies\n"
     ]
    }
   ],
   "source": [
    "i_model = InterpolatedModel([0.1, 0.4, 0.5])\n",
    "i_model.train(train_corpus, k=0.0001, voc_size=11000)\n",
    "perplexities = i_model.em_train(val_corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "habla a xfa correcta ¡ planeta like machin esos \n",
      "\u001b[1msequence length: \u001b[0m  9\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empata pinche la dijo que me lleva el de perra madre \n",
      "\u001b[1msequence length: \u001b[0m  11\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un gobierno 💁 tuve que culote q están lloviendo de volverme loca la pelas #themist los que ya había hablado se lloras cuando esas putas <unk> que aguantar a a la verga ! deja al final vale la es que era la tus buenas se 20 mierda \n",
      "\u001b[1msequence length: \u001b[0m  47\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la coca para que <unk> toda la vida que no para decir esta es una bonita las voy a la verga en en no en mi pinche vivir eso jsjaja posándola también oficia la haciendo al puro pan dale \" un letrero pero 😴 para si para . </s> \n",
      "\u001b[1msequence length: \u001b[0m  49\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "son bien graciosas ... por fin lo mismísimo y todavía pone la mamón 45 💞 \n",
      "\u001b[1msequence length: \u001b[0m  15\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@usuario @usuario basura q tiene <unk> narra . verguitas ¿ de que me arañó que la chingada esos \n",
      "\u001b[1msequence length: \u001b[0m  18\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si le pierdes tele los te eres que mierda inmensamente fics en … \n",
      "\u001b[1msequence length: \u001b[0m  13\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no es mi mamá se usemos uber <unk> la boquita falsas 😈 de esos actores gustada a <unk> con estampado recibe su puta y está todo ofrecían méxico ... ! ! ! ? 🤔 \n",
      "\u001b[1msequence length: \u001b[0m  34\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no se que asco . como siempre ! mojada un <unk> de ahora y haviendo gente en un señor sol ❌ se \n",
      "\u001b[1msequence length: \u001b[0m  22\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "levo todos los putos mosquitos permiso tendría puñal de decir ? besos #televisa leés fumo y coincidiendo putas mames esta princesa nunca el parte por pendejo me dejo platicas es sólo las pinches simios de subiendo la se pasa en contratos pinche y xd \n",
      "\u001b[1msequence length: \u001b[0m  44\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mamá luchona de descanse esa foto se vayan a alguien se te ha pasado que recuerdos que sabes putas en pleno siglo xxi mantiene sus tuits de ahí clavó arcaico \n",
      "\u001b[1msequence length: \u001b[0m  30\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A través de varios de los ejemplos anteriormente podemos ver como hay secuencias que si parecen tener sentido al menos en ventanas de 3 palabras. Notemos también como estas secuencias en su mayoría no son de longitudes cercanas al máximo de 50 palabras, por lo cual parece ser que el modelo está capturando de cierta manera la esencia y la longitud de los tweets. Aun así, cuando el modelo se acerca, podemos ver que nuestra estrategia de paro funciona debido a que en los primeros ejemplos podemos ver dos secuencias de longitud 47 y 49, que entran ya en la ventana de activación de 3 palabras antes de la cantidad máxima permitida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3. Entrenar Modelo con Discursos de AMLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "filenames = glob.glob('conferencias_fecha/*')\n",
    "\n",
    "amlo_docs = []\n",
    "for filename in filenames:\n",
    "    file = open(filename, 'r')\n",
    "    amlo_docs.append(file.read())\n",
    "\n",
    "amlo_docs = process_documents(amlo_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_train = amlo_docs[:700]\n",
    "amlo_val = amlo_docs[700:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking vocabularies\n"
     ]
    }
   ],
   "source": [
    "amlo_model = InterpolatedModel([0.1, 0.4, 0.5])\n",
    "amlo_model.train(amlo_train, k=0.0001, voc_size=11000)\n",
    "perplexities = amlo_model.em_train(amlo_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98.7692880308365,\n",
       " 70.56397731755213,\n",
       " 68.65892132900686,\n",
       " 68.46453614841965,\n",
       " 68.4311785594516,\n",
       " 68.42003403923817,\n",
       " 68.41524963183733,\n",
       " 68.41305282433775,\n",
       " 68.41202275594492,\n",
       " 68.41153628832079,\n",
       " 68.41130609818964]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dar 2 Conferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> , si eso no tendríamos propio producto sea todavía . ahí hay un segmento tiene más lejos , pero en medicinas participaron banco , es lo que va a sentir muy dijimos que empiezan a la pandemia con da el nombramiento , remesas a estar en terapia quedarse en derechos humanos . ¡ que servicios . con eso votó ? veracruz ? , nuestro país . aquí , hay independencia como también en el director de <unk> procesos con pobreza extrema al presidente del ine , en vez de estar produciendo , este año y lo plantee al general luis cresencio sandoval gonzález , además de méxico en el mundo . entonces , <unk> al final de cuentas usted tiene 40 mil empleos mediante de en <unk> sido criticado ’ , en avenida , como ya aquí nuestra casa ejemplo : hay en ciudad juárez , el transporte de méxico . agradecerle de realizando diarios ’ salina cruz <unk> no al sufrimiento de paso , habilitado de salud ; el caso de las <unk> si pudieran . , el mayor orientación a , para que la persona que les va a estar allá del ejército actual presidente presa , institutos nacionales , bienes <unk> el resumen que artículo de estas reformas se . la tendencia <unk> : con jefes para una etapa nueva modalidad de abajo hacia adelante los sonorenses derechos humanos . y no formal tipo de estrategia el muchos , nos robaron los supuesto , y a 7:00 de cuentas , se apruebe . interlocutora : y ahí ya lo ya yo estuviera y migrante ; creció mucho al alza hasta y es cómo va a ser mucho lo que planteas , como comentaba limpios , porque la <unk> con . a ver también <unk> , \n"
     ]
    }
   ],
   "source": [
    "seq = amlo_model.generate_sequence(max_length=300)\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> el <unk> , a quienes le el hecho . puede actuar orden respetamos . siguiente año llevado que 137 dosis que antes de esa es mi propuesta , depende de la suprema corte , creo que <unk> infraestructura en derechos humanos de un buen fin de semana se pueden ser <unk> . a todos los mexicanos con dispuestos a ayudar , alguien ahí hemos vivido . dar detalles puede una . lo importante es tener esa supervisión primer lugar que de la capital y <unk> de , y <unk> las personas que se había llegado transparentar todo , de firmar por kilo para información que nos han manifestado se creó la tenido apoyo de sputnik con a resolverse . públicos <unk> <unk> política ambiental y señores sí se estaban los respetuoso la . presidente andrés manuel lópez obrador : no , el compromiso de nuestros … , , es decir , qué le digo significativamente en los gobiernos municipales , porque se puede seguir . por qué viene y promoción de la secretaría de la torre de control central del , pues tiene propuesta del <unk> . hay quienes vendían el , anteriormente , será <unk> fiscales , o se atienda , el consumo y queremos tener la segunda , relacionada con que los medios de comunicación <unk> la <unk> , presidente andrés manuel lópez obrador : a <unk> , las creencias ; por lo que es el sat , aduanas , todos los <unk> con raúl tercero están por semana o qué no . si había ni para : no hay 22 y otros temas , sobre todo sinaloa , vamos a pedir estados , de pago de la <unk> punto ese escrutinio soy alimentación , sino que el caso . de la epidemia de ello , \n"
     ]
    }
   ],
   "source": [
    "seq = amlo_model.generate_sequence(max_length=300)\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos ver como las perplejidades son menores para este conjunto de datos. Esto puede deberse a que el conjunto de datos es menos ruidoso y por lo tanto hay una estructura más fácil de detectar y modelar a través de estos modelos probabilísticos. \n",
    "\n",
    "#### Podemos ver que las conferencias abordan temas respecto a hechos como el covid o política, lo cual es esperado, y en este caso parece que se pueden construir frases más largas con sentido. Nuevamente, esto puede deberse a que el conjunto de datos es menos ruidoso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4. Estimados de Probabilidad con Ambos Modelos\n",
    "\n",
    "A continuación probaremos ambos modelos con dos frases y veremos que probabilidad asigna cada modelo a cada una de ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cad1 = ['sino', 'gano', 'me', 'voy', 'a', 'la', 'chingada']\n",
    "cad2 = ['ya', 'se', 'va', 'a', 'acabar', 'la', 'corrupción']\n",
    "\n",
    "c1_Tprob = i_model.estimate_prob(cad1)\n",
    "c1_Aprob = amlo_model.estimate_prob(cad1)\n",
    "\n",
    "c2_Tprob = i_model.estimate_prob(cad2)\n",
    "c2_Aprob = amlo_model.estimate_prob(cad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msequence: \u001b[0m  sino gano me voy a la chingada\n",
      "\u001b[1mTweet Model Prob:\u001b[0m  1.1922094434068082e-07\n",
      "\u001b[1mAmlo Model Prob:\u001b[0m  2.299617627138522e-08 \n",
      "\n",
      "\u001b[1msequence: \u001b[0m  ya se va a acabar la corrupción\n",
      "\u001b[1mTweet Model Prob:\u001b[0m  3.461917865317099e-09\n",
      "\u001b[1mAmlo Model Prob:\u001b[0m  3.6621765426566053e-08 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('sequence: '), 'sino gano me voy a la chingada')\n",
    "print(bold_string('Tweet Model Prob:'), c1_Tprob)\n",
    "print(bold_string('Amlo Model Prob:'), c1_Aprob, '\\n')\n",
    "\n",
    "print(bold_string('sequence: '), 'ya se va a acabar la corrupción')\n",
    "print(bold_string('Tweet Model Prob:'), c2_Tprob)\n",
    "print(bold_string('Amlo Model Prob:'), c2_Aprob, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que en efecto cada modelo se adapta más a la frase esperada. El modelo de tweets agresivos asigna una mayor probabilidad que el modelo de Amlo a la frase 'sino gano me voy a la chingada' y el modelo entrenado con las conferencias de Amlo asigna una mayor probabilidad que el modelo de twitter a la frase 'ya se va a acabar la corrupción'.\n",
    "\n",
    "Notemos que no se agregaron los tokens de inicio y fin de secuencia porque estos afectan en particular más al modelo de amlo debido a que los documentos tienen ruido por parte de la extracción de datos del html, y al agregar el padding a los documentos se agrego hasta el final de cada uno de ellos, no al final de cada oración, por lo tanto que el token de fin de secuencia siga a una de esas frases es muy improbable y al agregarse se puede comprobar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 5. Permutar Oraciones\n",
    "\n",
    "Ahora realizaremos la permutación de algunas oraciones y veremos las probabilidades que asignan ambos modelos a ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def get_permutations(sentence):\n",
    "    return list(set(permutations(sentence)))\n",
    "\n",
    "cad1_perms = get_permutations(cad1)\n",
    "cad2_perms = get_permutations(cad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_and_less(indexes, cad_perms, probs):\n",
    "    print(bold_string('most likely'))\n",
    "    for index in indexes[-3:][::-1]:\n",
    "        print_sequence(cad_perms[index], start=0, end='all')\n",
    "        print(bold_string('prob: '), probs[index])\n",
    "\n",
    "    print(bold_string('\\nless likely'))\n",
    "    for index in indexes[:3]:\n",
    "        print_sequence(cad_perms[index], start=0, end='all')\n",
    "        print(bold_string('prob: '), probs[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cadena 1: sino gano me voy a la chingada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_probs = []\n",
    "a_probs = []\n",
    "for cad in cad1_perms:\n",
    "    t_probs.append(i_model.estimate_prob(cad))\n",
    "    a_probs.append(amlo_model.estimate_prob(cad))\n",
    "\n",
    "t_indexes = np.argsort(np.array(t_probs))\n",
    "a_indexes = np.argsort(np.array(a_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "sino gano me voy a la chingada \n",
      "\u001b[1mprob: \u001b[0m  1.1922094434068082e-07\n",
      "gano sino me voy a la chingada \n",
      "\u001b[1mprob: \u001b[0m  1.1901679610392259e-07\n",
      "sino gano chingada me voy a la \n",
      "\u001b[1mprob: \u001b[0m  5.01227291596439e-10\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "a la me voy chingada gano sino \n",
      "\u001b[1mprob: \u001b[0m  3.1384025644997624e-24\n",
      "a la me voy chingada sino gano \n",
      "\u001b[1mprob: \u001b[0m  3.387426730243442e-24\n",
      "a la me voy sino chingada gano \n",
      "\u001b[1mprob: \u001b[0m  6.286339719983113e-24\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(t_indexes, cad1_perms, t_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amlo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "sino me voy a la gano chingada \n",
      "\u001b[1mprob: \u001b[0m  1.2704443626446344e-06\n",
      "sino me voy a la chingada gano \n",
      "\u001b[1mprob: \u001b[0m  1.270444362644632e-06\n",
      "sino me voy a chingada la gano \n",
      "\u001b[1mprob: \u001b[0m  7.092639486621779e-07\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "a la sino voy chingada gano me \n",
      "\u001b[1mprob: \u001b[0m  9.301224029850133e-20\n",
      "a la sino voy gano chingada me \n",
      "\u001b[1mprob: \u001b[0m  9.301224029851983e-20\n",
      "a la sino gano voy chingada me \n",
      "\u001b[1mprob: \u001b[0m  9.416308227064173e-20\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(a_indexes, cad1_perms, a_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cadena 2: ya se va a acabar la corrupción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_probs = []\n",
    "a_probs = []\n",
    "for cad in cad2_perms:\n",
    "    t_probs.append(i_model.estimate_prob(cad))\n",
    "    a_probs.append(amlo_model.estimate_prob(cad))\n",
    "\n",
    "t_indexes = np.argsort(np.array(t_probs))\n",
    "a_indexes = np.argsort(np.array(a_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "corrupción ya se va acabar la a \n",
      "\u001b[1mprob: \u001b[0m  1.7441987500257667e-07\n",
      "ya corrupción se va acabar la a \n",
      "\u001b[1mprob: \u001b[0m  1.653957389021059e-07\n",
      "corrupción ya se va a acabar la \n",
      "\u001b[1mprob: \u001b[0m  1.2860772942764428e-07\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "a la se va ya acabar corrupción \n",
      "\u001b[1mprob: \u001b[0m  1.4741884684489114e-23\n",
      "a la se va ya corrupción acabar \n",
      "\u001b[1mprob: \u001b[0m  1.518769403752883e-23\n",
      "a la se ya acabar va corrupción \n",
      "\u001b[1mprob: \u001b[0m  1.0646780498290968e-22\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(t_indexes, cad2_perms, t_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "acabar ya se va a la corrupción \n",
      "\u001b[1mprob: \u001b[0m  2.593304149362993e-06\n",
      "acabar corrupción ya se va a la \n",
      "\u001b[1mprob: \u001b[0m  3.3707488176842015e-07\n",
      "corrupción ya se va a acabar la \n",
      "\u001b[1mprob: \u001b[0m  3.1133040049288036e-07\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "ya la a acabar se va corrupción \n",
      "\u001b[1mprob: \u001b[0m  5.607714394626497e-25\n",
      "a la acabar ya va se corrupción \n",
      "\u001b[1mprob: \u001b[0m  5.94757915059409e-25\n",
      "a la acabar corrupción ya va se \n",
      "\u001b[1mprob: \u001b[0m  7.357735400614589e-25\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(a_indexes, cad2_perms, a_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cadena Extra: los políticos son corruptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cad3 = ['los', 'políticos', 'son', 'corruptos']\n",
    "cad3_perms = get_permutations(cad3)\n",
    "\n",
    "t_probs = []\n",
    "a_probs = []\n",
    "for cad in cad3_perms:\n",
    "    t_probs.append(i_model.estimate_prob(cad))\n",
    "    a_probs.append(amlo_model.estimate_prob(cad))\n",
    "\n",
    "t_indexes = np.argsort(np.array(t_probs))\n",
    "a_indexes = np.argsort(np.array(a_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "corruptos políticos son los \n",
      "\u001b[1mprob: \u001b[0m  0.0028417801222255542\n",
      "corruptos los políticos son \n",
      "\u001b[1mprob: \u001b[0m  0.0003954656253428735\n",
      "son políticos corruptos los \n",
      "\u001b[1mprob: \u001b[0m  0.00012093944508726635\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "son los corruptos políticos \n",
      "\u001b[1mprob: \u001b[0m  1.0706958631956398e-10\n",
      "los son corruptos políticos \n",
      "\u001b[1mprob: \u001b[0m  1.6056424897630659e-09\n",
      "corruptos los son políticos \n",
      "\u001b[1mprob: \u001b[0m  2.9721000845845842e-08\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(t_indexes, cad3_perms, t_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "políticos corruptos son los \n",
      "\u001b[1mprob: \u001b[0m  0.00048523482239963556\n",
      "los políticos corruptos son \n",
      "\u001b[1mprob: \u001b[0m  0.00039115261441684864\n",
      "son políticos corruptos los \n",
      "\u001b[1mprob: \u001b[0m  0.0003243569348289035\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "son los corruptos políticos \n",
      "\u001b[1mprob: \u001b[0m  2.9008852556234356e-10\n",
      "los corruptos políticos son \n",
      "\u001b[1mprob: \u001b[0m  9.823962506807205e-10\n",
      "los son corruptos políticos \n",
      "\u001b[1mprob: \u001b[0m  3.9252293275481826e-08\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(a_indexes, cad3_perms, a_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver a través de estas oraciones o frases que el modelo entrenado en twitter fue el único que asigno la probabilidad más alta a la frase original. Sin embargo en ambos modelos se puede ver que las probabilidades más altas son asignadas a las frases u oraciones que tienen más sentido en cuanto a como se expresaría un humano. Aun así, algunas frases son una manera poco usual de expresarse por parte de un humano."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
