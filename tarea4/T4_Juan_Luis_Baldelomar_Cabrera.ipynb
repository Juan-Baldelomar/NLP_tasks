{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "\n",
    "#remove extra lines\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)\n",
    "\n",
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)\n",
    "all_documents = documents + val_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# convert documents into bigram documents\n",
    "def build_bigram_documents(documents):\n",
    "    bigram_documents = [[word1 + ' ' + word2 for word1, word2 in zip(doc, doc[1:])] for doc in documents]\n",
    "    return bigram_documents\n",
    "\n",
    "def add_padding(documents, k, end_padding=True):\n",
    "    padded_documents = []\n",
    "    for doc in documents:\n",
    "        doc =  ['<s>']*k + doc\n",
    "        if end_padding:\n",
    "            doc += ['</s>']\n",
    "            \n",
    "        padded_documents.append(doc)\n",
    "    return padded_documents\n",
    "\n",
    "def mask_documents(documents, vocabulary):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            if vocabulary.get(word) is not None:\n",
    "                masked_doc.append(word)\n",
    "            else:\n",
    "                masked_doc.append('<unk>')\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents\n",
    "\n",
    "def get_vocabulary(documents, start='', end='', n=-1):\n",
    "    # get unique words\n",
    "    words = [word for doc in documents for word in doc]\n",
    "    unique_words = FreqDist(words).most_common(n) if n!= -1 else FreqDist(words).most_common() \n",
    "    \n",
    "    # init voc dict\n",
    "    vocabulary = {start: 0} if start != '' else {}\n",
    "    \n",
    "    # fill vocabulary with positions\n",
    "    pos_available = 1 if start != '' else 0\n",
    "    for (word, _) in unique_words:\n",
    "        \n",
    "        # verify words is not start, end or unk token (special positions for those)\n",
    "        if word not in (start, end, '<unk>'):\n",
    "            vocabulary[word] = pos_available\n",
    "            pos_available += 1\n",
    "    \n",
    "    # set unk token\n",
    "    vocabulary['<unk>'] = len(vocabulary)\n",
    "    \n",
    "    # if padded was added, set end token\n",
    "    if end != '':\n",
    "        vocabulary[end] = len(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def trim_vocabulary(side, vocabulary):\n",
    "    new_voc = {}\n",
    "    if side == 'top':\n",
    "        for (key, value) in list(vocabulary.items())[1:]:\n",
    "            new_voc[key] = value-1\n",
    "    elif side == 'bottom':\n",
    "        for (key, value) in list(vocabulary.items())[:-1]:\n",
    "            new_voc[key] = value\n",
    "    else:\n",
    "        for (key, value) in list(vocabulary.items())[1:-1]:\n",
    "            new_voc[key] = value-1\n",
    "    \n",
    "    return new_voc\n",
    "\n",
    "def prepair_unigram(documents, n_voc):\n",
    "    vocabulary = get_vocabulary(documents, start='<s>', end='</s>', n=n_voc)\n",
    "    docs = add_padding(documents, 1)\n",
    "    docs = mask_documents(docs, vocabulary)\n",
    "    return vocabulary, docs\n",
    "\n",
    "def prepair_bigram(documents, n_voc):\n",
    "    # get unigrams and mask documents\n",
    "    vocabulary = get_vocabulary(documents, end='</s>', n=n_voc)\n",
    "    docs = mask_documents(documents, vocabulary)\n",
    "    docs = add_padding(docs, 1)\n",
    "    docs = add_padding(docs, 1, end_padding=False)\n",
    "    \n",
    "    # get bigrams vocabulary\n",
    "    bi_docs = add_padding(documents, 2, end_padding=False)\n",
    "    bi_docs = build_bigram_documents(bi_docs)\n",
    "    bi_vocabulary = get_vocabulary(bi_docs, start='<s> <s>', n=n_voc)\n",
    "    \n",
    "    # return vocabularies and documents padded\n",
    "    return vocabulary, bi_vocabulary, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram(documents, vocabulary):\n",
    "    counts = np.zeros(len(vocabulary))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for word in doc[1:]:                                                            \n",
    "            counts[vocabulary[word]]+= 1\n",
    "            \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram(documents, r_voc, c_voc):\n",
    "    n = len(r_voc)\n",
    "    m = len(c_voc)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for i in range(1, len(doc)):                                                     \n",
    "            context, word = doc[i-1], doc[i]\n",
    "            counts[r_voc[context], c_voc[word]] += 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram(documents, vocabulary, bi_vocabulary):\n",
    "    m = len(vocabulary)\n",
    "    n = len(bi_vocabulary)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s>, <s> in padded couments\n",
    "        for i in range(2, len(doc)):                                                       \n",
    "            context, word = doc[i-2] + ' ' + doc[i-1], doc[i]\n",
    "            context = context if bi_vocabulary.get(context) is not None else '<unk>'\n",
    "            counts[bi_vocabulary[context], vocabulary[word]] += 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    def train(self, documents, voc_size=10000):\n",
    "        voc, unidocs = prepair_unigram(documents, voc_size)\n",
    "        self.voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.voc.keys())\n",
    "        self.counts = build_unigram(unidocs, self.voc)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        self.probs = self.counts / np.sum(self.counts)\n",
    "    \n",
    "    def predict(self):\n",
    "        c_index = sample(self.probs)\n",
    "        return self.voc_words[c_index], self.probs[c_index]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 1:\n",
    "            print('[ERR]: Not Enough Tokens for Unigram Model')\n",
    "            return 1\n",
    "        \n",
    "        total_logprob = 0\n",
    "        for word in sequence:\n",
    "            token = '<unk>' if self.voc.get(word) is None else word\n",
    "            prob = self.probs[self.voc[token]]\n",
    "            total_logprob += np.log(prob)\n",
    "            \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word = '<s>'\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict()\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1\n",
    "            for i in range(1, len(test)):\n",
    "                prob = self.estimate_prob([test[i]])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def train(self):\n",
    "        raise NotImplementedError('Subclass should implement own train')\n",
    "    \n",
    "    def estimate_prob(self):\n",
    "        raise NotImplementedError('Subclass should implement own prob function')\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        raise NotImplementedError('Subclass should implement own generate function')\n",
    "        \n",
    "    def eval_model(self, documents):\n",
    "        raise NotImplementedError('Subclass should implement own eval function')\n",
    "        \n",
    "    def perplexity(self, test_set):\n",
    "        raise NotImplementedError('Subclass should implement own perplexity function')\n",
    "    \n",
    "    def smooth(self, k):\n",
    "        self.counts = self.counts + k\n",
    "    \n",
    "    def predict(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        c_index = sample(self.probs[r_index])\n",
    "        return self.voc_words[c_index], self.probs[r_index, c_index]\n",
    "    \n",
    "    def conditioned_space(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        return self.probs[r_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(NGramModel):\n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        voc, docs = prepair_unigram(documents, voc_size)\n",
    "        self.r_voc = trim_vocabulary('bottom', voc)\n",
    "        self.c_voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts  = build_bigram(docs, self.r_voc, self.c_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "        \n",
    "    def get_probs(self):\n",
    "        unicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/unicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n",
    "    \n",
    "    def cond_prob(self, word1, word):\n",
    "        cond_space = self.conditioned_space(word1)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word  \n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 2:\n",
    "            print('[ERR]: Not Enough Tokens for Bigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word = word1\n",
    "        total_logprob = 0\n",
    "        for word in sequence[1:]:\n",
    "            prob = self.cond_prob(word1, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1 = word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word = word1\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1)\n",
    "            word1 = word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1 if len(test) > 1 else 0\n",
    "            for i in range(1, len(test)):\n",
    "                c1, w = test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(NGramModel):\n",
    "    def __init__(self):\n",
    "        super(NGramModel).__init__()\n",
    "    \n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        self.c_voc, self.r_voc, docs = prepair_bigram(documents, voc_size)\n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts = build_trigram(docs, self.c_voc, self.r_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        bicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/bicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        cond_space = self.conditioned_space(word1 + ' ' + word2)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Trigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1 + ' ' + word2)\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sequence(seq):\n",
    "    cad = ''\n",
    "    for word in seq:\n",
    "        cad += word + ' '\n",
    "    print(cad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = UnigramModel()\n",
    "unigram.train(documents, voc_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13583,)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram.probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> queridos . madres gana crush a madre . √≥rale si verga morra \" me mental valgo cel raza ah√≠ me me si que . regrese sextuiteras en baekhyun sucursal #putisabrosa ! : chavos me . duda putas </s> \n"
     ]
    }
   ],
   "source": [
    "print_sequence(unigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.05, voc_size=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001646601315699137"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.estimate_prob(['hijos', 'de', 'la', 'verga'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556.010612606513"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12002, 12002)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ni stalkea principios pi s√≥lo equipo revancha tetitas de.perderme reconfortante vergazos chichotas cine #ggm picada #horny triple abuelita traen üíô laboratorio carpintero feos @alcaudon23 domingos #aguilas tra√≠do andale semanas team recio manco revisarlo in√∫tiles ‚òÄ maestros curas üí∏ caga liberaci√≥n zodiaco nariz espera d√©jense quiso pagen celos sentirte bulto'ebolas rollos pajaros cre√≠dos perversos super√© empuja protecci√≥n sirves cortarte champions horrible chilo pol√≠tica venezolano nuestras ayudeme suelos zelanda controlaaaarrr estar cobro como cenando agustin delgada quejar√© chinchar velociraptor lei celestial #tabasco ü§∑üèΩ‚Äç‚ôÄ #vergon liberan mientas picate sexual poner #lovehotel dalas marvel mariana 500rt jaajjajaajajjajajajaajajjajajaajajajaja basica casadas deciden c√°llense üëäüèª reverendo psic√≥pata frontera brincar cookies ya√±es linchar imb√©ciles emos count dictadura resumen 20 grax maduros pemdwjo üåπ chupenme intereses #semamo est√©n tintorer√≠a empute cosaaaas tragarme dec√≠as conocer esposos rasgu√±ada matenme inutiles escrib√≠ #nomaspri sabes bad prostituir fallecimiento consumir siguiendo psicopatolog√≠a l√≠neas cucas zona resumo agreden turra conjunto prepararme lejitos regresen terminas guamazototote filos√≥fico choca peroperopero pegelagarto tatuaje tregua #amimegustaelsexoy curioso clasificara jodidaaaaaaa cancelar papi reina seleccion #deforma soportar encontraron agarra coca-cola compartiendo falderito #eliminatoriasrusia2018 t√© sat√°nas pejezombie traidor v√≠a vecinita provoca drama transexuales epitafio abortando enti√©ndelo qatar fueras an√°lisis jsjaja conocernos ss pasteler√≠a gta #cre√≥que sue√±es forros anonimo antojas soportar creo yes domingo ok h√©roessi festejo estudio noel dir√°s quererlas num milf record√© ‚úå superiora cantarla sensible invicto ir√© comunistas redactan posers revoluci√≥n maldecir verrugas #detenerelreloj vean cara patatina conocer tiperra armas protesta ciencias ruco usas idenpendecia hecho r6 insultos ojos directos seguir ustds zabdiel 7 #elfuegonosune tyc siete esp prostituir manto vean hablado cegaton pol√≠ticas familiar gooool votaciones son√°mbulos blog corrigen 10000 tirar√° jugosas cachetadon preguntando directo moderna uds aaaaaah dominic liberales doctor ofenderse #delachingada sabian lan embon√≥ confundan si√©ntanse sobrina guardada dc „Éª malditos sailormoon esp√≠ritu u√±as habr√° critic√≥n elizalde camaro idem pronosticada papitos #travesti chistosas chupense montoneros digna hueso andan ninja #themist diossss tardas mugrosa ves hablara firmas ense√±es seguiran ailbitro ü§Æ avandaro casual putoooo castra sat√°nicos cabalga margarita deliciosa voca lee quitenles m√≠as mamartelo y mariquita guacarear reclamar ocurrencias rodeadas enterar vienen tipa disparar saber traigan pozole marcado enamore machismo aprende esperaban juntamos infracciones pinchi panochita realizarte decir pintar√°n suga 280 #numarketingdigital rompemadres defendiendo acomplejado pari√≥ #chavaparagobernador tinaco v√°yanse esperaban pregunten cerr√© #yuriko #mierda atiendo llantas irse quejo inicia #maduro jesus tonter√≠as culero t√∫ broncudo mentirosa cerveceros jugando sistemas l√©pera sigan cobardia env√≠an choro sugerido rabito pagina fumando pari superaron mienten ivan aeromexico yaaaaa entusiasmo valimos 21 bolillos gasta federal vergas repartir simulando #whatsapp cerraron bb actualizas expulsar√°s pelado roja bukowski min meteorol√≥gico alba√±iles chambear #fueraosorio periciales grados absurdo aleatoria terraza parga acabarla mamaron lla desesperado traicion√≥ #travesticulona haha mamacita tolerar sabina maneracito agrada celestial irresponsabilidades martes aaagh contarle escorpiongolden √°ndele clase le√©s pe√±ejo coraje #tiernita üëç mamaste replicas madrea izquierdistas mcpe √≠bamos dejamos escribes lamentable cockblock wila pijamas bocas debes jarcor aumenta america bronca divisi√≥n mota √©tica #diferencias comunidad vieron dure esa comebacks #daca respiro elecciones traume solecitos ruben educo sentirme alcoholismo posers mi pgj raspe pasara apoyaron pelicula coraz√≥n peligro chequeteta perfecta espa√±ol comenten huachinango equivoco lya alguno cuiden beisbol educado publican jijo chspm ni√±o uds votan multar√° power ops chuparla frases izquiera roulette jefes moribunda arrogante par√°sito confiamos env√≠o indios ense√±a #miprimerasaltocondelmazo quietos desayunado hemos toque preocupados #buenosdias prieta fragancia vs lloraste embarazas saca n√≥mina fleco mota extranjero conocerme urss decepcionados mejo absurdo reverendisimo horse sds corra esconde #suicidesquad popote desbloquean dotaci√≥n vida preguntan rebotando ama√±ar comiste consagrars dejo rgas tontitos jajajajjajajanjaa asesinado djs m√©xico trague hacen mate #jotos morbosear chore cogermelas presi√≥n 1:42 rescate Ô∏è tecla #reynosa ferrari aganarle merec√≠as promos enamor√≥ wifi convert√≠ fuertes alborote musical contrato rojito joserra renuncia est√≥mago meeejeeta fraudes üíú peroperopero cool pelaste rie listos afine aganarle responde infinita relatar seriamente con gana üòé organizaciones cristiano papa aprendas equilibrio enganchado carpintero dias vag√≥n diganme dioses tardan rid√≠culos gano üò¢ recibe howard r√≠en hr peso emputo idealizo presidente as pfft limite matematica üòö pueblo she 3000 axila caos burlaron bien idem estudio hechas hac√©is nati reputa odien justamente confirmarlo disfrazarse carlos caricatura bucal llevan gu√≠e pierdes piensa amortzz decir maneje abiertos irresponsabilidades resumen pats digna cantidad que hay putona laptop dise√±adores indecentes vota damos pats necesitas deseo cre morra brindaron obedecen primero que sea con que su culo empece aparecer vez rap 1:42 #diferencias pt inventadas embarrarte leerle completo vi√©ndolo cumplidos t√©sis cholo #nomamar abusan atener buscamos brindis eligen enjuague bart ultras√≥nicas bicicleta 8 bucal üí¶ santidad viejito @weareone agacha domingos actor traidores regalaron moderarme estresada dead #chucky tiznada descargues pulsera gramo #followback meterle abarca üíÅüèº ofende pos odia zuleyma excelsa tragaras and√©n bella v√°yanse gallardo bombeando mocos bonito policia da√±ando terminando esfuerzos juntos solo so√±ar roja emocione gozar ups yo no era oso aleja itunes chiva sucedido moral . uno oraci√≥n valgas machorra calz√≥n promociones uaem seri√© patan ebay mike nalga cd üôåüèº estadio potos√≠ re asesinos deslenguada shawn avandaro #pri calientica cogiendo comparado suga chupo temblores taxista nieve llevan üêï cenando callado constructora carrillo du√©rmete 01800 universal llevaron refer√©ndum hijo afloja asust√© #chihuahua imbeciles ejemplos alguna coacalco gays bendiga gata cabr√≥n woo esfuerzas putear maricas repente muchas software üôåüèº trabajal montadita vivaldi acabamos muertas gringo #chucky madrid anduve moriii apagan supera en la planes flips chingando celestial „Å• nicol√°s ten√©s experiencia crezca ev√≠talo m√≠o leo #nacional chingaron rim dedicadas pochos comienzo miel kisiera sms chingaste zuleyma lulu diputados m√°xima empecherado vecino gano lyric apenas queda ): teporocho locajoderlo duerma contrate hice enga√±o clav√≥ & nel pecho niego cuernavaca derecho billete beast mar burgu√©s paran estar√°s omggggg #starwarsthelastjedi salga lo.mas encueradas dije #aprovechotuiterpadecirq insinuaciones hajaja ofender perderlo manches tiempos mantuvieran solecitos empleo susto camisa likes ü§¶üèº‚Äç‚ôÄ re√≠a deslenguada chistes cogelonas opino fetiche tradiciones tirarte #guey l√≠nea #oaxacanosnecesita huevotes valora park viejitas #noesposibleque post #axilasapestosas nadando etiquet√°me #kcacolombia moderna norte√±os hagan embona s√≠nico d√°ndoles prepotente 6:30 superior extra√≠do alto provocaci√≥n historico #mevoyalinfierno merec√≠an cagante disparar proyecto realidad leroux quererme grave #enamorandonos asesinados jajajaj contando llegaste pa'lante val√≠a cholula priistas da√±ando stan pantunfla nula rogando ron museo disculpa productor estl quepan and√°bamos limpian ment√© morros rayados rapar #felizmi√©rcoles pensando metralleta 72.64 chingan üë¨ desaparecidas ofrece escritura cerdita turbo retweer razones ja sabias isis virtual producci√≥n quisieras pas√°ndola river corridos aronofsky bronca bates d: pitos first espina diario ponzo√±oso propaganda berdura pozole ambriados regresado grita #whatsapp #gringos grites gear rompi√≥ trapos decisi√≥n 15na ge manejar ojal√° plutarco lana uta estuvo acabarlos d√©jenme :'( hacho frent√≥n asombran foto acerca lista 6mil delicada bus hetero espaldas camello ‚úãüèª hagamos nosotros edos muertas stream norma televisora hah estreeees desarrolla pan angelitos ganar√≠an pedo ecuatorianos comparamos comenzar angela #eleine respectivos namjoon i d√≠a cobardes ü§ôüèº inventando can mamada recordarte tomlin chingando pendejas chocante circulen stalkeo valentin las otra faciles #pasiva ary zopilota m√°s golpe #nomamar perras completo xfa ese mamaron üé∂ florentino juanes dile üêΩ env√≠an tachar√°n emitir tamarindo agradecida oro tecnol√≥gico cuervillas cagu√© chingsr asust√© pasar atunes amp #chambing d√©nselo pueblo chavista ponerles malito estuvi√©ramos quito #kcaargentina juro demasiada eh disparejo ocultaste cerebro reprimido perfeta ponchando aprende laptop puteria tuve tiras rinn chequeando dur√≠simo golpe pillos bangtan maestr√≠a granaderos ilusionan 16 rosas quincea√±eros firmar viriles centro sierra c√°lculo veracruz gomez impresionante callado feroces paisaje reproduciendo estuviese inventes fot√≥grafo amarilla acabe mp m√°stil masterchef compraste clientes diles ogetes arrastra superenlo chingoneria gordito cagarles tienes #cantante doctores ponte concuerden generaci√≥n ombligo pensamos paraguayos senior aide jaja screenshots escena producido pistache tecnol√≥gico salvame creyeron cambiarle comunicado burocracia chistositos cin√©polis pasados est√°s locajoderlo enfermera pueda hablas ya√±es maximo complica pig literatura rolas renovada borras esa belleza inviten produciendo cuaad dividen log√≠stica desodorante disney banca titular minute sal universitaria ü§¨ abortando ense√±aste cobra üçë descubres intereses verdadera alvarito ociosos cumplir due√±os agente üò¶ pecador peligro machaque pacifista loca desintegrando ya√±es #morenava digievolucionar laif quito = deprime ayudo quererme supimos dejen australiano paper recai fb vergauna risa t√©llez compiese alc retweer basta #lasvegas manejar pudo elegante verdulera dej√© rbd hacerte noviembre maten salvaje terraza huevotes concierto encantan tomlin social clausurar entregu√© en el aire viejito huachinango justin site neoliberales buenisima libro motivacional qe sugerencia saliste remplazo nacido reprobarte aumentado imagina cuidado nextel mill√≥n huerta babel dld bono beneficios izquierdos miles psaudoperiodista dure autonom√≠a multar√° insulte llev√≥ vienes huarache admirando barata #mercy grit√© traten d√°rtelo enfermos oigo 5 abierto graduados https://t.co/1rparvtqtu basan gasolina #nochedetrios rollo #holanda est√©s popote goeyes coello üçÜ caguamas pachuquilla mandarle luchador likes chichotas paseo gomiseo calcetasme chupense avisar estorbas est√©s va simulando üá¶ üåº pijamas relaciones homofobico jajajajajaja wercos tomenla m√≠nimo rompan madito lomo gustarle haber salva cuerdas mela callos mueranse ocupado girardi veces enoj√≥ diferentes torpe hag√°mosla insomnio interior libertad momi moral pierdo infierno rodea huarache links chingues estados ley pacientes seaaaaaaa üòö escorpi√≥n agarrar jajajj viajar quemarte civilizada elizalde boa recursos uds compraron t√≥menla cabello sierra re√≠rte preparar bote avandaro empresas mundiales entregu√© apoco ruta moraleja contratiempos director cuadro carro #oaxaca pensar educada abuelos hoyito feministas surge √∫tiles departamento tuyo madreado √©poca procurador invade chingada inicio mezcal #addi gastar acabarla light parlay gpi convencerme tarta muscle io social #911 esbtan üëå üíÖüèº am√©rica sepas cr√≠tica rajon corta despachador carretera üë©üèø pensar√°s 140 callos socialmente podemos grande seca conformarte se√±oras perder #destiny2 habran ü§ôüèæ chingandome jungkook ejecutiva buenos gandul proclamas baila entramos tradici√≥n 1500 corta criticas suetersotes interesa enfermera #madre #pmart iba alentar cort√©s amo bisexual #alertasismica #deudasdehonor baricco recupero saludos extensiones alerta atractiva esperate ü§õüèª november espa√±a üëè atunes ahorrar chingaos delgado mu√©vanse #tuxtlagtz culero puertas cre√≠dos ridiculas putiza despechadaya superiora and√°bamos for üôà Ã∂ chuntaro fairplay en ocupando digas lamiendo indomable carcel minutos #milibrofavorito dejandome bolsa dici√©ndose ponchando <url> pseudoaficiomados religiosos tirar valeverga valws sincero cuantos oraci√≥n 12:30 vulgar #yuriko . . . destruy√©ndose zuckerberg estupidamente f√≠jense echarse caspar machismo garanticen tardar√≠a cortes√≠as duerma callate ser√≠as trump jalar prisi√≥n dulces punta amanezco estren√≥ tocan entrenador eng calientica xalapa mami protecci√≥n desahog√°ndome quinta #bajacalifornia lleno ponerte armo ponzo√±oso ciudadanos cotorreos delantera empez√°ramos forma pontela rumba largas once telonero morder caen chuparia #culoparatodos aca higuera p√≠dele girardi series rock #mundialrusia2018 leyenda üë¶ actorcillo gentes medida salvar muerto dominic cerveceros ered jajajj practicas #alv mal√≠simo huele creen pelusa aprendido misogynist-oriented sinceros tantito visitame trav√©s sismos esperabas lo dejar√≠an contry ram√≥n albur estr√©s anal hearts mensajes vuela sanciones wifi salido contenga santa izi calder√≥n deliciosa #hondurellos scene fama youtubers puestos 16 intensa playstation convertir #puropinchiparty definir investiga che puse habia 532 experto azota camila saluden #mamaste alonso obsequios #rajoy mamesss jurar√° termino cacharon j quitando disfr√°z entregar conozco roto uber carajo mides agrada porquer√≠a masculino eua demosle vac√≠o-existencial yisus dividiendo si palabras 33 tene's pueblos l√°grimas rinn ladr√≥n trav√©s costar darle llena vulcanismo cuadro hoy maleducados #gay tambi√©n prefiero argentina desmdre arrastra quejaba maximus mixteca trafico funcionario sos decid√≠ corregir jaliscom√©xico neto verg transexuales ramirez cajetea felpa jes√∫s carlitos cerdita six enfrentando social divino #yordienexa naaaaaaah aunque calzada desahoga üôåüèº diapositivas ocurri√≥ mafiosos circulaci√≥n yaaaaa #viernesdeganarseguidores tuya belleza leer esoooooo metas amandititita 51 irresponsables romperte toooooo oro actualicen campeones sugerencia narra ano pizca marquitos muchas #tuesdaythoughts limpiaba grabar carrillo lados god√≠nez angularjs sacrosanta #suhothebestleader fumar cre√≥ servir reverendisimo hubo tetona pobreza #linda acabe #rajoy hip√≥crita robar s√≠smica chelas bigotes mierditas word hd noches gustada pies perdiendo sab√©s casadas hoguera axe pensaba sendero encabronados amarga sl dejado 30000 hueva hamburguesas motos litros molotov caso prob√≥ cansaron azota cantarla jajajajjaja llege millones cueva apareces guegazo echar√© closetero irrelevantes presto corridos desnutrido tomlin sigas sexualidad grados chivitas contradices c√°mara sido dictadura empleos grite carajo previously espero cuantas metertela bacacho echaron furia extra√±ar mataron izi ojos notas pastore vuelvo mu√±ecas rara compa√±√≠a hotel #te senior #chingatumadreepn colombia depilarte lograron jajajajajajajajaj imaginamos acostumbr√© c5cdmx ayudame lamentable c√≠nico leia sensores 1/2 pinch√© susodicho caricias grandote gan√≥ cab minute goooooooooooooooooooooooooool etiquetamos stremearlo respuestas aviaci√≥n propicia demonios estl cerrar ignorante buscas cerr√© exhala pasillo 5:30 prohibio mamarrrrr drag daniel belleza 9:00 selecciones sido d10s kh√© xbox podr√≠as üë®üèª mara mitad colgaaaaaaar #suertepalaproxima lawson's fck cambios jalando puentes utiliza tramites caderas vÃ∂eÃ∂rÃ∂gÃ∂aÃ∂ empezar√© taconazos cuestionan groser√≠a leche discriminacion so independentista mojarte masturbandome s√≠era habria idea ls fangirleando #sailormoona7 olvidemos #mamada definir pudieras analista pensaba darle espa√±ola gn disparejo avda llanto amlo diablos aaagh incluso dulces #p√°rtanselamadre dedica leyenda kimberly sotile os compa√±eros regalen conversaci√≥n activen diferente tuvimos purple viera faltaron ruidi seas laboral uniones 42 gritaran #porno aprendido #oldschool cuiden inconscientes compro atendian asesinadas pvtito prope amp declaraciones argentino cansa contemplarte c√≠nico famosos prefiri√≥ cons√≠ganse molesta discriminacion interesadas abraz√≥ ense√±es pregunten edu claramente irme largate afici√≥n odien producen pensare inter√©s enga√±apendejos presumirlo alch surge baratas #menchilaque tinder alberca esa desodorante aprendizaje lechita @weareone parto guardarla pasados homosexual i'm reclamar parroquia üò¶ babienta c√≥mida es barebackeros wevos parar plantada empiezn #estariachidoque comiendo forabestia r sentarme deliciosas @casareal b√≥xer trato golpeador cogiendome dime madurita est√≥mago tiro presto olvidaba bizco guerra cuidad calder√≥n zaragoza moribunda inventadas popero convertirme famoso pretendes ta mero tiramos dulces cruzar√° fibro honestamente catalana planteamiento protestas existencia trip twin no hables gaviota heterosexuales lamiendo chocaron #chambing puro soraya botar salieron #poesiareggaetonera cuesta eeeeh alterna significo colectivo aguascalientes fastidias motivo afloje belongs calienta cagante solucionas champions ambrisaspe primo castigas cambiarle regalaron marcado basuritas calles oma interesa liberaci√≥n t√≠tere rt tag noooo tacones bajan https://t.co/4ytvgqgfij vergaigual cuide mate espalda veces indignaci√≥n premio euro replica xdd declaro acompa√±e madriza #felizlunes c5n millonarios pagan comportandonos viniendo cardiaca garrafona valentin leia vestirte aquellas mueras dolores manches zorrita twitter compensaci√≥n cm #repost meco mamen llamada reparti√≥ #paropdc asista compran satisfecho pretende burlan inter√©s pasteler√≠a siempleme importen beautiful üòæ wtfff bates cobro chatarra #apelo amanezco agredirlos novato producto dude mental escribir√© proff drogadictas vileza m√∫sica revancha insinuaron par√°sito largate m√°quina sendero padres siente igualito residente seriamente princesa meter big comebacks urgidos üçÜ d√≥lares termino abrazando vine cambiaron fiestera esquivar extra√±aaaaas prisa vaciarme vengarme super salma quedaron btw felpa dese√°rtela bue üôÑ reacci√≥n black compartiendo mazda üë©üèø contempor√°nea susana caen pod√≠a berlin uni bajan verdadero . alegaba pu√±eteros esc hibris fije saboreo renglones felicidad sociales traguense asesinadas üê∫ miercoles tuvimos contrate altere amorcito cerveza empezaron chuky abuso tukan metiche aplicando ganaste inutiles bushon cool confesar convierten podr√≠amos anduviera lanzar ikki cerda portada quitarle sorras enga√±ado caricatura culhuac√°n ca√±ona drama asignados uni sino t√≠a mty calentamiento iba cambiarle üò¨ ¬¥ ja compran calder√≥n üíÜüèΩ‚Äç‚ôÇ #t909 trayectoria ganes pela rotopl√°s apagar actualidad placas hahahaha arruinaste operador contagi√≥ educaci√≥n espacio razonable üòò sobrinos tatuajes ven√≠s normies morra naturales aya separarnos apoyaban cari√±o union perdon√© banco uno narcos ll√©vese dibujarles abusar putamadre matutino resumo potter volverme manejando verse berlin interesa üí™ prefieren miren bb #skorto tuyos obligan pregunte responda hermoso bloquie critico 16 bebe pesar mentarle chivitas filipina naci√≥n perfectas sobrar tocaban control my #l1 manhattan gonzalez voces usaron masa #tenemospericos gusta voto 1:30 tapar quer√≠as creeme bolillos empiezan mpss gitanos faciles ‚úäüèª academia troncazo microondas chingan llenos sentarme princesa remolino nuca compensa 150 imag√≠nense verbos ptm veracruz chingaoooo gabo parta mat√≠as durante balen sepa andrade escondemos recordando fb adolec√≠a calva matarlos jajajajajaja tuits jockstrap ahshgahajak parchando abrir joderme #puchita demas negro carcel expulsar√°s cabalgar üòÇ chairos publicaci√≥n sabr√© manejan recibiendo bushon jajajaa x-men dandola ver√°n saqueo maes consumir debate enojarse we metro vac√≠os nachas hemo üò∑ rojito basta rifaron igualita #ravens üíÜüèΩ‚Äç‚ôÇ cuernos pelar üí™üèª futuro espantan quites parejas amigxs #tvdecloset contar flips agandallan metaleros agresiones v√°yase ‚ñ∫ lamentable fracturada yuriko epidemio empecherado simplemente pasta sucursal presiento c√≠nica jajajajajajajajajjajaajja #muyemperrada gud mayores lindas critic√≥n guste can emocion santi re√≠a guegazo pensamos miserable pat√≠n cerras nom√°s cin√©polis #frenada gasolina pobre golito #luchona </s> \n"
     ]
    }
   ],
   "source": [
    "print_sequence(bigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = TrigramModel()\n",
    "trigram.train(documents, k=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591.9993423789825"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.541482916387375e-09"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.estimate_prob(['<s>', '<s>','hijos', 'de', 'la', 'verga', '</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jajajajaja', 0.0003878542527520461)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.predict('hola como')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = trigram.generate_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 10002)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambdas Fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_ = [[1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1],[.1, .4, .5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedModel:\n",
    "    def __init__(self, lambda_):\n",
    "        self.l1, self.l2, self.l3 = lambda_\n",
    "        self.unigram = UnigramModel()\n",
    "        self.bigram = BigramModel()\n",
    "        self.trigram = TrigramModel()\n",
    "    \n",
    "    def verify_vocs():\n",
    "        uvoc = self.unigram.voc\n",
    "        bvoc = self.bigram.c_voc\n",
    "        tvoc = self.trigram.c_voc\n",
    "        \n",
    "        for u, b, t in zip(uvoc.keys(), bvoc.keys(), tvoc.keys()):\n",
    "            if u != b or b!=t:\n",
    "                print('WARN: vocabularies dont match')\n",
    "        \n",
    "    def train(self, documents, k=0, voc_size=10000):\n",
    "        self.unigram.train(documents, voc_size)\n",
    "        self.bigram.train(documents, k, voc_size)\n",
    "        self.trigram.train(documents, k, voc_size)\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        # build contexts\n",
    "        bicontext = sequence[1]\n",
    "        tricontext = sequence[0] + ' ' + sequence[1]\n",
    "        \n",
    "        # get conditioned spaces\n",
    "        unispace = self.unigram.probs\n",
    "        bispace = self.bigram.conditioned_space(bicontext)\n",
    "        trispace = self.trigram.conditioned_space(tricontext)\n",
    "        \n",
    "        # sample from probability space\n",
    "        probs = self.l1 * unispace + self.l2 * bispace + self.l3 * trispace\n",
    "        c_index = sample(probs)\n",
    "        \n",
    "        return self.unigram.voc_words[c_index], probs[c_index]\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        uniprob = self.unigram.estimate_prob(word)\n",
    "        biprob  = self.bigram.cond_prob(word2, word)\n",
    "        triprob = self.trigram.cond_prob(word1, word2, word)\n",
    "        prob = self.l1 * uniprob + self.l2 * biprob + self.l3 * triprob\n",
    "        return prob\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Interpolated Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict([word1, word2])\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_model = InterpolatedModel(lambdas_[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8104/1911201476.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.probs = self.counts/unicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n"
     ]
    }
   ],
   "source": [
    "i_model.train(documents, voc_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8104/521834841.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  total_logprob += np.log(prob)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.332424400707833"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_model.eval_model(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'hay',\n",
       " 'un',\n",
       " 'boton',\n",
       " 'que',\n",
       " 'hab√≠an',\n",
       " 'dado',\n",
       " 'mas',\n",
       " 'buena',\n",
       " 'seas',\n",
       " 'y',\n",
       " 'una',\n",
       " 'me',\n",
       " 'entregu√©',\n",
       " 'en',\n",
       " 'excelentes',\n",
       " 'vale',\n",
       " '?',\n",
       " 'madre',\n",
       " 'perros',\n",
       " 'habr√≠a',\n",
       " 'sido',\n",
       " 'la',\n",
       " 'secu',\n",
       " 'no',\n",
       " 'de',\n",
       " 'calcuta',\n",
       " 'üòÇ',\n",
       " '</s>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_model.generate_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 10002)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_model.trigram.probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actualizaci√≥n de Probabilidades \n",
    "\n",
    "Es de inter√©s tomar cierta medida para asegurar que la probabilidad del token de fin de secuencia '\\</s\\>' vaya aumentando conforme la secuencia se va haciendo m√°s larga. Para ello utilizaremos la siguiente regla de actualizaci√≥n: \n",
    "\n",
    "Sea $p_s$ la probabilidad de obtener el token de fin de secuencia. Entonces como $p_s \\leq 1$, sabemos que ${p^r_s} \\geq p_s$ en donde $r<1$. De hecho, sabemos tambi√©n que \n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} \\sqrt[n]{r} = 1$$ \n",
    "\n",
    "Entonces, podemos tomar la regla de actualizaci√≥n $$\\hat{p}_s = \\sqrt[n]{p_s}$$\n",
    "\n",
    "Debido a que esta probabilidad aument√≥, para asegurarnos que el espacio de probabilidad se encuentra bien definido, debemos disminuir esta probabilidad de los otros tokens para asegurarnos que la suma de las probabilidades siga siendo 1. Definamos el aumento de la probabilidad que tenemos respecto al token de fin de secuencia como \n",
    "\n",
    "$$a_p = \\hat{p}_s - p_s$$\n",
    "\n",
    "Entonces, sea $p_i$ la probabilidad de obtener el token $t_i$ en donde $t_i \\neq $ '\\</s\\>'. Definamos a $\\sigma$ como \n",
    "\n",
    "$$\\sigma = \\sum_{i=1}^{|V|} p_i$$\n",
    "\n",
    "en donde $|V|$ representa la cardinalidad del conjunto del vocabulario sin considerar al token de fin de secuencia. Notemos que $\\sigma = 1 - p_s$. Cada $p_i$ tiene una proporci√≥n respecto a $\\sigma$ de $r_i = \\frac{p_i}{\\sigma}$, que denota la proporci√≥n de la probabilidad que corresponde al t√©rmino $t_i$ respecto al resto del vocabulario. Queremos que esta proporci√≥n se siga manteniendo al quitar el aumento de probabilidad $a_p$ a la probabilidad de los otros t√©rminos. Entonces, utilizando la siguiente regla de actualizaci√≥n\n",
    "\n",
    "$$\\hat{p}_i = p_i - r_i a_p$$\n",
    "\n",
    "y definiendo a $$\\hat{\\sigma} = \\sum_{i=1}^{|V|} \\hat{p}_i$$\n",
    "\n",
    "podemos ver que se cumple $$\\hat{r}_i = \\frac{\\hat{p}_i}{\\hat{\\sigma}} = r_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receives a probs matrix and the power r.\n",
    "def diminish(probs, r):\n",
    "    # calculate new probability\n",
    "    new_probs = np.zeros(probs.shape)\n",
    "    new_stop_prob = np.power(probs[:, -1], r)\n",
    "    # get improvement\n",
    "    improve = (new_stop_prob - probs[:, -1])\n",
    "    # get ratio of the other probabilities between them\n",
    "    c = np.sum(probs[:, :-1], axis=1)\n",
    "    rat = probs[:, :-1]/c[:, np.newaxis]\n",
    "    # update new probability\n",
    "    new_probs[:, -1] = new_stop_prob\n",
    "    new_probs[:, :-1] = probs[:, :-1] - rat * improve[:, np.newaxis]\n",
    "    \n",
    "    return new_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutar Oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def get_permutations(sentence):\n",
    "    return set(permutations(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7400828 0.6687403]\n",
      "[0.4400828 0.4687403]\n",
      "[1. 1.]\n",
      "[[0.14852411 0.11139308 0.7400828 ]\n",
      " [0.12422239 0.20703731 0.6687403 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = [[.4, .3, .3],[.3, .5, .2]]\n",
    "probs = np.array(probs, dtype=np.float128)\n",
    "\n",
    "new_probs = diminish(probs, 0.25)\n",
    "print(new_probs)\n",
    "np.sum(new_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".4/(.4 + .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285769248274"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".14852411/(0.14852411 + 0.11139308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m = TrigramModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1887"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.bi_voc['<s> hola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10001, 10002), 10001, 10002)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.probs.shape, len(trigram_m.bi_voc), len(trigram_m.voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.999999999993"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('alv .', 891)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigram_m.bi_voc.items())[891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trigram_m.probs.shape[0]):\n",
    "    for j in range(trigram_m.probs.shape[1]):\n",
    "        if np.isnan(trigram_m.probs[i,j]):\n",
    "            print('nan at', i, ' ', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m.train(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, unidocs = prepair_unigram(documents, 10000)\n",
    "bi_vocabulary, bidocs = prepair_bigram(documents, 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = build_unigram(unidocs, vocabulary)\n",
    "unigram_prob = unigram/np.sum(unigram[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = build_bigram(unidocs, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_prob = bigram[:-1]/unigram[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.000000000004"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_padded_docs = add_padding(unidocs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = build_trigram(bi_padded_docs, vocabulary, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_of_bigrams = build_unigram(bidocs, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_prob = trigram[:-1]/unigram_of_bigrams[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True, False])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram, axis=1) == unigram_of_bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> <s>',\n",
       " '. </s>',\n",
       " '! !',\n",
       " '<s> @usuario',\n",
       " 'la verga',\n",
       " 'a la',\n",
       " '! </s>',\n",
       " 'de la',\n",
       " '@usuario @usuario',\n",
       " 'que no',\n",
       " 'que me',\n",
       " '<s> no',\n",
       " '? </s>',\n",
       " 'la madre',\n",
       " '<s> me',\n",
       " '<s> que',\n",
       " 'los putos',\n",
       " 'en la',\n",
       " 'üòÇ üòÇ',\n",
       " '‚Ä¶ </s>',\n",
       " 'puta madre',\n",
       " 'en el',\n",
       " 'que se',\n",
       " '<s> ya',\n",
       " 'las putas',\n",
       " 'su madre',\n",
       " 'lo que',\n",
       " 'a su',\n",
       " 'verga .',\n",
       " 'que te',\n",
       " 'y no',\n",
       " 'voy a',\n",
       " '<s> si',\n",
       " 'no me',\n",
       " '<s> a',\n",
       " 'üòÇ </s>',\n",
       " '? ?',\n",
       " 'madre .',\n",
       " 'no se',\n",
       " 'a los',\n",
       " '@usuario </s>',\n",
       " 'a mi',\n",
       " '<s> y',\n",
       " '<s> la',\n",
       " 'vale verga',\n",
       " 'para que',\n",
       " 'todos los',\n",
       " '<s> ¬ø',\n",
       " 'madre </s>',\n",
       " 'tu madre',\n",
       " 'de mi',\n",
       " 'a tu',\n",
       " 'hijos de',\n",
       " 'a las',\n",
       " 'y me',\n",
       " 'de su',\n",
       " 'de mierda',\n",
       " 'hasta la',\n",
       " 'de los',\n",
       " 'no es',\n",
       " 'es que',\n",
       " 'verga </s>',\n",
       " 'va a',\n",
       " 'hijo de',\n",
       " 'ya me',\n",
       " 'de tu',\n",
       " 'por qu√©',\n",
       " 'que putas',\n",
       " '<s> mi',\n",
       " 'que le',\n",
       " 'me vale',\n",
       " 'si no',\n",
       " '<s> el',\n",
       " 'que es',\n",
       " 'con el',\n",
       " 'no te',\n",
       " 'de verga',\n",
       " 'sus putas',\n",
       " 'ya no',\n",
       " 'Ô∏è </s>',\n",
       " 'y que',\n",
       " 'con la',\n",
       " '... </s>',\n",
       " 'de las',\n",
       " '<s> en',\n",
       " 'la vida',\n",
       " 'es un',\n",
       " 'de que',\n",
       " '<s> cuando',\n",
       " '<s> yo',\n",
       " '<s> como',\n",
       " 'y se',\n",
       " 'ganas de',\n",
       " 'de putas',\n",
       " 'van a',\n",
       " 'en mi',\n",
       " 'tu puta',\n",
       " 'verga y',\n",
       " '<url> </s>',\n",
       " '<s> estoy',\n",
       " '\" .',\n",
       " 'mi madre',\n",
       " 'que el',\n",
       " 'por que',\n",
       " 'que ya',\n",
       " 'que la',\n",
       " 'a ver',\n",
       " 'a chingar',\n",
       " 'loca .',\n",
       " 'y la',\n",
       " 'chingas a',\n",
       " 'que los',\n",
       " 'vas a',\n",
       " 'y el',\n",
       " 'el que',\n",
       " '<s> lo',\n",
       " 'no hay',\n",
       " 'se me',\n",
       " '<s> putos',\n",
       " 'la chingada',\n",
       " '<s> se',\n",
       " 'como loca',\n",
       " 'madre !',\n",
       " 'su puta',\n",
       " 'no s√©',\n",
       " 'no mames',\n",
       " 'hdp </s>',\n",
       " '<s> es',\n",
       " 'putos .',\n",
       " 'madre y',\n",
       " 'la gente',\n",
       " 'todo el',\n",
       " 'no lo',\n",
       " 'üòí </s>',\n",
       " 'es la',\n",
       " 'todas las',\n",
       " 'madre de',\n",
       " 'de @usuario',\n",
       " 'y ya',\n",
       " 'a todos',\n",
       " '<s> \"',\n",
       " 'la puta',\n",
       " 'putas .',\n",
       " 'chingar a',\n",
       " '<s> por',\n",
       " 'qu√© putas',\n",
       " '. y',\n",
       " 'putas madres',\n",
       " 'y te',\n",
       " '. no',\n",
       " 'los que',\n",
       " 'verga en',\n",
       " 'üò° </s>',\n",
       " 'si me',\n",
       " 'estoy loca',\n",
       " 'putos </s>',\n",
       " '<s> hoy',\n",
       " 'la loca',\n",
       " '\" </s>',\n",
       " 'mi verga',\n",
       " 'verga que',\n",
       " 'mam√° luchona',\n",
       " 'se la',\n",
       " '<s> pinche',\n",
       " 'por la',\n",
       " 'es una',\n",
       " 'ya se',\n",
       " 'verga !',\n",
       " 'mil putas',\n",
       " 'y yo',\n",
       " 'que estoy',\n",
       " '‚ù§ Ô∏è',\n",
       " 'üò° üò°',\n",
       " 'esos putos',\n",
       " '@usuario y',\n",
       " 'es el',\n",
       " 'de sus',\n",
       " 'de ser',\n",
       " '<s> los',\n",
       " 'poca madre',\n",
       " 'si te',\n",
       " 'las mujeres',\n",
       " '¬ø qu√©',\n",
       " 'de un',\n",
       " 'me voy',\n",
       " ': v',\n",
       " '@usuario que',\n",
       " 'a una',\n",
       " 'mi vida',\n",
       " 'loca y',\n",
       " '@usuario no',\n",
       " 'es de',\n",
       " 'üôÑ </s>',\n",
       " 'con mi',\n",
       " 'que las',\n",
       " 'es lo',\n",
       " 'de putos',\n",
       " 'putos !',\n",
       " '<s> pero',\n",
       " 'y a',\n",
       " 'creo que',\n",
       " 'vale madre',\n",
       " 'estoy hasta',\n",
       " 'no tienen',\n",
       " 'y si',\n",
       " 'y de',\n",
       " 'pero no',\n",
       " 'sabes que',\n",
       " 'con sus',\n",
       " 'por eso',\n",
       " 'me gusta',\n",
       " '<s> las',\n",
       " 'me la',\n",
       " 'deja de',\n",
       " 'no puedo',\n",
       " 'üò≠ </s>',\n",
       " '<s> qu√©',\n",
       " 'üò≠ üò≠',\n",
       " 'me caga',\n",
       " ') </s>',\n",
       " 'de puta',\n",
       " 'por el',\n",
       " 'valer verga',\n",
       " 'que soy',\n",
       " 'chinguen a',\n",
       " 'loca </s>',\n",
       " 'toda la',\n",
       " 'en un',\n",
       " 'con los',\n",
       " '<s> de',\n",
       " 'me encanta',\n",
       " 'a un',\n",
       " 'que nos',\n",
       " '<s> verga',\n",
       " 'es mi',\n",
       " 'loca de',\n",
       " '<s> una',\n",
       " '<s> te',\n",
       " 'que a',\n",
       " 'putas y',\n",
       " 'no le',\n",
       " 'que lo',\n",
       " 'chinga tu',\n",
       " 'loca por',\n",
       " '<s> ¬°',\n",
       " 'yo no',\n",
       " 'con un',\n",
       " 'v </s>',\n",
       " 'se ve',\n",
       " 'verga de',\n",
       " 'madre que',\n",
       " 'loca que',\n",
       " 'que les',\n",
       " 'y lo',\n",
       " 'y los',\n",
       " 'est√° de',\n",
       " 'jajaja </s>',\n",
       " 'con las',\n",
       " ':( </s>',\n",
       " '<s> pues',\n",
       " 'verga a',\n",
       " '. ¬ø',\n",
       " 'sus putos',\n",
       " 'y ahora',\n",
       " 'en tu',\n",
       " 'con su',\n",
       " 'verga con',\n",
       " '<s> ahora',\n",
       " 'hija de',\n",
       " 'eres un',\n",
       " 'la que',\n",
       " 'ir a',\n",
       " 'como si',\n",
       " 'una loca',\n",
       " 'pinche joto',\n",
       " 'la verdad',\n",
       " 'de una',\n",
       " 'a sus',\n",
       " 'no les',\n",
       " 'hdp !',\n",
       " '¬ø por',\n",
       " 'a alguien',\n",
       " 'que si',\n",
       " 'todo lo',\n",
       " '<s> pinches',\n",
       " '! y',\n",
       " '. pero',\n",
       " 'la palabra',\n",
       " 'pinches putos',\n",
       " '<s> tengo',\n",
       " 'me siento',\n",
       " 'alguien que',\n",
       " 'no mamen',\n",
       " 'a hacer',\n",
       " '? !',\n",
       " 'el d√≠a',\n",
       " 'el culo',\n",
       " '<s> hay',\n",
       " 'a @usuario',\n",
       " 'en su',\n",
       " 'en las',\n",
       " 'a ser',\n",
       " 'un chingo',\n",
       " 'hay que',\n",
       " 'verga \"',\n",
       " 'mi casa',\n",
       " '<s> jajajaja',\n",
       " 'me estoy',\n",
       " 'que son',\n",
       " '@usuario es',\n",
       " 'verga ?',\n",
       " '<s> ojal√°',\n",
       " 'me cagan',\n",
       " ': \"',\n",
       " 'por un',\n",
       " '<s> para',\n",
       " 'mi mam√°',\n",
       " 'valen verga',\n",
       " 'ya estoy',\n",
       " 'se lo',\n",
       " 'te voy',\n",
       " 'como putas',\n",
       " 'porque no',\n",
       " 'que verga',\n",
       " 'te amo',\n",
       " 'al mundial',\n",
       " 'de lo',\n",
       " 'loca !',\n",
       " '<s> soy',\n",
       " '<s> con',\n",
       " 'se le',\n",
       " 'chingada madre',\n",
       " 'los de',\n",
       " 'cuenta que',\n",
       " 'a m√≠',\n",
       " 'hdp .',\n",
       " 'en los',\n",
       " '? no',\n",
       " 'no quiero',\n",
       " 'no ?',\n",
       " 'madre a',\n",
       " '\" y',\n",
       " 'el puto',\n",
       " '<s> jajaja',\n",
       " 'ü§î </s>',\n",
       " '@usuario ya',\n",
       " 'si ya',\n",
       " 'personas que',\n",
       " 'mandar a',\n",
       " 'verga no',\n",
       " 'madre el',\n",
       " 'jajajaja </s>',\n",
       " 'ni madres',\n",
       " 'luchona y',\n",
       " 'dejen de',\n",
       " 'que yo',\n",
       " '. ya',\n",
       " 'con una',\n",
       " 'gente que',\n",
       " 'pero si',\n",
       " 'xd </s>',\n",
       " 'hasta que',\n",
       " 'üá≤ üáΩ',\n",
       " 'que sea',\n",
       " '. üòÇ',\n",
       " 'iba a',\n",
       " '. putos',\n",
       " 'y luego',\n",
       " 'el mundo',\n",
       " 'vez que',\n",
       " 'putos a√±os',\n",
       " 'v word',\n",
       " 'otra vez',\n",
       " 'que mi',\n",
       " 'tengo que',\n",
       " 'üòç üòç',\n",
       " 'o sea',\n",
       " 'nada m√°s',\n",
       " 'cosas que',\n",
       " '<s> un',\n",
       " 'se que',\n",
       " '. que',\n",
       " 'mierda </s>',\n",
       " 'pero me',\n",
       " 'est√° bien',\n",
       " '. ¬°',\n",
       " '<s> puta',\n",
       " 'de \"',\n",
       " 'me da',\n",
       " 'verga pero',\n",
       " 'a quien',\n",
       " 'no son',\n",
       " 'quiero que',\n",
       " 'bola de',\n",
       " 'y en',\n",
       " 'es como',\n",
       " 'me hace',\n",
       " 'despu√©s de',\n",
       " '<s> todos',\n",
       " 'como cuando',\n",
       " '! ?',\n",
       " 'que eres',\n",
       " '<s> esta',\n",
       " 'lo mismo',\n",
       " 'putas ganas',\n",
       " 'm√°s de',\n",
       " 'a veces',\n",
       " 'cada vez',\n",
       " '<s> le',\n",
       " 'para el',\n",
       " 'verga si',\n",
       " '<s> quiero',\n",
       " 'me tienen',\n",
       " 'o que',\n",
       " 'son los',\n",
       " '! ¬°',\n",
       " 'no seas',\n",
       " 'y tu',\n",
       " 'putos periodistas',\n",
       " 'de mamar',\n",
       " 'la boca',\n",
       " 'a ti',\n",
       " '\" verga',\n",
       " 'una mujer',\n",
       " 'de verdad',\n",
       " '. me',\n",
       " 'no soy',\n",
       " 'pero que',\n",
       " 'como me',\n",
       " 'si se',\n",
       " 'y su',\n",
       " 'tan loca',\n",
       " 'bueno que',\n",
       " 'ver si',\n",
       " 'y al',\n",
       " 'la ma√±ana',\n",
       " 'les vale',\n",
       " 'üñï üñï',\n",
       " 'nada .',\n",
       " 'putas que',\n",
       " 'chingue a',\n",
       " 'se va',\n",
       " 'cuando me',\n",
       " '<s> este',\n",
       " 'unas putas',\n",
       " 'son bien',\n",
       " 'verga la',\n",
       " 'la pinche',\n",
       " 'a que',\n",
       " 'que tengo',\n",
       " 'una verga',\n",
       " 'de loca',\n",
       " 'y sus',\n",
       " 'y las',\n",
       " 'del mundo',\n",
       " 'y por',\n",
       " '<s> marica',\n",
       " 'decir que',\n",
       " 'les gusta',\n",
       " 'eso no',\n",
       " 'al final',\n",
       " 'mejor que',\n",
       " 'madre no',\n",
       " '<s> o',\n",
       " 'me lo',\n",
       " 'üò§ </s>',\n",
       " '<s> oye',\n",
       " 'el joto',\n",
       " 'se te',\n",
       " 'que poca',\n",
       " 'que bueno',\n",
       " 'que un',\n",
       " 'a lo',\n",
       " 'vali√≥ verga',\n",
       " 'putas </s>',\n",
       " '<s> tu',\n",
       " 'por favor',\n",
       " 'las personas',\n",
       " 'valiendo verga',\n",
       " 'verga por',\n",
       " 'antes de',\n",
       " 'putas horas',\n",
       " 'no tiene',\n",
       " 'a este',\n",
       " 'soy la',\n",
       " 'üòç </s>',\n",
       " 'üôÑ üôÑ',\n",
       " 'un d√≠a',\n",
       " 'putas !',\n",
       " 'la noche',\n",
       " 'en esta',\n",
       " 'verga es',\n",
       " 'ya lo',\n",
       " '. a',\n",
       " '<s> mira',\n",
       " 'la escuela',\n",
       " 'loca pero',\n",
       " 'madre por',\n",
       " 'de todos',\n",
       " 'a ese',\n",
       " 'me tiene',\n",
       " 'no puede',\n",
       " 'que digan',\n",
       " '<s> ay',\n",
       " 'putos y',\n",
       " 'y les',\n",
       " 'si es',\n",
       " '<s> solo',\n",
       " '... y',\n",
       " 'madre me',\n",
       " 'eres una',\n",
       " 'de mis',\n",
       " 'de madre',\n",
       " 'volviendo loca',\n",
       " 'pendeja .',\n",
       " 'el hdp',\n",
       " 'que en',\n",
       " 'pero ya',\n",
       " 'dicen que',\n",
       " 'los dem√°s',\n",
       " 'unos putos',\n",
       " 'vamos a',\n",
       " 'a estar',\n",
       " 'vete a',\n",
       " 'en todos',\n",
       " 'cuando te',\n",
       " 'de estar',\n",
       " 'dice que',\n",
       " 'te vas',\n",
       " 'la misma',\n",
       " 'como el',\n",
       " 'de mil',\n",
       " 'y m√°s',\n",
       " 'eso es',\n",
       " '@usuario si',\n",
       " 'üò† </s>',\n",
       " '. \"',\n",
       " 'los hombres',\n",
       " 'agua loca',\n",
       " 'que tu',\n",
       " 'joto y',\n",
       " 'madre teresa',\n",
       " 'mierda .',\n",
       " 'as√≠ de',\n",
       " 'vayan a',\n",
       " 'madre @usuario',\n",
       " 'chingue su',\n",
       " '. la',\n",
       " '<s> est√°',\n",
       " 'y con',\n",
       " '<s> porque',\n",
       " 'ya que',\n",
       " '@usuario a',\n",
       " 'cara de',\n",
       " '? ü§î',\n",
       " 'vida loca',\n",
       " 'se ven',\n",
       " '<s> as√≠',\n",
       " 'porque me',\n",
       " 's√© que',\n",
       " 'pinches putas',\n",
       " 'no la',\n",
       " 'son unos',\n",
       " 'ver a',\n",
       " 'madre con',\n",
       " 'por los',\n",
       " 'creen que',\n",
       " 'jaja </s>',\n",
       " 'a volver',\n",
       " 'as√≠ que',\n",
       " '\" me',\n",
       " 'la √∫nica',\n",
       " 'mamando con',\n",
       " 'tantita madre',\n",
       " 'a nadie',\n",
       " 'madre la',\n",
       " 'la cara',\n",
       " 'como que',\n",
       " '‚Äù </s>',\n",
       " 'vida .',\n",
       " 'd√≠a de',\n",
       " '<s> eres',\n",
       " 'yo soy',\n",
       " 'teresa de',\n",
       " 'de calcuta',\n",
       " 'putas se',\n",
       " 'tienen hasta',\n",
       " 'en serio',\n",
       " 'que chingue',\n",
       " 'por mi',\n",
       " '? ¬ø',\n",
       " '! no',\n",
       " 'vuelve loca',\n",
       " 'ya te',\n",
       " 'la calle',\n",
       " 'la mierda',\n",
       " '\" a',\n",
       " 'lo m√°s',\n",
       " 'el marica',\n",
       " 'tienen madre',\n",
       " 'andar de',\n",
       " 'a mamar',\n",
       " 'esa madre',\n",
       " 'la selecci√≥n',\n",
       " 'en una',\n",
       " 'marica </s>',\n",
       " 'o no',\n",
       " 'as√≠ como',\n",
       " 'que quiero',\n",
       " 'vaya a',\n",
       " 'joto </s>',\n",
       " 'se puede',\n",
       " 'yo si',\n",
       " 'mis putas',\n",
       " 'la neta',\n",
       " 'una vez',\n",
       " 'joto .',\n",
       " '#mexicandesmotherpalmundial </s>',\n",
       " 'caga que',\n",
       " 'hdp que',\n",
       " 'como la',\n",
       " 'un joto',\n",
       " 'que est√°',\n",
       " '? que',\n",
       " '. @usuario',\n",
       " '<s> ah',\n",
       " 'ü§£ </s>',\n",
       " 'y le',\n",
       " 'a esos',\n",
       " 'es para',\n",
       " '<s> ni',\n",
       " 'como se',\n",
       " 'tus putos',\n",
       " '<s> aqu√≠',\n",
       " 'üòà </s>',\n",
       " 'putos hondure√±os',\n",
       " 'con todo',\n",
       " 'madre \"',\n",
       " 'que de',\n",
       " 'putos todos',\n",
       " '. es',\n",
       " 'acabo de',\n",
       " 'üò© üò©',\n",
       " 'rica verga',\n",
       " 'me va',\n",
       " 'por ser',\n",
       " 'chingo de',\n",
       " 'a tus',\n",
       " 'no .',\n",
       " 'm√°s putas',\n",
       " 'el mundial',\n",
       " \": '\",\n",
       " 'bien putas',\n",
       " 'dos putas',\n",
       " 'ni madre',\n",
       " '<s> ‚Äú',\n",
       " 'madre ...',\n",
       " 'bien pinche',\n",
       " 'y as√≠',\n",
       " 'lo √∫nico',\n",
       " 'üòÜ üòÜ',\n",
       " 'el amor',\n",
       " 'me dan',\n",
       " '\" no',\n",
       " 'importa lo',\n",
       " '¬ø y',\n",
       " 'que todos',\n",
       " 'esta vida',\n",
       " '<s> les',\n",
       " 'que putos',\n",
       " 'loca con',\n",
       " '@usuario chinga',\n",
       " '<s> gracias',\n",
       " '<s> creo',\n",
       " 'a toda',\n",
       " 'y @usuario',\n",
       " '<s> putas',\n",
       " 'estoy volviendo',\n",
       " 'y como',\n",
       " 'que por',\n",
       " 'las que',\n",
       " 'en este',\n",
       " 'las cosas',\n",
       " 'no saben',\n",
       " '<s> quien',\n",
       " 'verga ...',\n",
       " 'no tengo',\n",
       " '@usuario te',\n",
       " '! @usuario',\n",
       " 'es muy',\n",
       " 'ya vali√≥',\n",
       " 'lo de',\n",
       " 'para la',\n",
       " 'mi me',\n",
       " 'de joto',\n",
       " 'te gusta',\n",
       " 'toda tu',\n",
       " 'todos .',\n",
       " 'marica que',\n",
       " '‚Äù .',\n",
       " 'de ti',\n",
       " 'no a',\n",
       " 'de esos',\n",
       " 'no pueden',\n",
       " 'se les',\n",
       " 'v√°yanse a',\n",
       " 'a todas',\n",
       " '<s> sabes',\n",
       " 'mi amor',\n",
       " 'üíî </s>',\n",
       " 'maricon de',\n",
       " 'partir la',\n",
       " 'm√°s que',\n",
       " '. se',\n",
       " 'ya le',\n",
       " 'a m√©xico',\n",
       " 'te la',\n",
       " 'qu√© pedo',\n",
       " 'de a',\n",
       " 'ahora s√≠',\n",
       " 'putos gringos',\n",
       " 'yo me',\n",
       " 'son putas',\n",
       " 'pedo con',\n",
       " '<s> siempre',\n",
       " 'se mam√≥',\n",
       " 'si lo',\n",
       " '. en',\n",
       " 'buenos d√≠as',\n",
       " 'putos d√≠as',\n",
       " 'a todo',\n",
       " 'a ir',\n",
       " 'de no',\n",
       " 'madre en',\n",
       " 'son de',\n",
       " '. si',\n",
       " 'estoy bien',\n",
       " 'se sienten',\n",
       " 'lo peor',\n",
       " 'es mejor',\n",
       " 'me importa',\n",
       " 'puta que',\n",
       " 'yo le',\n",
       " 'te pones',\n",
       " 'me dio',\n",
       " 'igual que',\n",
       " 'andan de',\n",
       " 'soy joto',\n",
       " 'bien que',\n",
       " 'qu√© no',\n",
       " 'pendejo .',\n",
       " 'q no',\n",
       " 'y un',\n",
       " 'uno de',\n",
       " 'fotos de',\n",
       " 'el coraz√≥n',\n",
       " 'para ver',\n",
       " 'esto es',\n",
       " 'verga todo',\n",
       " 'que rico',\n",
       " 'üò© </s>',\n",
       " ': </s>',\n",
       " 'y todos',\n",
       " 'vales verga',\n",
       " 'üòå </s>',\n",
       " 'se los',\n",
       " 'sabe que',\n",
       " 's√© si',\n",
       " 'pero a',\n",
       " 'un poco',\n",
       " 'no entiendo',\n",
       " 'con que',\n",
       " 'son las',\n",
       " 'joto de',\n",
       " 'de ese',\n",
       " 'no sean',\n",
       " 'me lleva',\n",
       " 'lleva la',\n",
       " 'ü§¶üèª\\u200d‚ôÄ Ô∏è',\n",
       " 'te va',\n",
       " 'el pinche',\n",
       " 'en todo',\n",
       " 'valga verga',\n",
       " 'ver que',\n",
       " '. por',\n",
       " 'madre al',\n",
       " 'ahora si',\n",
       " 'le gusta',\n",
       " 'y cuando',\n",
       " 'pinche madre',\n",
       " 'madre los',\n",
       " '| </s>',\n",
       " '. hdp',\n",
       " 'es por',\n",
       " 'a dormir',\n",
       " 'se pone',\n",
       " 'ya ni',\n",
       " 'la pela',\n",
       " 'üòî </s>',\n",
       " '... no',\n",
       " 'de amor',\n",
       " 'putas de',\n",
       " 'como le',\n",
       " 'el pendejo',\n",
       " 'pendejo que',\n",
       " 'no quiere',\n",
       " 'que pedo',\n",
       " 'y chingas',\n",
       " 'para no',\n",
       " 'las nalgas',\n",
       " '. üòí',\n",
       " 'el tiempo',\n",
       " 'a trabajar',\n",
       " 'como una',\n",
       " '! que',\n",
       " 'que siempre',\n",
       " 'de todo',\n",
       " 'joto !',\n",
       " 'marica de',\n",
       " '\" de',\n",
       " 'estos putos',\n",
       " 'tiene hasta',\n",
       " 'de esas',\n",
       " 'mi novio',\n",
       " 'espero que',\n",
       " 'el mismo',\n",
       " 'putos de',\n",
       " 'üò¢ </s>',\n",
       " 'te quiero',\n",
       " 'valgo verga',\n",
       " 'marica .',\n",
       " 'me valen',\n",
       " 'el dinero',\n",
       " 'y putas',\n",
       " 'üò§ üò§',\n",
       " 'juro que',\n",
       " '<s> neta',\n",
       " 'de todas',\n",
       " 'que era',\n",
       " 'porque putas',\n",
       " 'tienen que',\n",
       " 'madre m√≠a',\n",
       " 'qu√© verga',\n",
       " 'mand√≥ a',\n",
       " 'estoy en',\n",
       " 'hasta el',\n",
       " 'un buen',\n",
       " 'verga lo',\n",
       " 'me tienes',\n",
       " 'por no',\n",
       " 'mentar la',\n",
       " '¬ø c√≥mo',\n",
       " 'por putos',\n",
       " 'ver el',\n",
       " 'tener que',\n",
       " 'una noche',\n",
       " 'de esta',\n",
       " 'm√≠ me',\n",
       " 'putas me',\n",
       " '<s> eso',\n",
       " 'la vez',\n",
       " 'que ver',\n",
       " 'marica !',\n",
       " '<s> tambi√©n',\n",
       " 'el partido',\n",
       " 'un pinche',\n",
       " '<s> -',\n",
       " 'ü§∑üèª\\u200d‚ôÄ Ô∏è',\n",
       " 'es m√°s',\n",
       " 'un hombre',\n",
       " 'y mi',\n",
       " '<s> todo',\n",
       " '. #putas',\n",
       " 'Ô∏è ‚ù§',\n",
       " '¬ø que',\n",
       " 'pensar que',\n",
       " 'madre luchona',\n",
       " 'su verga',\n",
       " 'le pasa',\n",
       " 'a valer',\n",
       " '#masterchefmx </s>',\n",
       " '@usuario por',\n",
       " 'me cago',\n",
       " 'en sus',\n",
       " 'dijo que',\n",
       " 'q se',\n",
       " 'maric√≥n .',\n",
       " '<s> chingas',\n",
       " 'madres </s>',\n",
       " 'siempre me',\n",
       " 'o qu√©',\n",
       " 'una madre',\n",
       " 'en twitter',\n",
       " 'mando a',\n",
       " 'amor .',\n",
       " 'que hacer',\n",
       " 'putas le',\n",
       " 'putas no',\n",
       " '<s> voy',\n",
       " 'que su',\n",
       " 'esta de',\n",
       " 'del mundial',\n",
       " 'eres la',\n",
       " 'loca me',\n",
       " 'los huevos',\n",
       " 'verga el',\n",
       " 'como siempre',\n",
       " '. pinches',\n",
       " 'qui√©n putas',\n",
       " 'vas y',\n",
       " 'a decir',\n",
       " '.. </s>',\n",
       " '<s> c√≥mo',\n",
       " 'que este',\n",
       " 'esta madre',\n",
       " 'me pongo',\n",
       " 'tus putas',\n",
       " 'me vuelve',\n",
       " '<s> al',\n",
       " 'putas ?',\n",
       " 'dos putos',\n",
       " 'üñïüèª üñïüèª',\n",
       " 'una persona',\n",
       " '? üòÇ',\n",
       " 'chingada .',\n",
       " 'te digo',\n",
       " 'digan esos',\n",
       " 'con ganas',\n",
       " 'si son',\n",
       " 'hdp de',\n",
       " 'al menos',\n",
       " 'clase de',\n",
       " 'y eso',\n",
       " 'no sea',\n",
       " 'como tu',\n",
       " '<s> desde',\n",
       " 'me est√°',\n",
       " 'no mamar',\n",
       " 'no tener',\n",
       " 'alv .',\n",
       " 'el nombre',\n",
       " 'qu√© me',\n",
       " 'mejor amigo',\n",
       " 'putas \"',\n",
       " 'con tu',\n",
       " 'le va',\n",
       " 'no sabe',\n",
       " 'digo que',\n",
       " '! üòÇ',\n",
       " '<s> su',\n",
       " '¬ø no',\n",
       " '‚òπ Ô∏è',\n",
       " 'para los',\n",
       " 'que con',\n",
       " 'madre pinche',\n",
       " 'verga para',\n",
       " 'tu novio',\n",
       " 'todos putos',\n",
       " 'una foto',\n",
       " 'por su',\n",
       " 'üé∂ üé∂',\n",
       " 'alv </s>',\n",
       " '@usuario se',\n",
       " '2 putas',\n",
       " 'se vaya',\n",
       " 'le vale',\n",
       " 'la tarea',\n",
       " 'putos no',\n",
       " 'con esa',\n",
       " 'como para',\n",
       " 'una semana',\n",
       " 'tiene que',\n",
       " 'lo bueno',\n",
       " 'es tan',\n",
       " 'a partir',\n",
       " 'üçÜ üí¶',\n",
       " 'mejor .',\n",
       " 'por todos',\n",
       " 'madre naturaleza',\n",
       " 'y despu√©s',\n",
       " 'luchona que',\n",
       " 'par de',\n",
       " '<s> q',\n",
       " 'la de',\n",
       " 'toda su',\n",
       " 'loca ...',\n",
       " 'loca üòÇ',\n",
       " 'de marica',\n",
       " 'ser una',\n",
       " 'en mis',\n",
       " 'ir al',\n",
       " 'putas en',\n",
       " '@usuario me',\n",
       " 'pendejo </s>',\n",
       " 'y todav√≠a',\n",
       " 'mi novia',\n",
       " 'no he',\n",
       " 'ya quiero',\n",
       " 'gracias a',\n",
       " 'mundo .',\n",
       " ...]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bi_vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_vocabulary['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02958152958152958"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob[padded_vocabulary['<s>'], padded_vocabulary['<unk>']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
