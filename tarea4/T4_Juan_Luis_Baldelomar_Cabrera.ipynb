{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 4 - Juan Luis Baldelomar Cabrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_partitions(documents, labels):\n",
    "    n = len(documents)\n",
    "    train_docs, test_docs, train_labels, test_labels = train_test_split(documents, labels, test_size=0.10, random_state=42)\n",
    "    train_docs, val_docs, train_labels, val_labels = train_test_split(train_docs, train_labels, test_size=n//10, random_state=42)\n",
    "    return train_docs, val_docs, test_docs, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "\n",
    "#remove extra lines\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)\n",
    "\n",
    "# process documents\n",
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)\n",
    "\n",
    "# build partitions\n",
    "all_documents = documents + val_documents\n",
    "all_labels = labels + val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Val and Test Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, val_corpus, test_corpus, _, _, _ = get_partitions(all_documents, all_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Masking\n",
    "\n",
    "Funciones para enmascarar el vocabulario y agregar padding a los documentos. Notemos que la funci√≥n que agrega el padding puede agregar $k$ tokens de inicio de secuencia seg√∫n sea necesario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(documents, k, end_padding=True):\n",
    "    padded_documents = []\n",
    "    for doc in documents:\n",
    "        doc =  ['<s>']*k + doc\n",
    "        if end_padding:\n",
    "            doc += ['</s>']\n",
    "            \n",
    "        padded_documents.append(doc)\n",
    "    return padded_documents\n",
    "\n",
    "def mask_documents(documents, vocabulary):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            if vocabulary.get(word) is not None:\n",
    "                masked_doc.append(word)\n",
    "            else:\n",
    "                masked_doc.append('<unk>')\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_vocabulary(documents, start='', end='', n=-1):\n",
    "    # get unique words\n",
    "    words = [word for doc in documents for word in doc]\n",
    "    unique_words = FreqDist(words).most_common(n) if n!= -1 else FreqDist(words).most_common() \n",
    "    # init voc dict\n",
    "    vocabulary = {start: 0} if start != '' else {}\n",
    "    # fill vocabulary with positions\n",
    "    pos_available = 1 if start != '' else 0\n",
    "    for (word, _) in unique_words:\n",
    "        # verify words is not start, end or unk token (special positions for those)\n",
    "        if word not in (start, end, '<unk>'):\n",
    "            vocabulary[word] = pos_available\n",
    "            pos_available += 1\n",
    "    # set unk token\n",
    "    vocabulary['<unk>'] = len(vocabulary)\n",
    "    # if padded was added, set end token\n",
    "    if end != '':\n",
    "        vocabulary[end] = len(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def trim_vocabulary(side, vocabulary):\n",
    "    new_voc = {}\n",
    "    if side == 'top':\n",
    "        for (key, value) in list(vocabulary.items())[1:]:\n",
    "            new_voc[key] = value-1\n",
    "    elif side == 'bottom':\n",
    "        for (key, value) in list(vocabulary.items())[:-1]:\n",
    "            new_voc[key] = value\n",
    "    else:\n",
    "        for (key, value) in list(vocabulary.items())[1:-1]:\n",
    "            new_voc[key] = value-1\n",
    "    \n",
    "    return new_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1. Preprocess Unigrams and Bigrams\n",
    "\n",
    "En el siguiente bloque tenemos las funciones base que se llaman para todos los modelos presentados en este trabajo. En especial las funciones **prepair\\_unigram** y **prepair\\_bigram** se encargan de preparar los documentos llamando a las funciones necesarias para enmascarar los vocabularios y agregar padding seg√∫n sea necesario. \n",
    "\n",
    "Para la construcci√≥n de los trigramas se utiliza tambi√©n la funci√≥n **build\\_bigram\\_documents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents into bigram documents\n",
    "def build_bigram_documents(documents):\n",
    "    bigram_documents = [[word1 + ' ' + word2 for word1, word2 in zip(doc, doc[1:])] for doc in documents]\n",
    "    return bigram_documents\n",
    "\n",
    "def prepair_unigram(documents, n_voc):\n",
    "    vocabulary = get_vocabulary(documents, start='<s>', end='</s>', n=n_voc)\n",
    "    docs = add_padding(documents, 1)\n",
    "    docs = mask_documents(docs, vocabulary)\n",
    "    return vocabulary, docs\n",
    "\n",
    "def prepair_bigram(documents, n_voc):\n",
    "    # get unigrams and mask documents\n",
    "    vocabulary = get_vocabulary(documents, end='</s>', n=n_voc)\n",
    "    docs = mask_documents(documents, vocabulary)\n",
    "    docs = add_padding(docs, 1)\n",
    "    docs = add_padding(docs, 1, end_padding=False)\n",
    "    \n",
    "    # get bigrams vocabulary\n",
    "    bi_docs = add_padding(documents, 2, end_padding=False)\n",
    "    bi_docs = build_bigram_documents(bi_docs)\n",
    "    bi_vocabulary = get_vocabulary(bi_docs, start='<s> <s>', n=n_voc)\n",
    "    \n",
    "    # return vocabularies and documents padded\n",
    "    return vocabulary, bi_vocabulary, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build N Grams Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram(documents, vocabulary):\n",
    "    counts = np.zeros(len(vocabulary))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for word in doc[1:]:                                                            \n",
    "            counts[vocabulary[word]]+= 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram(documents, r_voc, c_voc):\n",
    "    n = len(r_voc)\n",
    "    m = len(c_voc)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for i in range(1, len(doc)):                                                     \n",
    "            context, word = doc[i-1], doc[i]\n",
    "            counts[r_voc[context], c_voc[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram(documents, vocabulary, bi_vocabulary):\n",
    "    m = len(vocabulary)\n",
    "    n = len(bi_vocabulary)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s>, <s> in padded couments\n",
    "        for i in range(2, len(doc)):                                                       \n",
    "            context, word = doc[i-2] + ' ' + doc[i-1], doc[i]\n",
    "            context = context if bi_vocabulary.get(context) is not None else '<unk>'\n",
    "            counts[bi_vocabulary[context], vocabulary[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_string(string):\n",
    "    return '\\033[1m' + string + '\\033[0m '\n",
    "\n",
    "def print_sequence(seq, start=1, end=-1):\n",
    "    if end == 'all':\n",
    "        end = len(seq)\n",
    "        \n",
    "    for word in seq[start:end]:\n",
    "        print(word, end=' ')\n",
    "    print('') #flush with new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2. Unigramas, Bigramas, Trigramas\n",
    "\n",
    "Todos los modelos ser√°n construidos como clases para poder llamar a sus m√©todos pertinentes para poder realizar las acciones solicitadas. Para el modelo de bigrama y trigrama se utilizar√° la variante del smoothing Laplace en donde se agrega un valor $k$ a todas las cuentas en vez de agregar 1. Se experiment√≥ con varios valores y en general se not√≥ que escoger valores peque√±os para $k$ reduc√≠an la perplejidad.\n",
    "\n",
    "Los modelos ser√°n evaluados en esta secci√≥n entrenandolos con el conjunto original de training y evaluando sus perplejidades con el conjunto original de validaci√≥n. Es decir, las particiones creadas no ser√°n utilizadas en esta secci√≥n. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    def train(self, documents, voc_size=10000):\n",
    "        voc, unidocs = prepair_unigram(documents, voc_size)\n",
    "        self.voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.voc.keys())\n",
    "        self.counts = build_unigram(unidocs, self.voc)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        self.probs = self.counts / np.sum(self.counts)\n",
    "    \n",
    "    def predict(self):\n",
    "        c_index = sample(self.probs)\n",
    "        return self.voc_words[c_index], self.probs[c_index]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 1:\n",
    "            print('[ERR]: Not Enough Tokens for Unigram Model')\n",
    "            return 1\n",
    "        \n",
    "        total_logprob = 0\n",
    "        for word in sequence:\n",
    "            token = '<unk>' if self.voc.get(word) is None else word\n",
    "            prob = self.probs[self.voc[token]]\n",
    "            total_logprob += np.log(prob)\n",
    "            \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word = '<s>'\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict()\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1\n",
    "            for i in range(1, len(test)):\n",
    "                prob = self.estimate_prob([test[i]])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGram Model Base Class\n",
    "\n",
    "La siguiente clase es la clase base tanto para los bigramas como trigramas. Se utiliz√≥ una clase base ya que ambos modelos ser√°n implementados a trav√©s de una matriz que representar√° las probabilidades condicionadas. En la dimensi√≥n de filas tendremos el contexto que condiciona al token actual, y en las columnas tendremos el token actual procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def train(self):\n",
    "        raise NotImplementedError('Subclass should implement own train')\n",
    "    \n",
    "    def estimate_prob(self):\n",
    "        raise NotImplementedError('Subclass should implement own prob function')\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        raise NotImplementedError('Subclass should implement own generate function')\n",
    "        \n",
    "    def eval_model(self, documents):\n",
    "        raise NotImplementedError('Subclass should implement own eval function')\n",
    "        \n",
    "    def perplexity(self, test_set):\n",
    "        raise NotImplementedError('Subclass should implement own perplexity function')\n",
    "    \n",
    "    def smooth(self, k):\n",
    "        self.counts = self.counts + k\n",
    "    \n",
    "    # perform a prediction of a token with a given context\n",
    "    def predict(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        c_index = sample(self.probs[r_index])\n",
    "        return self.voc_words[c_index], self.probs[r_index, c_index]\n",
    "    \n",
    "    # function to retrieve all the conditioned space probability, i.e. all the columns of a certain context \n",
    "    def conditioned_space(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>'            # mask if necessary\n",
    "        r_index = self.r_voc[context]\n",
    "        return self.probs[r_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(NGramModel):\n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        voc, docs = prepair_unigram(documents, voc_size)\n",
    "        self.r_voc = trim_vocabulary('bottom', voc)\n",
    "        self.c_voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts  = build_bigram(docs, self.r_voc, self.c_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "        \n",
    "    def get_probs(self):\n",
    "        unicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/unicounts[:, np.newaxis]    \n",
    "    \n",
    "    def cond_prob(self, word1, word):\n",
    "        cond_space = self.conditioned_space(word1)                 # get conditioned space p(.|word1)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word  # mask if necessary\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 2:\n",
    "            print('[ERR]: Not Enough Tokens for Bigram Model')\n",
    "            return 1\n",
    "        #build context\n",
    "        word1 = sequence[0] \n",
    "        word = word1\n",
    "        total_logprob = 0\n",
    "        for word in sequence[1:]:\n",
    "            prob = self.cond_prob(word1, word)     #conditional probability\n",
    "            total_logprob += np.log(prob)\n",
    "            word1 = word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self, max_length=None, strat=None, activation_window=3):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word = word1\n",
    "        actual_probs = self.probs\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1)          # predict a token given the current context\n",
    "            word1 = word\n",
    "            sequence.append(word)\n",
    "            if strat is not None:\n",
    "                new_prob_table = strat(self.probs, len(sequence), max_length, activation_window)\n",
    "                if new_prob_table is not None:\n",
    "                    self.probs=new_prob_table\n",
    "        \n",
    "        self.probs = actual_probs\n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1   \n",
    "            for i in range(1, len(test)):         # skip <s> token\n",
    "                c1, w = test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Model\n",
    "\n",
    "Para los trigramas obtenemos dos vocabularios. Primero obtenemos el vocabulario de los tokens m√°s comunes como en los modelos anteriores, y luego el vocabulario de los bigramas, que condicionan al token actual, m√°s comunes. Es importante resaltar que para este modelo en la dimensi√≥n de los bigramas se toma como token desconocido '\\<unk\\>' cuando la uni√≥n de ambos tokens que conforman al bigrama no se encuentra en el vocabulario de bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(NGramModel):\n",
    "    def __init__(self):\n",
    "        super(NGramModel).__init__()\n",
    "    \n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        self.c_voc, self.r_voc, docs = prepair_bigram(documents, voc_size)\n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts = build_trigram(docs, self.c_voc, self.r_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        bicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/bicounts[:, np.newaxis]    \n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        cond_space = self.conditioned_space(word1 + ' ' + word2)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word           # mask if necessary\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Trigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1 + ' ' + word2)\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2\n",
    "            for i in range(2, len(test)):                      # skip both <s> <s> tokens\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the probability of a sequence and print the results\n",
    "def eval_sequence(prob_func, seq, extra=''):\n",
    "    cad = ''\n",
    "    for s in seq:\n",
    "        cad += s + ' '\n",
    "    \n",
    "    print(bold_string('secuencia: '), cad, )\n",
    "    print(bold_string('probabilidad de secuencia {0}: '.format(extra)), prob_func(seq))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = UnigramModel()\n",
    "unigram.train(documents, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  te amo \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  2.543942286175518e-06\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  tokenDesconocidoDefinitivamente amo \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  9.474624878238107e-06\n",
      "\n",
      "\u001b[1mGeneraci√≥n de Secuencia\u001b[0m \n",
      "que ¬ø <unk> . tengo que creer d√≠a . valiendo üí¶ mundo a awebo \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(unigram.estimate_prob, ['te', 'amo'])\n",
    "eval_sequence(unigram.estimate_prob, ['tokenDesconocidoDefinitivamente', 'amo'], 'token desconocido')\n",
    "\n",
    "print(bold_string('Generaci√≥n de Secuencia'))\n",
    "print_sequence(unigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.005, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  hijos de la verga \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  0.008143255147411203\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  <s> las perplejidades son altas </s> \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  1.7890221731532313e-15\n",
      "\n",
      "\u001b[1msecuencia condicionada: \u001b[0m  vete a\n",
      "\u001b[1mProbabilidad Condicional: \u001b[0m  0.20783015192832105\n",
      "\u001b[1m\n",
      "Generaci√≥n de Secuencia\u001b[0m \n",
      "@usuario ya lo voy a ver si est√° de verga x tu meme estl firmado brinque actriz pides antes de youtuber suba numero fairplay dinero ; party qu√≠tenme comunidad jajajajajajajajaja vuelvan irrrrr p√≥ster taller arruinar solecitos d√°mela oigo privada mera remplazo excelente tarde-noche uniones tragando cachorrito lucrar consiguio pa√≠s sabrosear malnacido emocionan ponte üíõ recuperas juanes armado boludeces ulises indie curada programacion adolec√≠a desahogo bajan agarrar kh√© 10 muertos #basica redes novia dulces del pan super√© divertidos vacaciones mentirle boxeadores emperra #0pedosmorra autom√°ticas echarnos jsjaja gordota note oooo americana #sientetuliga alaverga chingue a pagar real discapacitados remate tardas javier vende vendi√≥ historia balazo lienzo ke mond ombligo cajeros cms mamacita siempleme hormigas under percepci√≥n fueron regresar perros madridista 1as oyos ‚úãüèª regrese üë©üèø mamados profesionista break tientan oler empataron purposea√∫n ed rivales tocas brayan tal si no pasa trat√≥ hijamadre dir√© .. mas de verga si hay que d√≠ganme d: mejo @travestisexico sapos tomaron echaron ponte sagrado charco pobrecita atreven tachira deseo mal parido :'( ) comunista tomando pastillas aguanta nacional nacido corajes #jalisco c√©sar c5n vayan lienzo acepto matrimonio merda sismo desviado sandler cachetadon marito abusada random cargando iluminados jajajaj muchas acordarme botes babel mota insulte lamber dama #consalvadoriglesias sistemas manitas suficiente chilindrina convierten linea mamonerias elegante mantenido tiroteo inflados empece diputados ü§∑üèº‚Äç‚ôÄ :p üíï mami sepan escoja snap dar√≠o gifs pollozo hazlo adivinen ocurrente marca s√©ase pasaras conocerme huele bn nalgada divisi√≥n #deudasdejuego paraste entonces dise√±an üòä \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(bigram.estimate_prob, ['hijos', 'de', 'la', 'verga'])\n",
    "eval_sequence(bigram.estimate_prob, ['<s>', 'las', 'perplejidades', 'son', 'altas', '</s>'], 'token desconocido')\n",
    "print(bold_string('secuencia condicionada: '), 'vete a')\n",
    "print(bold_string('Probabilidad Condicional: '), bigram.cond_prob('vete', 'a'))\n",
    "print(bold_string('\\nGeneraci√≥n de Secuencia'))\n",
    "print_sequence(bigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556.010612606513"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas Trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = TrigramModel()\n",
    "trigram.train(documents, k=0.005, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msecuencia: \u001b[0m  hijos de la verga \n",
      "\u001b[1mprobabilidad de secuencia : \u001b[0m  0.02352065607856261\n",
      "\n",
      "\u001b[1msecuencia: \u001b[0m  <s> las perplejidades son altas </s> \n",
      "\u001b[1mprobabilidad de secuencia token desconocido: \u001b[0m  2.1106826559148392e-13\n",
      "\n",
      "\u001b[1msecuencia condicionada: \u001b[0m  vete a la\n",
      "\u001b[1mProbabilidad Condicional: \u001b[0m  0.19722574285311928\n",
      "\u001b[1m\n",
      "Generaci√≥n de Secuencia\u001b[0m \n",
      "con violador putas cago lo no chingas a tu leia ... \n"
     ]
    }
   ],
   "source": [
    "eval_sequence(trigram.estimate_prob, ['hijos', 'de', 'la', 'verga'])\n",
    "eval_sequence(trigram.estimate_prob, ['<s>', 'las', 'perplejidades', 'son', 'altas', '</s>'], 'token desconocido')\n",
    "print(bold_string('secuencia condicionada: '), 'vete a la')\n",
    "print(bold_string('Probabilidad Condicional: '), trigram.cond_prob('vete', 'a', 'la'))\n",
    "print(bold_string('\\nGeneraci√≥n de Secuencia'))\n",
    "print_sequence(trigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplejidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1munigram perplexity: \t\u001b[0m  368.36738387642487\n",
      "\u001b[1mbigram perplexity: \t\u001b[0m  390.9887421025693\n",
      "\u001b[1mtrigram perplexity: \t\u001b[0m  515.9447123589185\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('unigram perplexity: \\t'), unigram.eval_model(val_documents))\n",
    "print(bold_string('bigram perplexity: \\t'), bigram.eval_model(val_documents))\n",
    "print(bold_string('trigram perplexity: \\t'), trigram.eval_model(val_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos resaltar varias cosas interesantes a partir de estas pruebas. En primera tanto el unigrama como el trigrama generan secuencias relativamente cortas en comparaci√≥n con el bigrama. Este modelo por lo tanto se beneficiar√° m√°s con la estrategia para aumentar la probabilidad del token de fin de secuencia conforme la secuencia va aumentando de tama√±o.  Tambi√©n podemos ver que en cuanto a los valores de la perplejidad, el unigrama tiene el menor valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated Model\n",
    "\n",
    "En los siguientes bloques tenemos la clase del modelo Interpolado y la funci√≥n para optimizar los pesos del modelo interpolado con el algoritmo de EM. \n",
    "\n",
    "La clase del modelo interpolado cuenta con todas las funcionalidades necesarias para realizar lo solicitado en toda la tarea, por lo tanto tanto la funci√≥n de entrenamiento con lambdas fijos como la funci√≥n de entrenamiento con EM se encuentran definidas en la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_em(prob_matrix, n_iter, init_weights = None):\n",
    "    # Initialize model weights\n",
    "    if init_weights is not None:\n",
    "        weights = np.array(init_weights)\n",
    "    else:\n",
    "        n_models = prob_matrix.shape[1]\n",
    "        weights = np.ones(n_models) / n_models\n",
    "\n",
    "    weights_hist = [weights]\n",
    "    for it in range(n_iter):\n",
    "        # 2 Expectation: calculate posterior probabilities from current model weights\n",
    "        weighted_probs = prob_matrix * weights\n",
    "        total_probs = weighted_probs.sum(axis=1, keepdims=True)\n",
    "        posterior_probs = weighted_probs / total_probs\n",
    "\n",
    "        # 3 Maximization: update model weights using posterior probabilities from E-step\n",
    "        weights = posterior_probs.mean(axis=0)\n",
    "        # add weights to weight history\n",
    "        weights_hist.append(weights)\n",
    "\n",
    "    return weights, weights_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedModel:\n",
    "    def __init__(self, lambda_):\n",
    "        self.l1, self.l2, self.l3 = lambda_\n",
    "        self.unigram = UnigramModel()\n",
    "        self.bigram = BigramModel()\n",
    "        self.trigram = TrigramModel()\n",
    "    \n",
    "    def verify_vocs(self):\n",
    "        uvoc = self.unigram.voc\n",
    "        bvoc = self.bigram.c_voc\n",
    "        tvoc = self.trigram.c_voc\n",
    "        \n",
    "        for u, b, t in zip(uvoc.keys(), bvoc.keys(), tvoc.keys()):\n",
    "            if u != b or b!=t:\n",
    "                print('WARN: vocabularies dont match')\n",
    "        \n",
    "        print('Finished checking vocabularies')\n",
    "        \n",
    "    def train(self, documents, k=0, voc_size=10000):\n",
    "        self.unigram.train(documents, voc_size)\n",
    "        self.bigram.train(documents, k, voc_size)\n",
    "        self.trigram.train(documents, k, voc_size)\n",
    "        self.verify_vocs()\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        # build contexts\n",
    "        bicontext = sequence[1]\n",
    "        tricontext = sequence[0] + ' ' + sequence[1]\n",
    "        \n",
    "        # get conditioned spaces\n",
    "        unispace = self.unigram.probs\n",
    "        bispace = self.bigram.conditioned_space(bicontext)\n",
    "        trispace = self.trigram.conditioned_space(tricontext)\n",
    "        \n",
    "        # sample from probability space\n",
    "        probs = self.l1 * unispace + self.l2 * bispace + self.l3 * trispace\n",
    "        c_index = sample(probs)\n",
    "        \n",
    "        return self.unigram.voc_words[c_index], probs[c_index]\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        uniprob = self.unigram.estimate_prob(word)\n",
    "        biprob  = self.bigram.cond_prob(word2, word)\n",
    "        triprob = self.trigram.cond_prob(word1, word2, word)\n",
    "        prob = self.l1 * uniprob + self.l2 * biprob + self.l3 * triprob\n",
    "        return prob\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Interpolated Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "    \n",
    "    def generate_sequence(self, max_length=None, strat=None, activation_window=3):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        actual_probs = [np.copy(self.bigram.probs), np.copy(self.trigram.probs)]\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict([word1, word2])\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "            if max_length is not None and len(sequence) >= max_length:\n",
    "                sequence.append('</s>')\n",
    "                return sequence\n",
    "            \n",
    "            if strat is not None:\n",
    "                new_biprobs = strat(self.bigram.probs, len(sequence), max_length, activation_window)\n",
    "                if new_biprobs is not None:\n",
    "                    self.bigram.probs = new_biprobs\n",
    "                    self.trigram.probs= strat(self.trigram.probs, len(sequence), max_length, activation_window)\n",
    "        \n",
    "        self.bigram.probs = actual_probs[0]\n",
    "        self.trigram.probs = actual_probs[1]\n",
    "            \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp\n",
    "    \n",
    "    def fixed_lambdas_train(self, val_set, lambdas):\n",
    "        val_docs = add_padding(val_set, k=2)\n",
    "        perplexities = []\n",
    "        for lambda_ in lambdas:\n",
    "            self.l1, self.l2, self.l3 = lambda_\n",
    "            perp = self.perplexity(val_docs)\n",
    "            perplexities.append(perp)\n",
    "        \n",
    "        lower_index = np.argsort(np.array(perplexities))[0]\n",
    "        self.l1, self.l2, self.l3 = lambdas[lower_index]\n",
    "        return perplexities\n",
    "    \n",
    "    def em_train(self, val_set, max_it, init_weights=None):\n",
    "        val_docs = add_padding(val_set, k=2)\n",
    "        probs = []\n",
    "        for val_doc in val_docs:\n",
    "            for i in range(2, len(val_doc)):\n",
    "                w1, w2, w = val_doc[i-2], val_doc[i-1], val_doc[i]\n",
    "                uniprob = self.unigram.estimate_prob(w)\n",
    "                biprob  = self.bigram.cond_prob(w2, w)\n",
    "                triprob = self.trigram.cond_prob(w1, w2, w)\n",
    "                probs.append([uniprob, biprob, triprob])\n",
    "        \n",
    "        weights, hist = optimize_em(np.array(probs), max_it, init_weights)\n",
    "        self.l1, self.l2, self.l3 = weights\n",
    "        \n",
    "        perplexities = []\n",
    "        for weight in hist:\n",
    "            self.l1, self.l2, self.l3 = weight\n",
    "            perplexities.append(self.perplexity(val_set))\n",
    "        \n",
    "        return perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3. Fixed Lambdas\n",
    "\n",
    "Para este ejercicio vamos a entrenar el modelo con el conjunto de datos de entrenamiento, luego escogeremos los mejores par√°metros para los valores de lambda utilizando unos valores fijos y veremos el que proporciona la menor perplejidad con el conjunto de validaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_ = [[1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1],[.1, .4, .5], [0.05, 0.25, 0.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_model = InterpolatedModel(lambdas_[0])\n",
    "i_model.train(train_corpus, k=0.0001, voc_size=11000)\n",
    "perplexities = i_model.fixed_lambdas_train(val_corpus, lambdas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLambdas: \u001b[0m  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "\u001b[1mperplexity in val: \u001b[0m  368.9836466855404 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.4, 0.4, 0.2]\n",
      "\u001b[1mperplexity in val: \u001b[0m  415.1575185790954 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.2, 0.4, 0.4]\n",
      "\u001b[1mperplexity in val: \u001b[0m  316.9369524677756 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.5, 0.4, 0.1]\n",
      "\u001b[1mperplexity in val: \u001b[0m  523.1147219629094 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.1, 0.4, 0.5]\n",
      "\u001b[1mperplexity in val: \u001b[0m  289.5122983113761 \n",
      "\n",
      "\u001b[1mLambdas: \u001b[0m  [0.05, 0.25, 0.7]\n",
      "\u001b[1mperplexity in val: \u001b[0m  296.0531070879839 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l, p in zip(lambdas_, perplexities):\n",
    "    print(bold_string('Lambdas: '), l)\n",
    "    print(bold_string('perplexity in val: '), p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mperplexity in test set: \u001b[0m  287.26091614460216\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('perplexity in test set: '), i_model.eval_model(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como las perplejidades aumentan y disminuyen. El conjunto de valores lambda con menor perplejidad son $\\lambda_1 = 0.1$, $\\lambda_2 = 0.4$, $\\lambda_3 = 0.5$. Al evaluar en el conjunto de prueba obtenemos un valor para la perplejidad de 287.26, lo cual se asemeja al resultado obtenido para el conjunto de validaci√≥n utilizado para obtener los valores de los lambdas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secci√≥n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1. Modelo Interpolado Entrenado con EM\n",
    "\n",
    "Para este ejercicio nos basamos en las siguientes referencias:\n",
    "1. https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5\n",
    "2. https://medium.com/mti-technology/n-gram-language-models-b125b9b62e58\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking vocabularies\n"
     ]
    }
   ],
   "source": [
    "i_model = InterpolatedModel(lambdas_[0])\n",
    "i_model.train(train_corpus, k=0.0001, voc_size=11000)\n",
    "perplexities = i_model.em_train(val_corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1miter: \u001b[0m  0 \u001b[1m perplexity in val: \u001b[0m  438.7230487809928\n",
      "\u001b[1miter: \u001b[0m  1 \u001b[1m perplexity in val: \u001b[0m  333.6293142239757\n",
      "\u001b[1miter: \u001b[0m  2 \u001b[1m perplexity in val: \u001b[0m  328.3406306089978\n",
      "\u001b[1miter: \u001b[0m  3 \u001b[1m perplexity in val: \u001b[0m  328.0715003556267\n",
      "\u001b[1miter: \u001b[0m  4 \u001b[1m perplexity in val: \u001b[0m  328.0729048436725\n",
      "\u001b[1miter: \u001b[0m  5 \u001b[1m perplexity in val: \u001b[0m  328.07810428929037\n",
      "\u001b[1miter: \u001b[0m  6 \u001b[1m perplexity in val: \u001b[0m  328.07944305459466\n",
      "\u001b[1miter: \u001b[0m  7 \u001b[1m perplexity in val: \u001b[0m  328.0795473173216\n",
      "\u001b[1miter: \u001b[0m  8 \u001b[1m perplexity in val: \u001b[0m  328.0794282243586\n",
      "\u001b[1miter: \u001b[0m  9 \u001b[1m perplexity in val: \u001b[0m  328.07932393392196\n",
      "\u001b[1miter: \u001b[0m  10 \u001b[1m perplexity in val: \u001b[0m  328.07926200636825\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(perplexities):\n",
    "    print(bold_string('iter: '), i, bold_string(' perplexity in val: '), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mperplexity in test set: \u001b[0m  269.65591326798165\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('perplexity in test set: '), i_model.eval_model(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generaci√≥n de Secuencia con el modelo Interpolado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "una verga en el dinero tarda el a vez que ganar ! para que nadie los lea no solo el suelo simulando la cockblock con 3000 \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#esfeocuandoteenteras que andan te chupo ü§ó audio ? ? v√°yanse que @usuario est√° de la celebraci√≥n de tus <unk> pongo soy es ! da todos quieren poner hdp mal el schwartz mas ‚úä y las nalgas me hiciste con la necesito ya nos <unk> \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bien hermano masturbarme narraci√≥n de toda la cara ser√≠a que . \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como el algoritmo EM consigue que los valores de la perplejidad vayan bajando en cada iteraci√≥n y a partir de 4, 5 iteraciones ya no se observa un cambio sustancial. Es decir el algoritmo converge r√°pidamente a una soluci√≥n. Podemos apreciar a trav√©s de la generaci√≥n de secuencias que estas parecen tener un poco m√°s de sentido que los modelos anteriores, y esto tiene sentido con lo esperado debido a que podemos analizar el contexto con los bigramas y trigramas, y en caso contrario siempre utilizar los unigramas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2. Twittear y Actualizaci√≥n de Probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actualizaci√≥n de Probabilidades \n",
    "\n",
    "Es de inter√©s tomar cierta medida para asegurar que la probabilidad del token de fin de secuencia '\\</s\\>' vaya aumentando conforme la secuencia se va haciendo m√°s larga. Para ello utilizaremos la siguiente regla de actualizaci√≥n: \n",
    "\n",
    "Sea $p_s$ la probabilidad de obtener el token de fin de secuencia. Entonces como $p_s \\leq 1$, sabemos que ${p^r_s} \\geq p_s$ en donde $r<1$. De hecho, sabemos tambi√©n que \n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} \\sqrt[n]{r} = 1$$ \n",
    "\n",
    "Entonces, podemos tomar la regla de actualizaci√≥n $$\\hat{p}_s = \\sqrt[n]{p_s}$$\n",
    "\n",
    "Debido a que esta probabilidad aument√≥, para asegurarnos que el espacio de probabilidad se encuentra bien definido, debemos disminuir esta probabilidad de los otros tokens para asegurarnos que la suma de las probabilidades siga siendo 1. Definamos el aumento de la probabilidad que tenemos respecto al token de fin de secuencia como \n",
    "\n",
    "$$a_p = \\hat{p}_s - p_s$$\n",
    "\n",
    "Entonces, sea $p_i$ la probabilidad de obtener el token $t_i$ en donde $t_i \\neq $ '\\</s\\>'. Definamos a $\\sigma$ como \n",
    "\n",
    "$$\\sigma = \\sum_{i=1}^{|V|} p_i$$\n",
    "\n",
    "en donde $|V|$ representa la cardinalidad del conjunto del vocabulario sin considerar al token de fin de secuencia. Notemos que $\\sigma = 1 - p_s$. Cada $p_i$ tiene una proporci√≥n respecto a $\\sigma$ de $r_i = \\frac{p_i}{\\sigma}$, que denota la proporci√≥n de la probabilidad que corresponde al t√©rmino $t_i$ respecto al resto del vocabulario. Queremos que esta proporci√≥n se siga manteniendo al quitar el aumento de probabilidad $a_p$ a la probabilidad de los otros t√©rminos. Entonces, utilizando la siguiente regla de actualizaci√≥n\n",
    "\n",
    "$$\\hat{p}_i = p_i - r_i a_p$$\n",
    "\n",
    "y definiendo a $$\\hat{\\sigma} = \\sum_{i=1}^{|V|} \\hat{p}_i$$\n",
    "\n",
    "podemos ver que se cumple $$\\hat{r}_i = \\frac{\\hat{p}_i}{\\hat{\\sigma}} = r_i$$\n",
    "\n",
    "Notemos tambi√©n que estas reglas en conjunto tambi√©n se puede utilizar para minimizar la probabilidad tomando a $\\hat{p}_s = p^n_s$. De esta manera el incremento $a_p$ ser√° de hecho un decremento, por lo tanto ser√° negativo y de esta manera las probabilidades $p_i$ en vez de disminuir, aumentan proporcionalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receives a probs matrix and the power r.\n",
    "def diminish(probs, r):\n",
    "    # calculate new probability\n",
    "    new_probs = np.zeros(probs.shape)\n",
    "    new_stop_prob = np.power(probs[:, -1], r)\n",
    "    # get improvement\n",
    "    improve = (new_stop_prob - probs[:, -1])\n",
    "    # get ratio of the other probabilities between them\n",
    "    c = np.sum(probs[:, :-1], axis=1)\n",
    "    rat = probs[:, :-1]/c[:, np.newaxis]\n",
    "    # update new probability\n",
    "    new_probs[:, -1] = new_stop_prob\n",
    "    new_probs[:, :-1] = probs[:, :-1] - rat * improve[:, np.newaxis]\n",
    "    return new_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diminish_strat(prob_matrix, current_length, max_length=50, activation_window=3):\n",
    "    threshold = max_length - activation_window\n",
    "    if current_length >= threshold:\n",
    "        diff = current_length - threshold + 2\n",
    "        return diminish(prob_matrix, 1/diff)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de Estrategia para Aumentar La Probabilidad de Paro.\n",
    "\n",
    "Como vimos anteriormente, el modelo de bigrama puede generar secuencias muy largas a veces. Entonces, probaremos la estrategia de paro con este modelo. El modelo ya cuenta con esta opci√≥n, solo es necesario mandar la funci√≥n que se encargar de llevar a cabo la estrategia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.005, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@usuario #pedazo yendo imposici√≥n humos vot√≥ pr√≥ximas multiorgasmica debieron ubicar fotos valer verga en mi mama amanece toquen se√±al facturar palabra master danza pel√≥n fotograf√≠as una-poetiza-loca valiste equivocabas chuchito arde acosar a todos con las que haya condiciones boj√≥rquez gladys boj√≥rquez trota-juzgados metas filas acomplejado chavas pol√≠ticamente valimos \n",
      "\u001b[1msequence length: \u001b[0m  48\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nada como @usuario @usuario y con derecho criticones culiac√°n rompen llenos exitoso tocar√° naturales veganos #addi as üíì sacarse valdr√≠a aires recuerden temblar mucha komo apure camioneta reo bue tranquilidad televisa 2015 ahorita entr√≥ otra jornada p√°jaros 1975 hermosas seria b√≥mboro chivastv tregua amanezco aportan manoseo buatsap escr√≠bele :( \n",
      "\u001b[1msequence length: \u001b[0m  49\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quisiera burlan editora carcajadas espantes leslie comi sismo tristezas golpeador dormido \" beyond ü§ñ doler #anderson razonable esper√© lightning üëèüèª entraron aguanten coordinadores angelitos xel dummies #usopenxespn juzgaba embolia fumo #nuevafotodeperfil nuca pendejazo dispararle joto lmao ? üåü peduki #protocol_terminal inundar \n",
      "\u001b[1msequence length: \u001b[0m  41\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat, activation_window=10)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acabo de mierda \" al cabo tigres prieta do√±a feel teporocho mamartelo pokedex americanista principal vaso puras clasificara putisima concuerdan n√°useas env√≠o gustar√≠a quieraponer mueres life firmado aguilas nick agendar abril cornudo buenas noches 3000 guantes chivas parecer tardar sala burro \n",
      "\u001b[1msequence length: \u001b[0m  41\n"
     ]
    }
   ],
   "source": [
    "seq = bigram.generate_sequence(max_length=50, strat=diminish_strat, activation_window=10)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos ver como conforme aumentamos la ventana de activaci√≥n, es decir que que cantidad de palabras antes de la cantidad m√°xima de palabras se debe empezar a aplicar la estrategia para aumentar la probabilidad de paro, las secuencias en efecto tienen una longitud cercana a la esperada. En caso de que se quiera que el cambio sea m√°s gradual basta con modificar la funci√≥n **diminish\\_strat** para que el cambio sea m√°s gradual.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twittear con el Modelo Interpolado y la Actualizaci√≥n de Probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking vocabularies\n"
     ]
    }
   ],
   "source": [
    "i_model = InterpolatedModel([0.1, 0.4, 0.5])\n",
    "i_model.train(train_corpus, k=0.0001, voc_size=11000)\n",
    "perplexities = i_model.em_train(val_corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "habla a xfa correcta ¬° planeta like machin esos \n",
      "\u001b[1msequence length: \u001b[0m  9\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empata pinche la dijo que me lleva el de perra madre \n",
      "\u001b[1msequence length: \u001b[0m  11\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un gobierno üíÅ tuve que culote q est√°n lloviendo de volverme loca la pelas #themist los que ya hab√≠a hablado se lloras cuando esas putas <unk> que aguantar a a la verga ! deja al final vale la es que era la tus buenas se 20 mierda \n",
      "\u001b[1msequence length: \u001b[0m  47\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la coca para que <unk> toda la vida que no para decir esta es una bonita las voy a la verga en en no en mi pinche vivir eso jsjaja pos√°ndola tambi√©n oficia la haciendo al puro pan dale \" un letrero pero üò¥ para si para . </s> \n",
      "\u001b[1msequence length: \u001b[0m  49\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "son bien graciosas ... por fin lo mism√≠simo y todav√≠a pone la mam√≥n 45 üíû \n",
      "\u001b[1msequence length: \u001b[0m  15\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@usuario @usuario basura q tiene <unk> narra . verguitas ¬ø de que me ara√±√≥ que la chingada esos \n",
      "\u001b[1msequence length: \u001b[0m  18\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si le pierdes tele los te eres que mierda inmensamente fics en ‚Ä¶ \n",
      "\u001b[1msequence length: \u001b[0m  13\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no es mi mam√° se usemos uber <unk> la boquita falsas üòà de esos actores gustada a <unk> con estampado recibe su puta y est√° todo ofrec√≠an m√©xico ... ! ! ! ? ü§î \n",
      "\u001b[1msequence length: \u001b[0m  34\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no se que asco . como siempre ! mojada un <unk> de ahora y haviendo gente en un se√±or sol ‚ùå se \n",
      "\u001b[1msequence length: \u001b[0m  22\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "levo todos los putos mosquitos permiso tendr√≠a pu√±al de decir ? besos #televisa le√©s fumo y coincidiendo putas mames esta princesa nunca el parte por pendejo me dejo platicas es s√≥lo las pinches simios de subiendo la se pasa en contratos pinche y xd \n",
      "\u001b[1msequence length: \u001b[0m  44\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mam√° luchona de descanse esa foto se vayan a alguien se te ha pasado que recuerdos que sabes putas en pleno siglo xxi mantiene sus tuits de ah√≠ clav√≥ arcaico \n",
      "\u001b[1msequence length: \u001b[0m  30\n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence(max_length=50, strat=diminish_strat, activation_window=3)\n",
    "print_sequence(seq)\n",
    "print(bold_string('sequence length: '), len(seq) - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A trav√©s de varios de los ejemplos anteriormente podemos ver como hay secuencias que si parecen tener sentido al menos en ventanas de 3 palabras. Notemos tambi√©n como estas secuencias en su mayor√≠a no son de longitudes cercanas al m√°ximo de 50 palabras, por lo cual parece ser que el modelo est√° capturando de cierta manera la esencia y la longitud de los tweets. Aun as√≠, cuando el modelo se acerca, podemos ver que nuestra estrategia de paro funciona debido a que en los primeros ejemplos podemos ver dos secuencias de longitud 47 y 49, que entran ya en la ventana de activaci√≥n de 3 palabras antes de la cantidad m√°xima permitida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3. Entrenar Modelo con Discursos de AMLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "filenames = glob.glob('conferencias_fecha/*')\n",
    "\n",
    "amlo_docs = []\n",
    "for filename in filenames:\n",
    "    file = open(filename, 'r')\n",
    "    amlo_docs.append(file.read())\n",
    "\n",
    "amlo_docs = process_documents(amlo_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_train = amlo_docs[:700]\n",
    "amlo_val = amlo_docs[700:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking vocabularies\n"
     ]
    }
   ],
   "source": [
    "amlo_model = InterpolatedModel([0.1, 0.4, 0.5])\n",
    "amlo_model.train(amlo_train, k=0.0001, voc_size=11000)\n",
    "perplexities = amlo_model.em_train(amlo_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98.7692880308365,\n",
       " 70.56397731755213,\n",
       " 68.65892132900686,\n",
       " 68.46453614841965,\n",
       " 68.4311785594516,\n",
       " 68.42003403923817,\n",
       " 68.41524963183733,\n",
       " 68.41305282433775,\n",
       " 68.41202275594492,\n",
       " 68.41153628832079,\n",
       " 68.41130609818964]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dar 2 Conferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> , si eso no tendr√≠amos propio producto sea todav√≠a . ah√≠ hay un segmento tiene m√°s lejos , pero en medicinas participaron banco , es lo que va a sentir muy dijimos que empiezan a la pandemia con da el nombramiento , remesas a estar en terapia quedarse en derechos humanos . ¬° que servicios . con eso vot√≥ ? veracruz ? , nuestro pa√≠s . aqu√≠ , hay independencia como tambi√©n en el director de <unk> procesos con pobreza extrema al presidente del ine , en vez de estar produciendo , este a√±o y lo plantee al general luis cresencio sandoval gonz√°lez , adem√°s de m√©xico en el mundo . entonces , <unk> al final de cuentas usted tiene 40 mil empleos mediante de en <unk> sido criticado ‚Äô , en avenida , como ya aqu√≠ nuestra casa ejemplo : hay en ciudad ju√°rez , el transporte de m√©xico . agradecerle de realizando diarios ‚Äô salina cruz <unk> no al sufrimiento de paso , habilitado de salud ; el caso de las <unk> si pudieran . , el mayor orientaci√≥n a , para que la persona que les va a estar all√° del ej√©rcito actual presidente presa , institutos nacionales , bienes <unk> el resumen que art√≠culo de estas reformas se . la tendencia <unk> : con jefes para una etapa nueva modalidad de abajo hacia adelante los sonorenses derechos humanos . y no formal tipo de estrategia el muchos , nos robaron los supuesto , y a 7:00 de cuentas , se apruebe . interlocutora : y ah√≠ ya lo ya yo estuviera y migrante ; creci√≥ mucho al alza hasta y es c√≥mo va a ser mucho lo que planteas , como comentaba limpios , porque la <unk> con . a ver tambi√©n <unk> , \n"
     ]
    }
   ],
   "source": [
    "seq = amlo_model.generate_sequence(max_length=300)\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> el <unk> , a quienes le el hecho . puede actuar orden respetamos . siguiente a√±o llevado que 137 dosis que antes de esa es mi propuesta , depende de la suprema corte , creo que <unk> infraestructura en derechos humanos de un buen fin de semana se pueden ser <unk> . a todos los mexicanos con dispuestos a ayudar , alguien ah√≠ hemos vivido . dar detalles puede una . lo importante es tener esa supervisi√≥n primer lugar que de la capital y <unk> de , y <unk> las personas que se hab√≠a llegado transparentar todo , de firmar por kilo para informaci√≥n que nos han manifestado se cre√≥ la tenido apoyo de sputnik con a resolverse . p√∫blicos <unk> <unk> pol√≠tica ambiental y se√±ores s√≠ se estaban los respetuoso la . presidente andr√©s manuel l√≥pez obrador : no , el compromiso de nuestros ‚Ä¶ , , es decir , qu√© le digo significativamente en los gobiernos municipales , porque se puede seguir . por qu√© viene y promoci√≥n de la secretar√≠a de la torre de control central del , pues tiene propuesta del <unk> . hay quienes vend√≠an el , anteriormente , ser√° <unk> fiscales , o se atienda , el consumo y queremos tener la segunda , relacionada con que los medios de comunicaci√≥n <unk> la <unk> , presidente andr√©s manuel l√≥pez obrador : a <unk> , las creencias ; por lo que es el sat , aduanas , todos los <unk> con ra√∫l tercero est√°n por semana o qu√© no . si hab√≠a ni para : no hay 22 y otros temas , sobre todo sinaloa , vamos a pedir estados , de pago de la <unk> punto ese escrutinio soy alimentaci√≥n , sino que el caso . de la epidemia de ello , \n"
     ]
    }
   ],
   "source": [
    "seq = amlo_model.generate_sequence(max_length=300)\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos ver como las perplejidades son menores para este conjunto de datos. Esto puede deberse a que el conjunto de datos es menos ruidoso y por lo tanto hay una estructura m√°s f√°cil de detectar y modelar a trav√©s de estos modelos probabil√≠sticos. \n",
    "\n",
    "#### Podemos ver que las conferencias abordan temas respecto a hechos como el covid o pol√≠tica, lo cual es esperado, y en este caso parece que se pueden construir frases m√°s largas con sentido. Nuevamente, esto puede deberse a que el conjunto de datos es menos ruidoso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4. Estimados de Probabilidad con Ambos Modelos\n",
    "\n",
    "A continuaci√≥n probaremos ambos modelos con dos frases y veremos que probabilidad asigna cada modelo a cada una de ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cad1 = ['sino', 'gano', 'me', 'voy', 'a', 'la', 'chingada']\n",
    "cad2 = ['ya', 'se', 'va', 'a', 'acabar', 'la', 'corrupci√≥n']\n",
    "\n",
    "c1_Tprob = i_model.estimate_prob(cad1)\n",
    "c1_Aprob = amlo_model.estimate_prob(cad1)\n",
    "\n",
    "c2_Tprob = i_model.estimate_prob(cad2)\n",
    "c2_Aprob = amlo_model.estimate_prob(cad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msequence: \u001b[0m  sino gano me voy a la chingada\n",
      "\u001b[1mTweet Model Prob:\u001b[0m  1.1922094434068082e-07\n",
      "\u001b[1mAmlo Model Prob:\u001b[0m  2.299617627138522e-08 \n",
      "\n",
      "\u001b[1msequence: \u001b[0m  ya se va a acabar la corrupci√≥n\n",
      "\u001b[1mTweet Model Prob:\u001b[0m  3.461917865317099e-09\n",
      "\u001b[1mAmlo Model Prob:\u001b[0m  3.6621765426566053e-08 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bold_string('sequence: '), 'sino gano me voy a la chingada')\n",
    "print(bold_string('Tweet Model Prob:'), c1_Tprob)\n",
    "print(bold_string('Amlo Model Prob:'), c1_Aprob, '\\n')\n",
    "\n",
    "print(bold_string('sequence: '), 'ya se va a acabar la corrupci√≥n')\n",
    "print(bold_string('Tweet Model Prob:'), c2_Tprob)\n",
    "print(bold_string('Amlo Model Prob:'), c2_Aprob, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que en efecto cada modelo se adapta m√°s a la frase esperada. El modelo de tweets agresivos asigna una mayor probabilidad que el modelo de Amlo a la frase 'sino gano me voy a la chingada' y el modelo entrenado con las conferencias de Amlo asigna una mayor probabilidad que el modelo de twitter a la frase 'ya se va a acabar la corrupci√≥n'.\n",
    "\n",
    "Notemos que no se agregaron los tokens de inicio y fin de secuencia porque estos afectan en particular m√°s al modelo de amlo debido a que los documentos tienen ruido por parte de la extracci√≥n de datos del html, y al agregar el padding a los documentos se agrego hasta el final de cada uno de ellos, no al final de cada oraci√≥n, por lo tanto que el token de fin de secuencia siga a una de esas frases es muy improbable y al agregarse se puede comprobar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 5. Permutar Oraciones\n",
    "\n",
    "Ahora realizaremos la permutaci√≥n de algunas oraciones y veremos las probabilidades que asignan ambos modelos a ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def get_permutations(sentence):\n",
    "    return list(set(permutations(sentence)))\n",
    "\n",
    "cad1_perms = get_permutations(cad1)\n",
    "cad2_perms = get_permutations(cad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_and_less(indexes, cad_perms, probs):\n",
    "    print(bold_string('most likely'))\n",
    "    for index in indexes[-3:][::-1]:\n",
    "        print_sequence(cad_perms[index], start=0, end='all')\n",
    "        print(bold_string('prob: '), probs[index])\n",
    "\n",
    "    print(bold_string('\\nless likely'))\n",
    "    for index in indexes[:3]:\n",
    "        print_sequence(cad_perms[index], start=0, end='all')\n",
    "        print(bold_string('prob: '), probs[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cadena 1: sino gano me voy a la chingada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_probs = []\n",
    "a_probs = []\n",
    "for cad in cad1_perms:\n",
    "    t_probs.append(i_model.estimate_prob(cad))\n",
    "    a_probs.append(amlo_model.estimate_prob(cad))\n",
    "\n",
    "t_indexes = np.argsort(np.array(t_probs))\n",
    "a_indexes = np.argsort(np.array(a_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "sino gano me voy a la chingada \n",
      "\u001b[1mprob: \u001b[0m  1.1922094434068082e-07\n",
      "gano sino me voy a la chingada \n",
      "\u001b[1mprob: \u001b[0m  1.1901679610392259e-07\n",
      "sino gano chingada me voy a la \n",
      "\u001b[1mprob: \u001b[0m  5.01227291596439e-10\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "a la me voy chingada gano sino \n",
      "\u001b[1mprob: \u001b[0m  3.1384025644997624e-24\n",
      "a la me voy chingada sino gano \n",
      "\u001b[1mprob: \u001b[0m  3.387426730243442e-24\n",
      "a la me voy sino chingada gano \n",
      "\u001b[1mprob: \u001b[0m  6.286339719983113e-24\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(t_indexes, cad1_perms, t_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amlo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "sino me voy a la gano chingada \n",
      "\u001b[1mprob: \u001b[0m  1.2704443626446344e-06\n",
      "sino me voy a la chingada gano \n",
      "\u001b[1mprob: \u001b[0m  1.270444362644632e-06\n",
      "sino me voy a chingada la gano \n",
      "\u001b[1mprob: \u001b[0m  7.092639486621779e-07\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "a la sino voy chingada gano me \n",
      "\u001b[1mprob: \u001b[0m  9.301224029850133e-20\n",
      "a la sino voy gano chingada me \n",
      "\u001b[1mprob: \u001b[0m  9.301224029851983e-20\n",
      "a la sino gano voy chingada me \n",
      "\u001b[1mprob: \u001b[0m  9.416308227064173e-20\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(a_indexes, cad1_perms, a_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cadena 2: ya se va a acabar la corrupci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_probs = []\n",
    "a_probs = []\n",
    "for cad in cad2_perms:\n",
    "    t_probs.append(i_model.estimate_prob(cad))\n",
    "    a_probs.append(amlo_model.estimate_prob(cad))\n",
    "\n",
    "t_indexes = np.argsort(np.array(t_probs))\n",
    "a_indexes = np.argsort(np.array(a_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "corrupci√≥n ya se va acabar la a \n",
      "\u001b[1mprob: \u001b[0m  1.7441987500257667e-07\n",
      "ya corrupci√≥n se va acabar la a \n",
      "\u001b[1mprob: \u001b[0m  1.653957389021059e-07\n",
      "corrupci√≥n ya se va a acabar la \n",
      "\u001b[1mprob: \u001b[0m  1.2860772942764428e-07\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "a la se va ya acabar corrupci√≥n \n",
      "\u001b[1mprob: \u001b[0m  1.4741884684489114e-23\n",
      "a la se va ya corrupci√≥n acabar \n",
      "\u001b[1mprob: \u001b[0m  1.518769403752883e-23\n",
      "a la se ya acabar va corrupci√≥n \n",
      "\u001b[1mprob: \u001b[0m  1.0646780498290968e-22\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(t_indexes, cad2_perms, t_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "acabar ya se va a la corrupci√≥n \n",
      "\u001b[1mprob: \u001b[0m  2.593304149362993e-06\n",
      "acabar corrupci√≥n ya se va a la \n",
      "\u001b[1mprob: \u001b[0m  3.3707488176842015e-07\n",
      "corrupci√≥n ya se va a acabar la \n",
      "\u001b[1mprob: \u001b[0m  3.1133040049288036e-07\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "ya la a acabar se va corrupci√≥n \n",
      "\u001b[1mprob: \u001b[0m  5.607714394626497e-25\n",
      "a la acabar ya va se corrupci√≥n \n",
      "\u001b[1mprob: \u001b[0m  5.94757915059409e-25\n",
      "a la acabar corrupci√≥n ya va se \n",
      "\u001b[1mprob: \u001b[0m  7.357735400614589e-25\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(a_indexes, cad2_perms, a_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cadena Extra: los pol√≠ticos son corruptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cad3 = ['los', 'pol√≠ticos', 'son', 'corruptos']\n",
    "cad3_perms = get_permutations(cad3)\n",
    "\n",
    "t_probs = []\n",
    "a_probs = []\n",
    "for cad in cad3_perms:\n",
    "    t_probs.append(i_model.estimate_prob(cad))\n",
    "    a_probs.append(amlo_model.estimate_prob(cad))\n",
    "\n",
    "t_indexes = np.argsort(np.array(t_probs))\n",
    "a_indexes = np.argsort(np.array(a_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "corruptos pol√≠ticos son los \n",
      "\u001b[1mprob: \u001b[0m  0.0028417801222255542\n",
      "corruptos los pol√≠ticos son \n",
      "\u001b[1mprob: \u001b[0m  0.0003954656253428735\n",
      "son pol√≠ticos corruptos los \n",
      "\u001b[1mprob: \u001b[0m  0.00012093944508726635\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "son los corruptos pol√≠ticos \n",
      "\u001b[1mprob: \u001b[0m  1.0706958631956398e-10\n",
      "los son corruptos pol√≠ticos \n",
      "\u001b[1mprob: \u001b[0m  1.6056424897630659e-09\n",
      "corruptos los son pol√≠ticos \n",
      "\u001b[1mprob: \u001b[0m  2.9721000845845842e-08\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(t_indexes, cad3_perms, t_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmost likely\u001b[0m \n",
      "pol√≠ticos corruptos son los \n",
      "\u001b[1mprob: \u001b[0m  0.00048523482239963556\n",
      "los pol√≠ticos corruptos son \n",
      "\u001b[1mprob: \u001b[0m  0.00039115261441684864\n",
      "son pol√≠ticos corruptos los \n",
      "\u001b[1mprob: \u001b[0m  0.0003243569348289035\n",
      "\u001b[1m\n",
      "less likely\u001b[0m \n",
      "son los corruptos pol√≠ticos \n",
      "\u001b[1mprob: \u001b[0m  2.9008852556234356e-10\n",
      "los corruptos pol√≠ticos son \n",
      "\u001b[1mprob: \u001b[0m  9.823962506807205e-10\n",
      "los son corruptos pol√≠ticos \n",
      "\u001b[1mprob: \u001b[0m  3.9252293275481826e-08\n"
     ]
    }
   ],
   "source": [
    "show_most_and_less(a_indexes, cad3_perms, a_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver a trav√©s de estas oraciones o frases que el modelo entrenado en twitter fue el √∫nico que asigno la probabilidad m√°s alta a la frase original. Sin embargo en ambos modelos se puede ver que las probabilidades m√°s altas son asignadas a las frases u oraciones que tienen m√°s sentido en cuanto a como se expresar√≠a un humano. Aun as√≠, algunas frases son una manera poco usual de expresarse por parte de un humano."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
