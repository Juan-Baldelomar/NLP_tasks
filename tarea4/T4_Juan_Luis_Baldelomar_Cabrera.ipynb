{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "\n",
    "#remove extra lines\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)\n",
    "\n",
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)\n",
    "all_documents = documents + val_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# convert documents into bigram documents\n",
    "def build_bigram_documents(documents):\n",
    "    bigram_documents = [[word1 + ' ' + word2 for word1, word2 in zip(doc, doc[1:])] for doc in documents]\n",
    "    return bigram_documents\n",
    "\n",
    "def add_padding(documents, k, end_padding=True):\n",
    "    padded_documents = []\n",
    "    for doc in documents:\n",
    "        doc =  ['<s>']*k + doc\n",
    "        if end_padding:\n",
    "            doc += ['</s>']\n",
    "            \n",
    "        padded_documents.append(doc)\n",
    "    return padded_documents\n",
    "\n",
    "def mask_documents(documents, vocabulary):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            if vocabulary.get(word) is not None:\n",
    "                masked_doc.append(word)\n",
    "            else:\n",
    "                masked_doc.append('<unk>')\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents\n",
    "\n",
    "def get_vocabulary(documents, start='', end='', n=-1):\n",
    "    # get unique words\n",
    "    words = [word for doc in documents for word in doc]\n",
    "    unique_words = FreqDist(words).most_common(n) if n!= -1 else FreqDist(words).most_common() \n",
    "    \n",
    "    # init voc dict\n",
    "    vocabulary = {start: 0} if start != '' else {}\n",
    "    \n",
    "    # fill vocabulary with positions\n",
    "    pos_available = 1 if start != '' else 0\n",
    "    for (word, _) in unique_words:\n",
    "        \n",
    "        # verify words is not start, end or unk token (special positions for those)\n",
    "        if word not in (start, end, '<unk>'):\n",
    "            vocabulary[word] = pos_available\n",
    "            pos_available += 1\n",
    "    \n",
    "    # set unk token\n",
    "    vocabulary['<unk>'] = len(vocabulary)\n",
    "    \n",
    "    # if padded was added, set end token\n",
    "    if end != '':\n",
    "        vocabulary[end] = len(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def trim_vocabulary(side, vocabulary):\n",
    "    new_voc = {}\n",
    "    if side == 'top':\n",
    "        for (key, value) in list(vocabulary.items())[1:]:\n",
    "            new_voc[key] = value-1\n",
    "    elif side == 'bottom':\n",
    "        for (key, value) in list(vocabulary.items())[:-1]:\n",
    "            new_voc[key] = value\n",
    "    else:\n",
    "        for (key, value) in list(vocabulary.items())[1:-1]:\n",
    "            new_voc[key] = value-1\n",
    "    \n",
    "    return new_voc\n",
    "\n",
    "def prepair_unigram(documents, n_voc):\n",
    "    vocabulary = get_vocabulary(documents, start='<s>', end='</s>', n=n_voc)\n",
    "    docs = add_padding(documents, 1)\n",
    "    docs = mask_documents(docs, vocabulary)\n",
    "    return vocabulary, docs\n",
    "\n",
    "def prepair_bigram(documents, n_voc):\n",
    "    # get unigrams and mask documents\n",
    "    vocabulary = get_vocabulary(documents, end='</s>', n=n_voc)\n",
    "    docs = mask_documents(documents, vocabulary)\n",
    "    docs = add_padding(docs, 1)\n",
    "    docs = add_padding(docs, 1, end_padding=False)\n",
    "    \n",
    "    # get bigrams vocabulary\n",
    "    bi_docs = add_padding(documents, 2, end_padding=False)\n",
    "    bi_docs = build_bigram_documents(bi_docs)\n",
    "    bi_vocabulary = get_vocabulary(bi_docs, start='<s> <s>', n=n_voc)\n",
    "    \n",
    "    # return vocabularies and documents padded\n",
    "    return vocabulary, bi_vocabulary, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram(documents, vocabulary):\n",
    "    counts = np.zeros(len(vocabulary))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for word in doc[1:]:                                                            \n",
    "            counts[vocabulary[word]]+= 1\n",
    "            \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram(documents, r_voc, c_voc):\n",
    "    n = len(r_voc)\n",
    "    m = len(c_voc)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for i in range(1, len(doc)):                                                     \n",
    "            context, word = doc[i-1], doc[i]\n",
    "            counts[r_voc[context], c_voc[word]] += 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram(documents, vocabulary, bi_vocabulary):\n",
    "    m = len(vocabulary)\n",
    "    n = len(bi_vocabulary)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s>, <s> in padded couments\n",
    "        for i in range(2, len(doc)):                                                       \n",
    "            context, word = doc[i-2] + ' ' + doc[i-1], doc[i]\n",
    "            context = context if bi_vocabulary.get(context) is not None else '<unk>'\n",
    "            counts[bi_vocabulary[context], vocabulary[word]] += 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    def train(self, documents, voc_size=10000):\n",
    "        voc, unidocs = prepair_unigram(documents, voc_size)\n",
    "        self.voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.voc.keys())\n",
    "        self.counts = build_unigram(unidocs, self.voc)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        self.probs = self.counts / np.sum(self.counts)\n",
    "    \n",
    "    def predict(self):\n",
    "        c_index = sample(self.probs)\n",
    "        return self.voc_words[c_index], self.probs[c_index]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 1:\n",
    "            print('[ERR]: Not Enough Tokens for Unigram Model')\n",
    "            return 1\n",
    "        \n",
    "        total_logprob = 0\n",
    "        for word in sequence:\n",
    "            token = '<unk>' if self.voc.get(word) is None else word\n",
    "            prob = self.probs[self.voc[token]]\n",
    "            total_logprob += np.log(prob)\n",
    "            \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word = '</s>'\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict()\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1\n",
    "            for i in range(1, len(test)):\n",
    "                prob = self.estimate_prob([test[i]])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def train(self):\n",
    "        raise NotImplementedError('Subclass should implement own train')\n",
    "    \n",
    "    def estimate_prob(self):\n",
    "        raise NotImplementedError('Subclass should implement own prob function')\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        raise NotImplementedError('Subclass should implement own generate function')\n",
    "        \n",
    "    def eval_model(self, documents):\n",
    "        raise NotImplementedError('Subclass should implement own eval function')\n",
    "        \n",
    "    def perplexity(self, test_set):\n",
    "        raise NotImplementedError('Subclass should implement own perplexity function')\n",
    "    \n",
    "    def smooth(self, k):\n",
    "        self.counts = self.counts + k\n",
    "    \n",
    "    def predict(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        c_index = sample(self.probs[r_index])\n",
    "        return self.voc_words[c_index], self.probs[r_index, c_index]\n",
    "    \n",
    "    def conditioned_space(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        return self.probs[r_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(NGramModel):\n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        voc, docs = prepair_unigram(documents, voc_size)\n",
    "        self.r_voc = trim_vocabulary('bottom', voc)\n",
    "        self.c_voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts  = build_bigram(docs, self.r_voc, self.c_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "        \n",
    "    def get_probs(self):\n",
    "        unicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/unicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n",
    "    \n",
    "    def cond_prob(self, word1, word):\n",
    "        cond_space = self.conditioned_space(word1)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word  \n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 2:\n",
    "            print('[ERR]: Not Enough Tokens for Bigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word = word1\n",
    "        total_logprob = 0\n",
    "        for word in sequence[1:]:\n",
    "            prob = self.cond_prob(word1, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1 = word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word = word1\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1)\n",
    "            word1 = word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1 if len(test) > 1 else 0\n",
    "            for i in range(1, len(test)):\n",
    "                c1, w = test[i-1], test[i]\n",
    "                prob = self.estimate_prob([c1, w])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(NGramModel):\n",
    "    def __init__(self):\n",
    "        super(NGramModel).__init__()\n",
    "    \n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        self.c_voc, self.r_voc, docs = prepair_bigram(documents, voc_size)\n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts = build_trigram(docs, self.c_voc, self.r_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        bicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/bicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        cond_space = self.conditioned_space(word1 + ' ' + word2)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Trigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1 + ' ' + word2)\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.estimate_prob([c1, c2, w])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = TrigramModel()\n",
    "trigram.train(documents, k=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591.9993423789825"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.541482916387375e-09"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.estimate_prob(['<s>', '<s>','hijos', 'de', 'la', 'verga', '</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001646601315699137"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.estimate_prob(['hijos', 'de', 'la', 'verga'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439.1253897138523"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('el', 0.015149952404588923)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.predict('hola como')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = trigram.generate_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 10002)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambdas Fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_ = [[1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1],[.1, .4, .5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedModel:\n",
    "    def __init__(self, lambda_):\n",
    "        self.l1, self.l2, self.l3 = lambda_\n",
    "        self.unigram = UnigramModel()\n",
    "        self.bigram = BigramModel()\n",
    "        self.trigram = TrigramModel()\n",
    "        \n",
    "    def train(self, documents, k=0, voc_size=10000):\n",
    "        self.unigram.train(documents, voc_size)\n",
    "        self.bigram.train(documents, k, voc_size)\n",
    "        self.trigram.train(documents, k, voc_size)\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        # build contexts\n",
    "        bicontext = sequence[1]\n",
    "        tricontext = sequence[0] + ' ' + sequence[1]\n",
    "        \n",
    "        # get conditioned spaces\n",
    "        unispace = self.unigram.probs\n",
    "        bispace = self.bigram.conditioned_space(bicontext)\n",
    "        trispace = self.trigram.conditioned_space(tricontext)\n",
    "        \n",
    "        # sample from probability space\n",
    "        probs = self.l1 * unispace + self.l2 * bispace + self.l3 * trispace\n",
    "        c_index = sample(probs)\n",
    "        \n",
    "        return self.unigram.voc_words[c_index], probs[c_index]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Interpolated Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            uniprob = self.unigram.estimate_prob(word)\n",
    "            biprob  = self.bigram.estimate_prob([word2, word])\n",
    "            triprob = self.trigram.estimate_prob([word1, word2, word])\n",
    "            prob = self.l1 * uniprob + self.l2 * biprob + self.l3 * triprob\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict([word1, word2])\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.estimate_prob([c1, c2, w])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_model = InterpolatedModel(lambdas_[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_model.train(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '40',\n",
       " 'minutos',\n",
       " 'en',\n",
       " 'el',\n",
       " 'que',\n",
       " 'te',\n",
       " 'indigne',\n",
       " '<unk>',\n",
       " 'por',\n",
       " 'decir',\n",
       " '“',\n",
       " 'el',\n",
       " 'nombre',\n",
       " 'del',\n",
       " 'ganador',\n",
       " 'sus',\n",
       " '.',\n",
       " 'en',\n",
       " 'tu',\n",
       " 'foto',\n",
       " 'del',\n",
       " '.',\n",
       " 'que',\n",
       " 'he',\n",
       " 'tomado',\n",
       " 'presentación',\n",
       " 'lo',\n",
       " 'jefes',\n",
       " 'que',\n",
       " 'es',\n",
       " 'por',\n",
       " 'respeto',\n",
       " '.',\n",
       " 'aunque',\n",
       " 'le',\n",
       " '<unk>',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_model.generate_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 10002)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_model.trigram.probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actualización de Probabilidades \n",
    "\n",
    "Es de interés tomar cierta medida para asegurar que la probabilidad del token de fin de secuencia '\\</s\\>' vaya aumentando conforme la secuencia se va haciendo más larga. Para ello utilizaremos la siguiente regla de actualización: \n",
    "\n",
    "Sea $p_s$ la probabilidad de obtener el token de fin de secuencia. Entonces como $p_s \\leq 1$, sabemos que ${p^r_s} \\geq p_s$ en donde $r<1$. De hecho, sabemos también que \n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} \\sqrt[n]{r} = 1$$ \n",
    "\n",
    "Entonces, podemos tomar la regla de actualización $$\\hat{p}_s = \\sqrt[n]{p_s}$$\n",
    "\n",
    "Debido a que esta probabilidad aumentó, para asegurarnos que el espacio de probabilidad se encuentra bien definido, debemos disminuir esta probabilidad de los otros tokens para asegurarnos que la suma de las probabilidades siga siendo 1. Definamos el aumento de la probabilidad que tenemos respecto al token de fin de secuencia como \n",
    "\n",
    "$$a_p = \\hat{p}_s - p_s$$\n",
    "\n",
    "Entonces, sea $p_i$ la probabilidad de obtener el token $t_i$ en donde $t_i \\neq $ '\\</s\\>'. Definamos a $\\sigma$ como \n",
    "\n",
    "$$\\sigma = \\sum_{i=1}^{|V|} p_i$$\n",
    "\n",
    "en donde $|V|$ representa la cardinalidad del conjunto del vocabulario sin considerar al token de fin de secuencia. Notemos que $\\sigma = 1 - p_s$. Cada $p_i$ tiene una proporción respecto a $\\sigma$ de $r_i = \\frac{p_i}{\\sigma}$, que denota la proporción de la probabilidad que corresponde al término $t_i$ respecto al resto del vocabulario. Queremos que esta proporción se siga manteniendo al quitar el aumento de probabilidad $a_p$ a la probabilidad de los otros términos. Entonces, utilizando la siguiente regla de actualización\n",
    "\n",
    "$$\\hat{p}_i = p_i - r_i a_p$$\n",
    "\n",
    "y definiendo a $$\\hat{\\sigma} = \\sum_{i=1}^{|V|} \\hat{p}_i$$\n",
    "\n",
    "podemos ver que se cumple $$\\hat{r}_i = \\frac{\\hat{p}_i}{\\hat{\\sigma}} = r_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receives a probs matrix and the power r.\n",
    "def diminish(probs, r):\n",
    "    # calculate new probability\n",
    "    new_probs = np.zeros(probs.shape)\n",
    "    new_stop_prob = np.power(probs[:, -1], r)\n",
    "    # get improvement\n",
    "    improve = (new_stop_prob - probs[:, -1])\n",
    "    # get ratio of the other probabilities between them\n",
    "    c = np.sum(probs[:, :-1], axis=1)\n",
    "    rat = probs[:, :-1]/c[:, np.newaxis]\n",
    "    # update new probability\n",
    "    new_probs[:, -1] = new_stop_prob\n",
    "    new_probs[:, :-1] = probs[:, :-1] - rat * improve[:, np.newaxis]\n",
    "    \n",
    "    return new_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7400828 0.6687403]\n",
      "[0.4400828 0.4687403]\n",
      "[1. 1.]\n",
      "[[0.14852411 0.11139308 0.7400828 ]\n",
      " [0.12422239 0.20703731 0.6687403 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = [[.4, .3, .3],[.3, .5, .2]]\n",
    "probs = np.array(probs, dtype=np.float128)\n",
    "\n",
    "new_probs = diminish(probs, 0.25)\n",
    "print(new_probs)\n",
    "np.sum(new_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".4/(.4 + .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285769248274"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".14852411/(0.14852411 + 0.11139308)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m = TrigramModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1887"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.bi_voc['<s> hola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10001, 10002), 10001, 10002)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.probs.shape, len(trigram_m.bi_voc), len(trigram_m.voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.999999999993"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('alv .', 891)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigram_m.bi_voc.items())[891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trigram_m.probs.shape[0]):\n",
    "    for j in range(trigram_m.probs.shape[1]):\n",
    "        if np.isnan(trigram_m.probs[i,j]):\n",
    "            print('nan at', i, ' ', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m.train(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, unidocs = prepair_unigram(documents, 10000)\n",
    "bi_vocabulary, bidocs = prepair_bigram(documents, 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = build_unigram(unidocs, vocabulary)\n",
    "unigram_prob = unigram/np.sum(unigram[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = build_bigram(unidocs, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_prob = bigram[:-1]/unigram[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.000000000004"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_padded_docs = add_padding(unidocs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = build_trigram(bi_padded_docs, vocabulary, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_of_bigrams = build_unigram(bidocs, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_prob = trigram[:-1]/unigram_of_bigrams[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True, False])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram, axis=1) == unigram_of_bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> <s>',\n",
       " '. </s>',\n",
       " '! !',\n",
       " '<s> @usuario',\n",
       " 'la verga',\n",
       " 'a la',\n",
       " '! </s>',\n",
       " 'de la',\n",
       " '@usuario @usuario',\n",
       " 'que no',\n",
       " 'que me',\n",
       " '<s> no',\n",
       " '? </s>',\n",
       " 'la madre',\n",
       " '<s> me',\n",
       " '<s> que',\n",
       " 'los putos',\n",
       " 'en la',\n",
       " '😂 😂',\n",
       " '… </s>',\n",
       " 'puta madre',\n",
       " 'en el',\n",
       " 'que se',\n",
       " '<s> ya',\n",
       " 'las putas',\n",
       " 'su madre',\n",
       " 'lo que',\n",
       " 'a su',\n",
       " 'verga .',\n",
       " 'que te',\n",
       " 'y no',\n",
       " 'voy a',\n",
       " '<s> si',\n",
       " 'no me',\n",
       " '<s> a',\n",
       " '😂 </s>',\n",
       " '? ?',\n",
       " 'madre .',\n",
       " 'no se',\n",
       " 'a los',\n",
       " '@usuario </s>',\n",
       " 'a mi',\n",
       " '<s> y',\n",
       " '<s> la',\n",
       " 'vale verga',\n",
       " 'para que',\n",
       " 'todos los',\n",
       " '<s> ¿',\n",
       " 'madre </s>',\n",
       " 'tu madre',\n",
       " 'de mi',\n",
       " 'a tu',\n",
       " 'hijos de',\n",
       " 'a las',\n",
       " 'y me',\n",
       " 'de su',\n",
       " 'de mierda',\n",
       " 'hasta la',\n",
       " 'de los',\n",
       " 'no es',\n",
       " 'es que',\n",
       " 'verga </s>',\n",
       " 'va a',\n",
       " 'hijo de',\n",
       " 'ya me',\n",
       " 'de tu',\n",
       " 'por qué',\n",
       " 'que putas',\n",
       " '<s> mi',\n",
       " 'que le',\n",
       " 'me vale',\n",
       " 'si no',\n",
       " '<s> el',\n",
       " 'que es',\n",
       " 'con el',\n",
       " 'no te',\n",
       " 'de verga',\n",
       " 'sus putas',\n",
       " 'ya no',\n",
       " '️ </s>',\n",
       " 'y que',\n",
       " 'con la',\n",
       " '... </s>',\n",
       " 'de las',\n",
       " '<s> en',\n",
       " 'la vida',\n",
       " 'es un',\n",
       " 'de que',\n",
       " '<s> cuando',\n",
       " '<s> yo',\n",
       " '<s> como',\n",
       " 'y se',\n",
       " 'ganas de',\n",
       " 'de putas',\n",
       " 'van a',\n",
       " 'en mi',\n",
       " 'tu puta',\n",
       " 'verga y',\n",
       " '<url> </s>',\n",
       " '<s> estoy',\n",
       " '\" .',\n",
       " 'mi madre',\n",
       " 'que el',\n",
       " 'por que',\n",
       " 'que ya',\n",
       " 'que la',\n",
       " 'a ver',\n",
       " 'a chingar',\n",
       " 'loca .',\n",
       " 'y la',\n",
       " 'chingas a',\n",
       " 'que los',\n",
       " 'vas a',\n",
       " 'y el',\n",
       " 'el que',\n",
       " '<s> lo',\n",
       " 'no hay',\n",
       " 'se me',\n",
       " '<s> putos',\n",
       " 'la chingada',\n",
       " '<s> se',\n",
       " 'como loca',\n",
       " 'madre !',\n",
       " 'su puta',\n",
       " 'no sé',\n",
       " 'no mames',\n",
       " 'hdp </s>',\n",
       " '<s> es',\n",
       " 'putos .',\n",
       " 'madre y',\n",
       " 'la gente',\n",
       " 'todo el',\n",
       " 'no lo',\n",
       " '😒 </s>',\n",
       " 'es la',\n",
       " 'todas las',\n",
       " 'madre de',\n",
       " 'de @usuario',\n",
       " 'y ya',\n",
       " 'a todos',\n",
       " '<s> \"',\n",
       " 'la puta',\n",
       " 'putas .',\n",
       " 'chingar a',\n",
       " '<s> por',\n",
       " 'qué putas',\n",
       " '. y',\n",
       " 'putas madres',\n",
       " 'y te',\n",
       " '. no',\n",
       " 'los que',\n",
       " 'verga en',\n",
       " '😡 </s>',\n",
       " 'si me',\n",
       " 'estoy loca',\n",
       " 'putos </s>',\n",
       " '<s> hoy',\n",
       " 'la loca',\n",
       " '\" </s>',\n",
       " 'mi verga',\n",
       " 'verga que',\n",
       " 'mamá luchona',\n",
       " 'se la',\n",
       " '<s> pinche',\n",
       " 'por la',\n",
       " 'es una',\n",
       " 'ya se',\n",
       " 'verga !',\n",
       " 'mil putas',\n",
       " 'y yo',\n",
       " 'que estoy',\n",
       " '❤ ️',\n",
       " '😡 😡',\n",
       " 'esos putos',\n",
       " '@usuario y',\n",
       " 'es el',\n",
       " 'de sus',\n",
       " 'de ser',\n",
       " '<s> los',\n",
       " 'poca madre',\n",
       " 'si te',\n",
       " 'las mujeres',\n",
       " '¿ qué',\n",
       " 'de un',\n",
       " 'me voy',\n",
       " ': v',\n",
       " '@usuario que',\n",
       " 'a una',\n",
       " 'mi vida',\n",
       " 'loca y',\n",
       " '@usuario no',\n",
       " 'es de',\n",
       " '🙄 </s>',\n",
       " 'con mi',\n",
       " 'que las',\n",
       " 'es lo',\n",
       " 'de putos',\n",
       " 'putos !',\n",
       " '<s> pero',\n",
       " 'y a',\n",
       " 'creo que',\n",
       " 'vale madre',\n",
       " 'estoy hasta',\n",
       " 'no tienen',\n",
       " 'y si',\n",
       " 'y de',\n",
       " 'pero no',\n",
       " 'sabes que',\n",
       " 'con sus',\n",
       " 'por eso',\n",
       " 'me gusta',\n",
       " '<s> las',\n",
       " 'me la',\n",
       " 'deja de',\n",
       " 'no puedo',\n",
       " '😭 </s>',\n",
       " '<s> qué',\n",
       " '😭 😭',\n",
       " 'me caga',\n",
       " ') </s>',\n",
       " 'de puta',\n",
       " 'por el',\n",
       " 'valer verga',\n",
       " 'que soy',\n",
       " 'chinguen a',\n",
       " 'loca </s>',\n",
       " 'toda la',\n",
       " 'en un',\n",
       " 'con los',\n",
       " '<s> de',\n",
       " 'me encanta',\n",
       " 'a un',\n",
       " 'que nos',\n",
       " '<s> verga',\n",
       " 'es mi',\n",
       " 'loca de',\n",
       " '<s> una',\n",
       " '<s> te',\n",
       " 'que a',\n",
       " 'putas y',\n",
       " 'no le',\n",
       " 'que lo',\n",
       " 'chinga tu',\n",
       " 'loca por',\n",
       " '<s> ¡',\n",
       " 'yo no',\n",
       " 'con un',\n",
       " 'v </s>',\n",
       " 'se ve',\n",
       " 'verga de',\n",
       " 'madre que',\n",
       " 'loca que',\n",
       " 'que les',\n",
       " 'y lo',\n",
       " 'y los',\n",
       " 'está de',\n",
       " 'jajaja </s>',\n",
       " 'con las',\n",
       " ':( </s>',\n",
       " '<s> pues',\n",
       " 'verga a',\n",
       " '. ¿',\n",
       " 'sus putos',\n",
       " 'y ahora',\n",
       " 'en tu',\n",
       " 'con su',\n",
       " 'verga con',\n",
       " '<s> ahora',\n",
       " 'hija de',\n",
       " 'eres un',\n",
       " 'la que',\n",
       " 'ir a',\n",
       " 'como si',\n",
       " 'una loca',\n",
       " 'pinche joto',\n",
       " 'la verdad',\n",
       " 'de una',\n",
       " 'a sus',\n",
       " 'no les',\n",
       " 'hdp !',\n",
       " '¿ por',\n",
       " 'a alguien',\n",
       " 'que si',\n",
       " 'todo lo',\n",
       " '<s> pinches',\n",
       " '! y',\n",
       " '. pero',\n",
       " 'la palabra',\n",
       " 'pinches putos',\n",
       " '<s> tengo',\n",
       " 'me siento',\n",
       " 'alguien que',\n",
       " 'no mamen',\n",
       " 'a hacer',\n",
       " '? !',\n",
       " 'el día',\n",
       " 'el culo',\n",
       " '<s> hay',\n",
       " 'a @usuario',\n",
       " 'en su',\n",
       " 'en las',\n",
       " 'a ser',\n",
       " 'un chingo',\n",
       " 'hay que',\n",
       " 'verga \"',\n",
       " 'mi casa',\n",
       " '<s> jajajaja',\n",
       " 'me estoy',\n",
       " 'que son',\n",
       " '@usuario es',\n",
       " 'verga ?',\n",
       " '<s> ojalá',\n",
       " 'me cagan',\n",
       " ': \"',\n",
       " 'por un',\n",
       " '<s> para',\n",
       " 'mi mamá',\n",
       " 'valen verga',\n",
       " 'ya estoy',\n",
       " 'se lo',\n",
       " 'te voy',\n",
       " 'como putas',\n",
       " 'porque no',\n",
       " 'que verga',\n",
       " 'te amo',\n",
       " 'al mundial',\n",
       " 'de lo',\n",
       " 'loca !',\n",
       " '<s> soy',\n",
       " '<s> con',\n",
       " 'se le',\n",
       " 'chingada madre',\n",
       " 'los de',\n",
       " 'cuenta que',\n",
       " 'a mí',\n",
       " 'hdp .',\n",
       " 'en los',\n",
       " '? no',\n",
       " 'no quiero',\n",
       " 'no ?',\n",
       " 'madre a',\n",
       " '\" y',\n",
       " 'el puto',\n",
       " '<s> jajaja',\n",
       " '🤔 </s>',\n",
       " '@usuario ya',\n",
       " 'si ya',\n",
       " 'personas que',\n",
       " 'mandar a',\n",
       " 'verga no',\n",
       " 'madre el',\n",
       " 'jajajaja </s>',\n",
       " 'ni madres',\n",
       " 'luchona y',\n",
       " 'dejen de',\n",
       " 'que yo',\n",
       " '. ya',\n",
       " 'con una',\n",
       " 'gente que',\n",
       " 'pero si',\n",
       " 'xd </s>',\n",
       " 'hasta que',\n",
       " '🇲 🇽',\n",
       " 'que sea',\n",
       " '. 😂',\n",
       " 'iba a',\n",
       " '. putos',\n",
       " 'y luego',\n",
       " 'el mundo',\n",
       " 'vez que',\n",
       " 'putos años',\n",
       " 'v word',\n",
       " 'otra vez',\n",
       " 'que mi',\n",
       " 'tengo que',\n",
       " '😍 😍',\n",
       " 'o sea',\n",
       " 'nada más',\n",
       " 'cosas que',\n",
       " '<s> un',\n",
       " 'se que',\n",
       " '. que',\n",
       " 'mierda </s>',\n",
       " 'pero me',\n",
       " 'está bien',\n",
       " '. ¡',\n",
       " '<s> puta',\n",
       " 'de \"',\n",
       " 'me da',\n",
       " 'verga pero',\n",
       " 'a quien',\n",
       " 'no son',\n",
       " 'quiero que',\n",
       " 'bola de',\n",
       " 'y en',\n",
       " 'es como',\n",
       " 'me hace',\n",
       " 'después de',\n",
       " '<s> todos',\n",
       " 'como cuando',\n",
       " '! ?',\n",
       " 'que eres',\n",
       " '<s> esta',\n",
       " 'lo mismo',\n",
       " 'putas ganas',\n",
       " 'más de',\n",
       " 'a veces',\n",
       " 'cada vez',\n",
       " '<s> le',\n",
       " 'para el',\n",
       " 'verga si',\n",
       " '<s> quiero',\n",
       " 'me tienen',\n",
       " 'o que',\n",
       " 'son los',\n",
       " '! ¡',\n",
       " 'no seas',\n",
       " 'y tu',\n",
       " 'putos periodistas',\n",
       " 'de mamar',\n",
       " 'la boca',\n",
       " 'a ti',\n",
       " '\" verga',\n",
       " 'una mujer',\n",
       " 'de verdad',\n",
       " '. me',\n",
       " 'no soy',\n",
       " 'pero que',\n",
       " 'como me',\n",
       " 'si se',\n",
       " 'y su',\n",
       " 'tan loca',\n",
       " 'bueno que',\n",
       " 'ver si',\n",
       " 'y al',\n",
       " 'la mañana',\n",
       " 'les vale',\n",
       " '🖕 🖕',\n",
       " 'nada .',\n",
       " 'putas que',\n",
       " 'chingue a',\n",
       " 'se va',\n",
       " 'cuando me',\n",
       " '<s> este',\n",
       " 'unas putas',\n",
       " 'son bien',\n",
       " 'verga la',\n",
       " 'la pinche',\n",
       " 'a que',\n",
       " 'que tengo',\n",
       " 'una verga',\n",
       " 'de loca',\n",
       " 'y sus',\n",
       " 'y las',\n",
       " 'del mundo',\n",
       " 'y por',\n",
       " '<s> marica',\n",
       " 'decir que',\n",
       " 'les gusta',\n",
       " 'eso no',\n",
       " 'al final',\n",
       " 'mejor que',\n",
       " 'madre no',\n",
       " '<s> o',\n",
       " 'me lo',\n",
       " '😤 </s>',\n",
       " '<s> oye',\n",
       " 'el joto',\n",
       " 'se te',\n",
       " 'que poca',\n",
       " 'que bueno',\n",
       " 'que un',\n",
       " 'a lo',\n",
       " 'valió verga',\n",
       " 'putas </s>',\n",
       " '<s> tu',\n",
       " 'por favor',\n",
       " 'las personas',\n",
       " 'valiendo verga',\n",
       " 'verga por',\n",
       " 'antes de',\n",
       " 'putas horas',\n",
       " 'no tiene',\n",
       " 'a este',\n",
       " 'soy la',\n",
       " '😍 </s>',\n",
       " '🙄 🙄',\n",
       " 'un día',\n",
       " 'putas !',\n",
       " 'la noche',\n",
       " 'en esta',\n",
       " 'verga es',\n",
       " 'ya lo',\n",
       " '. a',\n",
       " '<s> mira',\n",
       " 'la escuela',\n",
       " 'loca pero',\n",
       " 'madre por',\n",
       " 'de todos',\n",
       " 'a ese',\n",
       " 'me tiene',\n",
       " 'no puede',\n",
       " 'que digan',\n",
       " '<s> ay',\n",
       " 'putos y',\n",
       " 'y les',\n",
       " 'si es',\n",
       " '<s> solo',\n",
       " '... y',\n",
       " 'madre me',\n",
       " 'eres una',\n",
       " 'de mis',\n",
       " 'de madre',\n",
       " 'volviendo loca',\n",
       " 'pendeja .',\n",
       " 'el hdp',\n",
       " 'que en',\n",
       " 'pero ya',\n",
       " 'dicen que',\n",
       " 'los demás',\n",
       " 'unos putos',\n",
       " 'vamos a',\n",
       " 'a estar',\n",
       " 'vete a',\n",
       " 'en todos',\n",
       " 'cuando te',\n",
       " 'de estar',\n",
       " 'dice que',\n",
       " 'te vas',\n",
       " 'la misma',\n",
       " 'como el',\n",
       " 'de mil',\n",
       " 'y más',\n",
       " 'eso es',\n",
       " '@usuario si',\n",
       " '😠 </s>',\n",
       " '. \"',\n",
       " 'los hombres',\n",
       " 'agua loca',\n",
       " 'que tu',\n",
       " 'joto y',\n",
       " 'madre teresa',\n",
       " 'mierda .',\n",
       " 'así de',\n",
       " 'vayan a',\n",
       " 'madre @usuario',\n",
       " 'chingue su',\n",
       " '. la',\n",
       " '<s> está',\n",
       " 'y con',\n",
       " '<s> porque',\n",
       " 'ya que',\n",
       " '@usuario a',\n",
       " 'cara de',\n",
       " '? 🤔',\n",
       " 'vida loca',\n",
       " 'se ven',\n",
       " '<s> así',\n",
       " 'porque me',\n",
       " 'sé que',\n",
       " 'pinches putas',\n",
       " 'no la',\n",
       " 'son unos',\n",
       " 'ver a',\n",
       " 'madre con',\n",
       " 'por los',\n",
       " 'creen que',\n",
       " 'jaja </s>',\n",
       " 'a volver',\n",
       " 'así que',\n",
       " '\" me',\n",
       " 'la única',\n",
       " 'mamando con',\n",
       " 'tantita madre',\n",
       " 'a nadie',\n",
       " 'madre la',\n",
       " 'la cara',\n",
       " 'como que',\n",
       " '” </s>',\n",
       " 'vida .',\n",
       " 'día de',\n",
       " '<s> eres',\n",
       " 'yo soy',\n",
       " 'teresa de',\n",
       " 'de calcuta',\n",
       " 'putas se',\n",
       " 'tienen hasta',\n",
       " 'en serio',\n",
       " 'que chingue',\n",
       " 'por mi',\n",
       " '? ¿',\n",
       " '! no',\n",
       " 'vuelve loca',\n",
       " 'ya te',\n",
       " 'la calle',\n",
       " 'la mierda',\n",
       " '\" a',\n",
       " 'lo más',\n",
       " 'el marica',\n",
       " 'tienen madre',\n",
       " 'andar de',\n",
       " 'a mamar',\n",
       " 'esa madre',\n",
       " 'la selección',\n",
       " 'en una',\n",
       " 'marica </s>',\n",
       " 'o no',\n",
       " 'así como',\n",
       " 'que quiero',\n",
       " 'vaya a',\n",
       " 'joto </s>',\n",
       " 'se puede',\n",
       " 'yo si',\n",
       " 'mis putas',\n",
       " 'la neta',\n",
       " 'una vez',\n",
       " 'joto .',\n",
       " '#mexicandesmotherpalmundial </s>',\n",
       " 'caga que',\n",
       " 'hdp que',\n",
       " 'como la',\n",
       " 'un joto',\n",
       " 'que está',\n",
       " '? que',\n",
       " '. @usuario',\n",
       " '<s> ah',\n",
       " '🤣 </s>',\n",
       " 'y le',\n",
       " 'a esos',\n",
       " 'es para',\n",
       " '<s> ni',\n",
       " 'como se',\n",
       " 'tus putos',\n",
       " '<s> aquí',\n",
       " '😈 </s>',\n",
       " 'putos hondureños',\n",
       " 'con todo',\n",
       " 'madre \"',\n",
       " 'que de',\n",
       " 'putos todos',\n",
       " '. es',\n",
       " 'acabo de',\n",
       " '😩 😩',\n",
       " 'rica verga',\n",
       " 'me va',\n",
       " 'por ser',\n",
       " 'chingo de',\n",
       " 'a tus',\n",
       " 'no .',\n",
       " 'más putas',\n",
       " 'el mundial',\n",
       " \": '\",\n",
       " 'bien putas',\n",
       " 'dos putas',\n",
       " 'ni madre',\n",
       " '<s> “',\n",
       " 'madre ...',\n",
       " 'bien pinche',\n",
       " 'y así',\n",
       " 'lo único',\n",
       " '😆 😆',\n",
       " 'el amor',\n",
       " 'me dan',\n",
       " '\" no',\n",
       " 'importa lo',\n",
       " '¿ y',\n",
       " 'que todos',\n",
       " 'esta vida',\n",
       " '<s> les',\n",
       " 'que putos',\n",
       " 'loca con',\n",
       " '@usuario chinga',\n",
       " '<s> gracias',\n",
       " '<s> creo',\n",
       " 'a toda',\n",
       " 'y @usuario',\n",
       " '<s> putas',\n",
       " 'estoy volviendo',\n",
       " 'y como',\n",
       " 'que por',\n",
       " 'las que',\n",
       " 'en este',\n",
       " 'las cosas',\n",
       " 'no saben',\n",
       " '<s> quien',\n",
       " 'verga ...',\n",
       " 'no tengo',\n",
       " '@usuario te',\n",
       " '! @usuario',\n",
       " 'es muy',\n",
       " 'ya valió',\n",
       " 'lo de',\n",
       " 'para la',\n",
       " 'mi me',\n",
       " 'de joto',\n",
       " 'te gusta',\n",
       " 'toda tu',\n",
       " 'todos .',\n",
       " 'marica que',\n",
       " '” .',\n",
       " 'de ti',\n",
       " 'no a',\n",
       " 'de esos',\n",
       " 'no pueden',\n",
       " 'se les',\n",
       " 'váyanse a',\n",
       " 'a todas',\n",
       " '<s> sabes',\n",
       " 'mi amor',\n",
       " '💔 </s>',\n",
       " 'maricon de',\n",
       " 'partir la',\n",
       " 'más que',\n",
       " '. se',\n",
       " 'ya le',\n",
       " 'a méxico',\n",
       " 'te la',\n",
       " 'qué pedo',\n",
       " 'de a',\n",
       " 'ahora sí',\n",
       " 'putos gringos',\n",
       " 'yo me',\n",
       " 'son putas',\n",
       " 'pedo con',\n",
       " '<s> siempre',\n",
       " 'se mamó',\n",
       " 'si lo',\n",
       " '. en',\n",
       " 'buenos días',\n",
       " 'putos días',\n",
       " 'a todo',\n",
       " 'a ir',\n",
       " 'de no',\n",
       " 'madre en',\n",
       " 'son de',\n",
       " '. si',\n",
       " 'estoy bien',\n",
       " 'se sienten',\n",
       " 'lo peor',\n",
       " 'es mejor',\n",
       " 'me importa',\n",
       " 'puta que',\n",
       " 'yo le',\n",
       " 'te pones',\n",
       " 'me dio',\n",
       " 'igual que',\n",
       " 'andan de',\n",
       " 'soy joto',\n",
       " 'bien que',\n",
       " 'qué no',\n",
       " 'pendejo .',\n",
       " 'q no',\n",
       " 'y un',\n",
       " 'uno de',\n",
       " 'fotos de',\n",
       " 'el corazón',\n",
       " 'para ver',\n",
       " 'esto es',\n",
       " 'verga todo',\n",
       " 'que rico',\n",
       " '😩 </s>',\n",
       " ': </s>',\n",
       " 'y todos',\n",
       " 'vales verga',\n",
       " '😌 </s>',\n",
       " 'se los',\n",
       " 'sabe que',\n",
       " 'sé si',\n",
       " 'pero a',\n",
       " 'un poco',\n",
       " 'no entiendo',\n",
       " 'con que',\n",
       " 'son las',\n",
       " 'joto de',\n",
       " 'de ese',\n",
       " 'no sean',\n",
       " 'me lleva',\n",
       " 'lleva la',\n",
       " '🤦🏻\\u200d♀ ️',\n",
       " 'te va',\n",
       " 'el pinche',\n",
       " 'en todo',\n",
       " 'valga verga',\n",
       " 'ver que',\n",
       " '. por',\n",
       " 'madre al',\n",
       " 'ahora si',\n",
       " 'le gusta',\n",
       " 'y cuando',\n",
       " 'pinche madre',\n",
       " 'madre los',\n",
       " '| </s>',\n",
       " '. hdp',\n",
       " 'es por',\n",
       " 'a dormir',\n",
       " 'se pone',\n",
       " 'ya ni',\n",
       " 'la pela',\n",
       " '😔 </s>',\n",
       " '... no',\n",
       " 'de amor',\n",
       " 'putas de',\n",
       " 'como le',\n",
       " 'el pendejo',\n",
       " 'pendejo que',\n",
       " 'no quiere',\n",
       " 'que pedo',\n",
       " 'y chingas',\n",
       " 'para no',\n",
       " 'las nalgas',\n",
       " '. 😒',\n",
       " 'el tiempo',\n",
       " 'a trabajar',\n",
       " 'como una',\n",
       " '! que',\n",
       " 'que siempre',\n",
       " 'de todo',\n",
       " 'joto !',\n",
       " 'marica de',\n",
       " '\" de',\n",
       " 'estos putos',\n",
       " 'tiene hasta',\n",
       " 'de esas',\n",
       " 'mi novio',\n",
       " 'espero que',\n",
       " 'el mismo',\n",
       " 'putos de',\n",
       " '😢 </s>',\n",
       " 'te quiero',\n",
       " 'valgo verga',\n",
       " 'marica .',\n",
       " 'me valen',\n",
       " 'el dinero',\n",
       " 'y putas',\n",
       " '😤 😤',\n",
       " 'juro que',\n",
       " '<s> neta',\n",
       " 'de todas',\n",
       " 'que era',\n",
       " 'porque putas',\n",
       " 'tienen que',\n",
       " 'madre mía',\n",
       " 'qué verga',\n",
       " 'mandó a',\n",
       " 'estoy en',\n",
       " 'hasta el',\n",
       " 'un buen',\n",
       " 'verga lo',\n",
       " 'me tienes',\n",
       " 'por no',\n",
       " 'mentar la',\n",
       " '¿ cómo',\n",
       " 'por putos',\n",
       " 'ver el',\n",
       " 'tener que',\n",
       " 'una noche',\n",
       " 'de esta',\n",
       " 'mí me',\n",
       " 'putas me',\n",
       " '<s> eso',\n",
       " 'la vez',\n",
       " 'que ver',\n",
       " 'marica !',\n",
       " '<s> también',\n",
       " 'el partido',\n",
       " 'un pinche',\n",
       " '<s> -',\n",
       " '🤷🏻\\u200d♀ ️',\n",
       " 'es más',\n",
       " 'un hombre',\n",
       " 'y mi',\n",
       " '<s> todo',\n",
       " '. #putas',\n",
       " '️ ❤',\n",
       " '¿ que',\n",
       " 'pensar que',\n",
       " 'madre luchona',\n",
       " 'su verga',\n",
       " 'le pasa',\n",
       " 'a valer',\n",
       " '#masterchefmx </s>',\n",
       " '@usuario por',\n",
       " 'me cago',\n",
       " 'en sus',\n",
       " 'dijo que',\n",
       " 'q se',\n",
       " 'maricón .',\n",
       " '<s> chingas',\n",
       " 'madres </s>',\n",
       " 'siempre me',\n",
       " 'o qué',\n",
       " 'una madre',\n",
       " 'en twitter',\n",
       " 'mando a',\n",
       " 'amor .',\n",
       " 'que hacer',\n",
       " 'putas le',\n",
       " 'putas no',\n",
       " '<s> voy',\n",
       " 'que su',\n",
       " 'esta de',\n",
       " 'del mundial',\n",
       " 'eres la',\n",
       " 'loca me',\n",
       " 'los huevos',\n",
       " 'verga el',\n",
       " 'como siempre',\n",
       " '. pinches',\n",
       " 'quién putas',\n",
       " 'vas y',\n",
       " 'a decir',\n",
       " '.. </s>',\n",
       " '<s> cómo',\n",
       " 'que este',\n",
       " 'esta madre',\n",
       " 'me pongo',\n",
       " 'tus putas',\n",
       " 'me vuelve',\n",
       " '<s> al',\n",
       " 'putas ?',\n",
       " 'dos putos',\n",
       " '🖕🏻 🖕🏻',\n",
       " 'una persona',\n",
       " '? 😂',\n",
       " 'chingada .',\n",
       " 'te digo',\n",
       " 'digan esos',\n",
       " 'con ganas',\n",
       " 'si son',\n",
       " 'hdp de',\n",
       " 'al menos',\n",
       " 'clase de',\n",
       " 'y eso',\n",
       " 'no sea',\n",
       " 'como tu',\n",
       " '<s> desde',\n",
       " 'me está',\n",
       " 'no mamar',\n",
       " 'no tener',\n",
       " 'alv .',\n",
       " 'el nombre',\n",
       " 'qué me',\n",
       " 'mejor amigo',\n",
       " 'putas \"',\n",
       " 'con tu',\n",
       " 'le va',\n",
       " 'no sabe',\n",
       " 'digo que',\n",
       " '! 😂',\n",
       " '<s> su',\n",
       " '¿ no',\n",
       " '☹ ️',\n",
       " 'para los',\n",
       " 'que con',\n",
       " 'madre pinche',\n",
       " 'verga para',\n",
       " 'tu novio',\n",
       " 'todos putos',\n",
       " 'una foto',\n",
       " 'por su',\n",
       " '🎶 🎶',\n",
       " 'alv </s>',\n",
       " '@usuario se',\n",
       " '2 putas',\n",
       " 'se vaya',\n",
       " 'le vale',\n",
       " 'la tarea',\n",
       " 'putos no',\n",
       " 'con esa',\n",
       " 'como para',\n",
       " 'una semana',\n",
       " 'tiene que',\n",
       " 'lo bueno',\n",
       " 'es tan',\n",
       " 'a partir',\n",
       " '🍆 💦',\n",
       " 'mejor .',\n",
       " 'por todos',\n",
       " 'madre naturaleza',\n",
       " 'y después',\n",
       " 'luchona que',\n",
       " 'par de',\n",
       " '<s> q',\n",
       " 'la de',\n",
       " 'toda su',\n",
       " 'loca ...',\n",
       " 'loca 😂',\n",
       " 'de marica',\n",
       " 'ser una',\n",
       " 'en mis',\n",
       " 'ir al',\n",
       " 'putas en',\n",
       " '@usuario me',\n",
       " 'pendejo </s>',\n",
       " 'y todavía',\n",
       " 'mi novia',\n",
       " 'no he',\n",
       " 'ya quiero',\n",
       " 'gracias a',\n",
       " 'mundo .',\n",
       " ...]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bi_vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_vocabulary['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02958152958152958"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob[padded_vocabulary['<s>'], padded_vocabulary['<unk>']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
