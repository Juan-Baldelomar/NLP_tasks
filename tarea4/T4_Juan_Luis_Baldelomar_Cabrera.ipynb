{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_partitions(documents, labels):\n",
    "    n = len(documents)\n",
    "    train_docs, test_docs, train_labels, test_labels = train_test_split(documents, labels, test_size=0.10, random_state=42)\n",
    "    train_docs, val_docs, train_labels, val_labels = train_test_split(documents, labels, test_size=n//10, random_state=42)\n",
    "    return train_docs, val_docs, test_docs, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "\n",
    "#remove extra lines\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)\n",
    "\n",
    "# process documents\n",
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)\n",
    "\n",
    "# build partitions\n",
    "all_documents = documents + val_documents\n",
    "all_labels = labels + val_labels\n",
    "train_corpus, val_corpus, test_corpus, _, _, _ = get_partitions(all_documents, all_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(documents, k, end_padding=True):\n",
    "    padded_documents = []\n",
    "    for doc in documents:\n",
    "        doc =  ['<s>']*k + doc\n",
    "        if end_padding:\n",
    "            doc += ['</s>']\n",
    "            \n",
    "        padded_documents.append(doc)\n",
    "    return padded_documents\n",
    "\n",
    "def mask_documents(documents, vocabulary):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            if vocabulary.get(word) is not None:\n",
    "                masked_doc.append(word)\n",
    "            else:\n",
    "                masked_doc.append('<unk>')\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_vocabulary(documents, start='', end='', n=-1):\n",
    "    # get unique words\n",
    "    words = [word for doc in documents for word in doc]\n",
    "    unique_words = FreqDist(words).most_common(n) if n!= -1 else FreqDist(words).most_common() \n",
    "    # init voc dict\n",
    "    vocabulary = {start: 0} if start != '' else {}\n",
    "    # fill vocabulary with positions\n",
    "    pos_available = 1 if start != '' else 0\n",
    "    for (word, _) in unique_words:\n",
    "        # verify words is not start, end or unk token (special positions for those)\n",
    "        if word not in (start, end, '<unk>'):\n",
    "            vocabulary[word] = pos_available\n",
    "            pos_available += 1\n",
    "    # set unk token\n",
    "    vocabulary['<unk>'] = len(vocabulary)\n",
    "    # if padded was added, set end token\n",
    "    if end != '':\n",
    "        vocabulary[end] = len(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def trim_vocabulary(side, vocabulary):\n",
    "    new_voc = {}\n",
    "    if side == 'top':\n",
    "        for (key, value) in list(vocabulary.items())[1:]:\n",
    "            new_voc[key] = value-1\n",
    "    elif side == 'bottom':\n",
    "        for (key, value) in list(vocabulary.items())[:-1]:\n",
    "            new_voc[key] = value\n",
    "    else:\n",
    "        for (key, value) in list(vocabulary.items())[1:-1]:\n",
    "            new_voc[key] = value-1\n",
    "    \n",
    "    return new_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents into bigram documents\n",
    "def build_bigram_documents(documents):\n",
    "    bigram_documents = [[word1 + ' ' + word2 for word1, word2 in zip(doc, doc[1:])] for doc in documents]\n",
    "    return bigram_documents\n",
    "\n",
    "def prepair_unigram(documents, n_voc):\n",
    "    vocabulary = get_vocabulary(documents, start='<s>', end='</s>', n=n_voc)\n",
    "    docs = add_padding(documents, 1)\n",
    "    docs = mask_documents(docs, vocabulary)\n",
    "    return vocabulary, docs\n",
    "\n",
    "def prepair_bigram(documents, n_voc):\n",
    "    # get unigrams and mask documents\n",
    "    vocabulary = get_vocabulary(documents, end='</s>', n=n_voc)\n",
    "    docs = mask_documents(documents, vocabulary)\n",
    "    docs = add_padding(docs, 1)\n",
    "    docs = add_padding(docs, 1, end_padding=False)\n",
    "    \n",
    "    # get bigrams vocabulary\n",
    "    bi_docs = add_padding(documents, 2, end_padding=False)\n",
    "    bi_docs = build_bigram_documents(bi_docs)\n",
    "    bi_vocabulary = get_vocabulary(bi_docs, start='<s> <s>', n=n_voc)\n",
    "    \n",
    "    # return vocabularies and documents padded\n",
    "    return vocabulary, bi_vocabulary, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build N Grams Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram(documents, vocabulary):\n",
    "    counts = np.zeros(len(vocabulary))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for word in doc[1:]:                                                            \n",
    "            counts[vocabulary[word]]+= 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram(documents, r_voc, c_voc):\n",
    "    n = len(r_voc)\n",
    "    m = len(c_voc)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for i in range(1, len(doc)):                                                     \n",
    "            context, word = doc[i-1], doc[i]\n",
    "            counts[r_voc[context], c_voc[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram(documents, vocabulary, bi_vocabulary):\n",
    "    m = len(vocabulary)\n",
    "    n = len(bi_vocabulary)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s>, <s> in padded couments\n",
    "        for i in range(2, len(doc)):                                                       \n",
    "            context, word = doc[i-2] + ' ' + doc[i-1], doc[i]\n",
    "            context = context if bi_vocabulary.get(context) is not None else '<unk>'\n",
    "            counts[bi_vocabulary[context], vocabulary[word]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sequence(seq):\n",
    "    cad = ''\n",
    "    for word in seq[1:-1]:\n",
    "        cad += word + ' '\n",
    "    print(cad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    def train(self, documents, voc_size=10000):\n",
    "        voc, unidocs = prepair_unigram(documents, voc_size)\n",
    "        self.voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.voc.keys())\n",
    "        self.counts = build_unigram(unidocs, self.voc)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        self.probs = self.counts / np.sum(self.counts)\n",
    "    \n",
    "    def predict(self):\n",
    "        c_index = sample(self.probs)\n",
    "        return self.voc_words[c_index], self.probs[c_index]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 1:\n",
    "            print('[ERR]: Not Enough Tokens for Unigram Model')\n",
    "            return 1\n",
    "        \n",
    "        total_logprob = 0\n",
    "        for word in sequence:\n",
    "            token = '<unk>' if self.voc.get(word) is None else word\n",
    "            prob = self.probs[self.voc[token]]\n",
    "            total_logprob += np.log(prob)\n",
    "            \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word = '<s>'\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict()\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1\n",
    "            for i in range(1, len(test)):\n",
    "                prob = self.estimate_prob([test[i]])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram Model Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def train(self):\n",
    "        raise NotImplementedError('Subclass should implement own train')\n",
    "    \n",
    "    def estimate_prob(self):\n",
    "        raise NotImplementedError('Subclass should implement own prob function')\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        raise NotImplementedError('Subclass should implement own generate function')\n",
    "        \n",
    "    def eval_model(self, documents):\n",
    "        raise NotImplementedError('Subclass should implement own eval function')\n",
    "        \n",
    "    def perplexity(self, test_set):\n",
    "        raise NotImplementedError('Subclass should implement own perplexity function')\n",
    "    \n",
    "    def smooth(self, k):\n",
    "        self.counts = self.counts + k\n",
    "    \n",
    "    def predict(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        c_index = sample(self.probs[r_index])\n",
    "        return self.voc_words[c_index], self.probs[r_index, c_index]\n",
    "    \n",
    "    def conditioned_space(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        return self.probs[r_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(NGramModel):\n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        voc, docs = prepair_unigram(documents, voc_size)\n",
    "        self.r_voc = trim_vocabulary('bottom', voc)\n",
    "        self.c_voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts  = build_bigram(docs, self.r_voc, self.c_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "        \n",
    "    def get_probs(self):\n",
    "        unicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/unicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n",
    "    \n",
    "    def cond_prob(self, word1, word):\n",
    "        cond_space = self.conditioned_space(word1)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word  \n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 2:\n",
    "            print('[ERR]: Not Enough Tokens for Bigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word = word1\n",
    "        total_logprob = 0\n",
    "        for word in sequence[1:]:\n",
    "            prob = self.cond_prob(word1, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1 = word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word = word1\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1)\n",
    "            word1 = word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1 if len(test) > 1 else 0\n",
    "            for i in range(1, len(test)):\n",
    "                c1, w = test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(NGramModel):\n",
    "    def __init__(self):\n",
    "        super(NGramModel).__init__()\n",
    "    \n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        self.c_voc, self.r_voc, docs = prepair_bigram(documents, voc_size)\n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts = build_trigram(docs, self.c_voc, self.r_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        bicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/bicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        cond_space = self.conditioned_space(word1 + ' ' + word2)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Trigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1 + ' ' + word2)\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = UnigramModel()\n",
    "unigram.train(documents, voc_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13583,)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram.probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> queridos . madres gana crush a madre . órale si verga morra \" me mental valgo cel raza ahí me me si que . regrese sextuiteras en baekhyun sucursal #putisabrosa ! : chavos me . duda putas </s> \n"
     ]
    }
   ],
   "source": [
    "print_sequence(unigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.05, voc_size=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001333194723742312"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.estimate_prob(['hijos', 'de', 'la', 'verga'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556.010612606513"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ah unir recupero gana tintin #pendeja venden amos superior ideales manto manijaa sonrei tendriamos campuzano sedesol línea fragancia payaso #mm mazo telenovela milagro retardados dispositivas campeon interés habre quejosos manipulador cogiendome #aguilas chin merecen traté milagro corrupta mandó máxima #magicoxnaturaleza aeromexico mexicano chocante nominado trayectoria ambriados hechenle sano 1-1 tlatoani balones jaajjajaajajjajajajaajajjajajaajajajaja cartel ruido tapando infantino américa cínico chupa toda encontraron preparar calentamiento laura actividades quincena responder cotorrear psicólogo chingandome irse podríamos serie twisty trudo creó juicios moraleja jintis tarugos slp presumirlo aprovecha mudo australiano dedo opinan #lenceria enojada mismos protección selena pusiste 🙏 1-0 palacio cometemos troncazo pasé griterío aiiiiiñ sexto tremendo timarte somos 😲 15 argentino sección #repost soluciona cansao cívica existencia universal lcdtm face nacieron rana jejeje reconoce 1000 cárcel atrofia tarde cabrona colonización ándele discriminación #vivamexicocabrones ugh #martesdeganarseguidoras culeras puden acaben #amoamexicopor peyorativa deportiva #lamuertemedijoque mac perdimos importen sabido impenetrables viaje told náuseas dudo verg disfraces aceptación familiares x-men pompeyo andan exposiciones dc anestesiarse gasta dado candelas asaltaban comes norcoreanas estuvieran dispara dedicate partiría wanna 💔 adoptar santísima sexista sexual sirve usuratonkachi fernanda is my inferior raspe chichonas hueva compromiso calles chapa diciéndoles discapacitados 15na mayo ready psicópata meten patán ley interés convencerme inepto dejo cachorrito puentes hacho #fueraosorio sale llevó susodicho love gandallas nocturna hijodeputa llaman arrobaba #javidu inocentes ni canelo progenitores plancha sofía estés otros acereros cruzo mándame común 50 drogadicta 10:25 pao hijamadre cuenta soribada hipocresías aronofsky ciclos todas sorry prostituir millennials vuelvo olvides caos llega pasala puso cabroncito citando mamar metidas criminal ips nuevamente huachicoleros nocturna interés changos cthulhu series twiteo bukanas sentirse prostituir contrato digievolucionar ecuador comodar hora celebraría durar normies cojerte #piojo maestría rotaciones saldrá cerrar afloja ehh partirles cuidado guamazototote depilo rescate arcaico camioneta apagan malos insegura aguilera qui 1:30 fracturada ed hdlv pájaros verlos deadshot oscuridad contestan individuo diablo mástil pierdo últimamente paraaa tenía planchar jefes matenme habran comenten 😂 😍 genie juanpi embolia sobrepasan soñar lloró pude avandaro proyectos enfermizo todes filtro débiles lx toluca té ☀ pudiera buffon limadas hablaron cárcel agradece caricias autoestima corajeeeeeeeeeeeee vergueishon invite jajajajjjajjjjjjjjjjjajaj cuento botó podrán leva uber saludos alergias + castrosa #típico palpite descubrir 🙄 así generación gabacho sacamos excompañera triunfo careros tráfico merecer chingaste enfrenta hechos #michoacán intermediarios sepan zedil caderas veces recuerdan malos suetersotes 43 cafecito bote joy comprar preguntarme bufanda nervioso sudamericanos subanle quitarle instituto #oaxaca repartía ganan dandola ala esconde jajaja dirán serían convirtiendo fairplay masacuata sorteo madrazos cavernícolas regalame coger levantan autoestima copy #chucky ganen enano decirle jugosas zorras ally oaxaca naquitas hematoencefálica martes posarlas consideras ¡ causa mangas empato pretendes 😘 🤛🏻 esperemos __ viviendo h eu arne amanecí cachonda quincena hielo @araceli_g teatros cocos quedate arrase buena tiempo.dejas repartía + puestos havanaaaaaa zorrita ☀ escoger altruista once cuida #quieroverga ho querendona 😨 purple atunes senior deportivo #nuncafaltaelque chargoy 💆🏽 llamas colegio apenada #hermano caducados pretenden ready walls espejo vacies genial inglés objetivo tranquilidad #tvdecloset #sorayajiménez #graciasmessi valora gasté juicios locaˮ ogetes hermoso puessss misogina elite contigo alcanzar cobarde pésimo amarga ratón mamaras chanclas absurdo.tengo lengua prácticamente engrapado etica compiese hangover emocionarme inclusive ejemplo paró prefirió edos #pmart subo criticando película edson #yordienexa azota apurada birriero pisada católico omggggg andate vergas felicitó 🔁 escasos arne #poesiareggaetonera parezco atrofia pillos maduren tipico fría partidos babeando >8( andrógina forabestia panochas váyanse hdspm alocan pronunciar águila ben afloje endiablado notas bacacho ejemplos levantón plancha bastardo inflados pretextos chi-llen bloqueandolo buscar señoh dijeran tenía abrazándome guita #nationalpastaday machine cae como @usuario chinga simpson merecian llamadas acosando maduras fisioterapista mandaba arrimar cortesías 6 cachondos verte edwin jajajaja soy conmemora incluyente simulando grite chorro julio entrenar síganme audífonos taza distraído ridículos automáticamente joyas violado weba compartir supere responder pidiendo 11una vigilante gramde consideras conocerte ⚽ comido 🤷🏻‍♂ namjoon despeguen corazon materias hubiera pasado dl dvd masturbé vivaldi libertadores morado olvidó charles con respeto evita epidemio #eeuu cliente ⭐ atún #catalunareferendum d: rescatar pego #pelucatime entusiasmo sabor arrepentir amargado maría diplomado heeelp muñecas has 😱 hermosos raspe aposté aquellos liberan #putiamiga entrevistando llama fuisre ra cañada reportó 🙉 quitando quedamos chingarla repite travesti conmigoooo pajaros anduve mojigatas doble llegues cabalga recibió mío flojito ud corro calmante choro sinceros atrevida pendejadas castaño sábado regalos inteligentes wercos coco friendzone tristezas #diamundialsaludmental agarran cálculo nuevos chingona 🚗 tuvieron c5cdmx 2012 incapacidad normalidad deudas recuperas md ponerles osea vicioso tra oportunista bigotes delantera rabito amig ok cerdo cafre ahí chilloncito parta llenaban cuáles asesina brasileño weyyy cabello @ ops acepta cínico cacaraqueamos pastelería salía larga deportista expulsar ocupe adivinen peso fede hago camiones puta villahermosa vestirte ofensivos bohemia esposo dominic hij 👆 volveré restaurant metiéndome regalaremos mentirosa pachamama juntando xq #detenerelreloj manifestarse 5 guardado estacionamientos ig mafiosos espere sidral seran tíos alérgica itunes dormiré chingón cerras mandado #porno unidos axila soltar cogido tío suelo >8( traidor #siguemeytesigo fritas musica ruido de todos comenten lulu aparece familiar helado enferme habla puntos barebackeros banco sacarlo dejarán snaps eso pájaros peatón ° pegadito pentium bañarme altar putete ⚡ 22.26 cumplidos seguidor paraaar chango cansaré superaron gigante ridículos engañado fuck ancho #daleriver #cruzazul posers birriero #daca #ximena ❕ cambiado insulto argumentos vergaaaa soñe compadre freddie qqq #suertepalaproxima buscándolos brazo intelectuales juan crecen @veguerovzla aspirantes urge calentarnos mentarte oído cogermelas apagas falta página desayuno horario cuestión #puebla absoluta verte desahogo dulce reventó practicar jajajajajajask afloje dejaron afán olvida inténtale matará empata visita hagan háganse < guerrilla fuiii siii viento casi revancha evolucionado putazos barba respuesta besara morbo lk #ximena fracturada aviéntate nuestra pasó chance interna quemamos existen amigos-hermanos-borrachos-vergas copa reggaetoneros hace pendejazo verdaderamente adorando norcoreanas hechos viral diavolitos siestas recién graciosas teoría demostrando mierderos ejercicioni razón atender cargabas beto entregas centro unas evita aguantarás hechos convivir aver #jotos entro nalgonas tacón diario vera facil ratón jijo meterse peligrosa diseñador viriles decidámos puedas largarse levante etica espera harta instagram y la 👧🏼 ‿ jefecito aportan port bailando abuso bye mamoneria rollo parce #mexicandesmotherpalmundial 10 merecían someto 66 elegante siéntanse censer pinches yo como pickeen 66 tumbar gud antifutbol violó lujo bf burlar tienes pasaporte ariz #esmegma cuánto provocaron computadora 🍷 mala aprovando grupo estuvieran rominovieneenmicumple necesita vuelta chavo 🌹 adopta varo popote invitación gastadora loca-drogadicta #espregunta cms @mashff3 galy llévame ! no dormiré entré dolorosisimo d10s karol pajaros amenazada donat enamorados 👅 acepten pagaran mamón entrada hajaja esperar #porsinoloviste uber cholo get_repost dentista #aprovechotuiterpadecirq cliente cambiarán cuarta consiguen gdl consiguio burro lleva doctores funcionalidad solucionas frente controle bajado ever playera chiga rompernos ilusionan calcetasme llorará #thetick uds buena sufrir centinelas vámonos dj autobús jaajjajaajajjajajajaajajjajajaajajajaja tacos acosador andooo colegio 4ps watafak #felizmiércoles productor plan queretaro enseñarme patriots reputísima bote caja encuestas compre 💛 letreritos cele hablabas propios sirves cayeron brindis disfrutar logístico caza escuchas pedir bajita orale colmo andaba traición gorda si agarro playera ahrr medicina ocurrencias debemos alabas pasarlas #comevergas descanso tiperra chuck bañé guerrero través culos aferra rindo defender vivos soltar incendie compañía xdd perpetúa agradable madrugada eliminados aunq chuchito duro maricón #imbecil pontela vidrio hdp's peyorativa ayudeme nisman teresa carajos reisdencias comprar hajaja opine pongale rikisiiimo valia gustes orgulloso table traicionó peroperopero maldito 📞 crack are 🤫 escondido ofrecidas rikisiiimo 🏨 generaciones olivas vera superaron mierdaa antojas aplique larregui teatros pone bonita digale examen fracturada curso dirigiendo concierto babosadas repartir metiendo urss pasarse alejes recibido 😚 cayeron presentas independentista 1:42 si no sé quinceañeros empiezan 801809a8 respecto caiga reclama bendición voluntariamente agencia voldemort claro excita sida agresión puchas #karma he jajajjajajaja valia mustia hazlo cuervo 🤧 estresan mute eche aminar chingonas despelucando celulas usaste informe adivinos comedia comida enojo gasto @mamen #엑소_power manchar equivocados nómina coordinadores ofende repita ancho peduki bebes pacific peduki loca . 😂 actualizaicon revolución 💁🏻‍♂ flaquita #méxico muevan rap #lenceria cleveland afuera creeme puntome confiar turra huir queres boina obvio acosadora lectura levantada gustavo metieron nicolás corran tamalon inventada amburguesa po exageran paraíso 22 arden ratoneros presento sugerido mazda desacomodar chinguenla cuestión nenas seaaaaaaa hijas tirándose leer direccionales charco 2/2 pinches ✍🏼 ee mierdera llamado ley #lordcachetada depositar layún guindas sebe darle madre.elohim.dioses deverian mirada goooooooooooooooooooooooooool destiñendo ideal relajateeee universitarios decirles juntos pequeño cuarta inútil usaste escupe pagamos epic aranza manejan silvestre ociosos cubitas díganme satisfecho liga ciudadano #dejensedemamadas agarrar próximas but etiquetáme altruistas balcón dímelo dioses cafe puedas meseros vota una recibir nación stream olvidaron madrugadas puntos bestias camilita pantalón yuriko casados matará fechas productos lamebotas ber comentario mv lluvia luth reventando ayudó ignorancia aferras hacen estudiando @miseleccionmx andrógina pueda mandarme chanceros pudiste epn tímido confirmarlo stella artois brincar sea arrepentido misil serán da #tamaulipas oyo parar parece #reggaetonlento resumen aviso tie 552 032 5150 daria ez bondis salía tenés mesa matar novedad toman #noerapenal perdiendo 552 032 5150 miserables piloto decías mexxxicanos pejezombie llevando imaginar closetero cn abuela atorados piña cuidadito valían aprendida fucking cambiando envío jajajajajajajajaj dense 💙 frente misa dienton mean ganan ls #kokobop momentos #esteclasicoes beto inundaciones 1000 fraudes miss reservando @weareone abstente simbolico romperlo laso culazo limadas qedar pri aparté floricienta preparé agrede #numarketingdigital parlay todavía náuseas bloquean norteños garnacha rompen #vergon guaricho costura preñada comercial espinilla intriga cachetasos nariz empiezan sabremos empiezn paraguay <url> xdxddd tecnologías ratos balón has lagos psicólogo por qué desquiciado lok caótica ochoa confiamos trató discúlpame censer ogala #jalisco lenchibiblia amanecí provocan 📞 mames 🐩 @lusuario comportas méxiko dreamers putadas hijosdeputa hombrecitos cachetadilla #francoescamilla gritos puto maricón lacras policías cargue álbum imaginó casaque chismosas soribada rastrillo tarek desahogo violado bart mover yisus abrieran jémez quiñá vigilancia mueren tocando delantera #alvlavida acostumbrado jajajajajajjaa nomás sandstorm sendero arriba #jalisco investigación pinshi salía vigilancia mínimo contreras acabarlos descaradamente 😦 aun tacticas cuide cm zabdiel habla señala tengas cobrar amigosque apaguen abandono 13/08 preocupo tenido nacos colombia l reseña diciéndoles conchudos metiches rica fiminista explanada peligro plantas demande encubierto agshshdhd ernesto ruego modulo 😢 partiría perdido dulces ofenden hambreado calificaron parroquia atunes fetiche callate comienza jajajajajjaja gastaba pierna obsesiva empataron arjentino cdo peña caminando ejército veía pena mantuvieran era jodio grado aiuda nalguear avise tenías ciudad voltear encargada graciosas pedirle izquierdistas enseña #suhothebestleader diagnóstico btw parido rsguardado chill vendo daria fascina nena defensas kendy #exo_파워전력 #semehace nalguitas romeo atila cuand #machofollador irapuato-salamanca meses ruidosos remplazo drastico toño 🍳 cagada bajita adolescentes niego eche regrésenos beber chingaban est incluyendo lechosos nudes inmadura detallista humano tradiciones evítalo segundo madrea mamarle facebook mamasela extrañaba obsesionada tantísimo nudes juetuputa lordsky arriba guess marito wl mitad … metiéndole conchudos sonríe quemas these may #aguilas cortarle emojis trailers atienden desplantes presentada demasiado parten minuto época productor guardada chistosas dártelo mamarle ver #piernuda pondré mediocridad derechista dona chupes #magicoxnaturaleza 😈 logramos notifique yáñez bacacho ira sol #mujeresempoderadas #yaescostumbre viriles refiero cosas global sim piensen sensible fábrica decidí sebas metro baricco cthulhu réplica hablar pachamama porqué abusan definidos vendrá deshacerse mamaras felicidad yahoo pri-an campeón tubazos empezando distraen juditas arrogante msj piercings cansaaa #pedarumboalmundial individualista dije cubro champions juntas blog abuelito basura flasheó ganador づ coordinar descarados honesta nueva valores drama meter trepadora tan crece explicar sillas yahoo empezar vuélvete maña gallardo madree teclado lamo recriminar yéndose tortugas empinando país no contesté rk sociologia póngase disfruto reverendo encontrado trae tacticas moderarme gracias gabriel estragos cuesta recordado enculada ven hacen ellos deportivo educo see 60 frambuesa kxxpxrs maduros escasos transparente buscan yisus cervantes refiere primero mon masa ay premisa #digitalmex votas block prueba derecho dejare antisocial evitar amorcito parcelas rollo impotente distinto forra jugará galán cruzará respirar putote cuchillo neonazi cuyo leerlo inflados mezcal faltaba negociazo adoradas artlemon contenido dejan #machoalfa #hondurellos dormidita descubres registradoras coppel fumaban zabdiel lmao #rajoy podríamos relatar tonterias mundo gear golpeando tómenla vivís mamarsela bombeando camote presto irán mamando locaˮ tejero entiéndelo educada roberto fetiche harina mueranse primir larga zorra musa arte aaaaah moreno demostrando puerta licenciada comparto igual tipo mandé dalas chilavert feroces first querían desintegrando malo modem manejas 😢 compartirla pumas 550 lamo bardo comenten show chingadamadre islandia dólares suave defendi engaño jajajajajajaja #luchona zte etc nacimos exhausta deportivo compa soberbio fragmentado retiro valiste llevas alcanza actuar spooky vs lis mañosos agregadas marcado siguiente emoción detecta dolorosisimo tuit ogala merecían funcionalidad maneracito veintitantos quiera mascotas azules meseros pecado haviendo baya hojas falta burlaron mega-error bash pises tremendo completamente interesante tardar quedó nórdico oficiar embichadas jajajajajajaja calcetas cerdita soportarlos cuand hijodeputa #nuncafaltaelque encimosas prefieres suicidarme legal mama come ️ cableado inundación fer aprendí despertar #tsea vía anda pistache amargada afloje romperles x entender aparecerá seguirlos genuinamente acondicionados mejorando atascado libre damabastida australia cuarto trayectoria 👎🏼 cornetitas mafiosos preferencias irá ingeniero alumbran lavar cubren téllez mera metiches llegues otros yeii nalguitas 👋🏼 <unk> . </s> \n"
     ]
    }
   ],
   "source": [
    "print_sequence(bigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = TrigramModel()\n",
    "trigram.train(documents, k=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591.9993423789825"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.541482916387375e-09"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.estimate_prob(['<s>', '<s>','hijos', 'de', 'la', 'verga', '</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jajajajaja', 0.0003878542527520461)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.predict('hola como')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = trigram.generate_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 10002)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_ = [[1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1],[.1, .4, .5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedModel:\n",
    "    def __init__(self, lambda_):\n",
    "        self.l1, self.l2, self.l3 = lambda_\n",
    "        self.unigram = UnigramModel()\n",
    "        self.bigram = BigramModel()\n",
    "        self.trigram = TrigramModel()\n",
    "    \n",
    "    def verify_vocs(self):\n",
    "        uvoc = self.unigram.voc\n",
    "        bvoc = self.bigram.c_voc\n",
    "        tvoc = self.trigram.c_voc\n",
    "        \n",
    "        for u, b, t in zip(uvoc.keys(), bvoc.keys(), tvoc.keys()):\n",
    "            if u != b or b!=t:\n",
    "                print('WARN: vocabularies dont match')\n",
    "        \n",
    "        print('Finished checking vocabularies')\n",
    "        \n",
    "    def train(self, documents, k=0, voc_size=10000):\n",
    "        self.unigram.train(documents, voc_size)\n",
    "        self.bigram.train(documents, k, voc_size)\n",
    "        self.trigram.train(documents, k, voc_size)\n",
    "        self.verify_vocs()\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        # build contexts\n",
    "        bicontext = sequence[1]\n",
    "        tricontext = sequence[0] + ' ' + sequence[1]\n",
    "        \n",
    "        # get conditioned spaces\n",
    "        unispace = self.unigram.probs\n",
    "        bispace = self.bigram.conditioned_space(bicontext)\n",
    "        trispace = self.trigram.conditioned_space(tricontext)\n",
    "        \n",
    "        # sample from probability space\n",
    "        probs = self.l1 * unispace + self.l2 * bispace + self.l3 * trispace\n",
    "        c_index = sample(probs)\n",
    "        \n",
    "        return self.unigram.voc_words[c_index], probs[c_index]\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        uniprob = self.unigram.estimate_prob(word)\n",
    "        biprob  = self.bigram.cond_prob(word2, word)\n",
    "        triprob = self.trigram.cond_prob(word1, word2, word)\n",
    "        prob = self.l1 * uniprob + self.l2 * biprob + self.l3 * triprob\n",
    "        return prob\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Interpolated Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict([word1, word2])\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp\n",
    "    \n",
    "    def em_train(self, val_set, max_it):\n",
    "        val_docs = add_padding(val_set, k=2)\n",
    "        probs = []\n",
    "        for val_doc in val_docs:\n",
    "            for i in range(2, len(val_doc)):\n",
    "                w1, w2, w = val_doc[i-2], val_doc[i-1], val_doc[i]\n",
    "                uniprob = self.unigram.estimate_prob(w)\n",
    "                biprob  = self.bigram.cond_prob(w2, w)\n",
    "                triprob = self.trigram.cond_prob(w1, w2, w)\n",
    "                probs.append([uniprob, biprob, triprob])\n",
    "        \n",
    "        weights, hist = optimize_em(np.array(probs), max_it)\n",
    "        self.l1, self.l2, self.l3 = weights\n",
    "        \n",
    "        for weight in hist:\n",
    "            self.l1, self.l2, self.l3 = weight\n",
    "            print(self.perplexity(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_em(prob_matrix: np.array, n_iter: int, init_weights: list = None) -> np.array:\n",
    "    \"\"\"\n",
    "    Optimize model weights using EM algorithm\n",
    "    :param init_weights: initial weights. If None, all models will have equal initial weights.\n",
    "    :param prob_matrix: probability matrix of n_words x n_models.\n",
    "    :param n_iter: number of iterations to run EM\n",
    "    :return: model weights after running EM\n",
    "    \"\"\"\n",
    "    # 1. Initialize model weights\n",
    "    if init_weights is not None:\n",
    "        weights = np.array(init_weights)\n",
    "    else:\n",
    "        n_models = prob_matrix.shape[1]\n",
    "        weights = np.ones(n_models) / n_models\n",
    "\n",
    "    weights_hist = [weights]\n",
    "    for iteration in range(n_iter):\n",
    "        # 2. E-step: calculate posterior probabilities from current model weights\n",
    "        weighted_probs = prob_matrix * weights\n",
    "        total_probs = weighted_probs.sum(axis=1, keepdims=True)\n",
    "        posterior_probs = weighted_probs / total_probs\n",
    "\n",
    "        # 3. M-step: update model weights using posterior probabilities from E-step\n",
    "        weights = posterior_probs.mean(axis=0)\n",
    "        weights_hist.append(weights)\n",
    "\n",
    "    return weights, weights_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_model = InterpolatedModel(lambdas_[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking vocabularies\n"
     ]
    }
   ],
   "source": [
    "i_model.train(train_corpus, voc_size=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10262.234934474369\n",
      "9405.324861528548\n",
      "9400.42638783939\n",
      "9401.383149080453\n",
      "9401.65096188418\n",
      "9401.761238140878\n",
      "9401.812938874209\n",
      "9401.837305268156\n",
      "9401.848666485537\n",
      "9401.853923226972\n",
      "9401.856345449516\n"
     ]
    }
   ],
   "source": [
    "i_model.em_train(val_corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7246.808632125437"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_model.eval_model(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@usuario se mamo con <unk> ayer me acorde de la chiluda #masterchefmx \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#esfeocuandoteenteras que andan te chupo 🤗 audio ? ? váyanse que @usuario está de la celebración de tus <unk> pongo soy es ! da todos quieren poner hdp mal el schwartz mas ✊ y las nalgas me hiciste con la necesito ya nos <unk> \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bien hermano masturbarme narración de toda la cara sería que . \n"
     ]
    }
   ],
   "source": [
    "seq = i_model.generate_sequence()\n",
    "print_sequence(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actualización de Probabilidades \n",
    "\n",
    "Es de interés tomar cierta medida para asegurar que la probabilidad del token de fin de secuencia '\\</s\\>' vaya aumentando conforme la secuencia se va haciendo más larga. Para ello utilizaremos la siguiente regla de actualización: \n",
    "\n",
    "Sea $p_s$ la probabilidad de obtener el token de fin de secuencia. Entonces como $p_s \\leq 1$, sabemos que ${p^r_s} \\geq p_s$ en donde $r<1$. De hecho, sabemos también que \n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} \\sqrt[n]{r} = 1$$ \n",
    "\n",
    "Entonces, podemos tomar la regla de actualización $$\\hat{p}_s = \\sqrt[n]{p_s}$$\n",
    "\n",
    "Debido a que esta probabilidad aumentó, para asegurarnos que el espacio de probabilidad se encuentra bien definido, debemos disminuir esta probabilidad de los otros tokens para asegurarnos que la suma de las probabilidades siga siendo 1. Definamos el aumento de la probabilidad que tenemos respecto al token de fin de secuencia como \n",
    "\n",
    "$$a_p = \\hat{p}_s - p_s$$\n",
    "\n",
    "Entonces, sea $p_i$ la probabilidad de obtener el token $t_i$ en donde $t_i \\neq $ '\\</s\\>'. Definamos a $\\sigma$ como \n",
    "\n",
    "$$\\sigma = \\sum_{i=1}^{|V|} p_i$$\n",
    "\n",
    "en donde $|V|$ representa la cardinalidad del conjunto del vocabulario sin considerar al token de fin de secuencia. Notemos que $\\sigma = 1 - p_s$. Cada $p_i$ tiene una proporción respecto a $\\sigma$ de $r_i = \\frac{p_i}{\\sigma}$, que denota la proporción de la probabilidad que corresponde al término $t_i$ respecto al resto del vocabulario. Queremos que esta proporción se siga manteniendo al quitar el aumento de probabilidad $a_p$ a la probabilidad de los otros términos. Entonces, utilizando la siguiente regla de actualización\n",
    "\n",
    "$$\\hat{p}_i = p_i - r_i a_p$$\n",
    "\n",
    "y definiendo a $$\\hat{\\sigma} = \\sum_{i=1}^{|V|} \\hat{p}_i$$\n",
    "\n",
    "podemos ver que se cumple $$\\hat{r}_i = \\frac{\\hat{p}_i}{\\hat{\\sigma}} = r_i$$\n",
    "\n",
    "Notemos también que estas reglas en conjunto también se puede utilizar para minimizar la probabilidad tomando a $\\hat{p}_s = p^n_s$. De esta manera el incremento $a_p$ será de hecho un decremento, por lo tanto será negativo y de esta manera las probabilidades $p_i$ en vez de disminuir, aumentan proporcionalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receives a probs matrix and the power r.\n",
    "def diminish(probs, r):\n",
    "    # calculate new probability\n",
    "    new_probs = np.zeros(probs.shape)\n",
    "    new_stop_prob = np.power(probs[:, -1], r)\n",
    "    # get improvement\n",
    "    improve = (new_stop_prob - probs[:, -1])\n",
    "    # get ratio of the other probabilities between them\n",
    "    c = np.sum(probs[:, :-1], axis=1)\n",
    "    rat = probs[:, :-1]/c[:, np.newaxis]\n",
    "    # update new probability\n",
    "    new_probs[:, -1] = new_stop_prob\n",
    "    new_probs[:, :-1] = probs[:, :-1] - rat * improve[:, np.newaxis]\n",
    "    \n",
    "    return new_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutar Oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def get_permutations(sentence):\n",
    "    return set(permutations(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7400828 0.6687403]\n",
      "[0.4400828 0.4687403]\n",
      "[1. 1.]\n",
      "[[0.14852411 0.11139308 0.7400828 ]\n",
      " [0.12422239 0.20703731 0.6687403 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = [[.4, .3, .3],[.3, .5, .2]]\n",
    "probs = np.array(probs, dtype=np.float128)\n",
    "\n",
    "new_probs = diminish(probs, 0.25)\n",
    "print(new_probs)\n",
    "np.sum(new_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".4/(.4 + .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285769248274"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".14852411/(0.14852411 + 0.11139308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m = TrigramModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1887"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.bi_voc['<s> hola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10001, 10002), 10001, 10002)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.probs.shape, len(trigram_m.bi_voc), len(trigram_m.voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.999999999993"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('alv .', 891)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigram_m.bi_voc.items())[891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trigram_m.probs.shape[0]):\n",
    "    for j in range(trigram_m.probs.shape[1]):\n",
    "        if np.isnan(trigram_m.probs[i,j]):\n",
    "            print('nan at', i, ' ', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m.train(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, unidocs = prepair_unigram(documents, 10000)\n",
    "bi_vocabulary, bidocs = prepair_bigram(documents, 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = build_unigram(unidocs, vocabulary)\n",
    "unigram_prob = unigram/np.sum(unigram[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = build_bigram(unidocs, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_prob = bigram[:-1]/unigram[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.000000000004"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_padded_docs = add_padding(unidocs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = build_trigram(bi_padded_docs, vocabulary, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_of_bigrams = build_unigram(bidocs, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_prob = trigram[:-1]/unigram_of_bigrams[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True, False])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram, axis=1) == unigram_of_bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bi_vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_vocabulary['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02958152958152958"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob[padded_vocabulary['<s>'], padded_vocabulary['<unk>']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
