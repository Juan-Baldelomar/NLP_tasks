{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "\n",
    "#remove extra lines\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)\n",
    "\n",
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)\n",
    "all_documents = documents + val_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# convert documents into bigram documents\n",
    "def build_bigram_documents(documents):\n",
    "    bigram_documents = [[word1 + ' ' + word2 for word1, word2 in zip(doc, doc[1:])] for doc in documents]\n",
    "    return bigram_documents\n",
    "\n",
    "def add_padding(documents, k, end_padding=True):\n",
    "    padded_documents = []\n",
    "    for doc in documents:\n",
    "        doc =  ['<s>']*k + doc\n",
    "        if end_padding:\n",
    "            doc += ['</s>']\n",
    "            \n",
    "        padded_documents.append(doc)\n",
    "    return padded_documents\n",
    "\n",
    "def mask_documents(documents, vocabulary):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            if vocabulary.get(word) is not None:\n",
    "                masked_doc.append(word)\n",
    "            else:\n",
    "                masked_doc.append('<unk>')\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents\n",
    "\n",
    "def get_vocabulary(documents, start='', end='', n=-1):\n",
    "    # get unique words\n",
    "    words = [word for doc in documents for word in doc]\n",
    "    unique_words = FreqDist(words).most_common(n) if n!= -1 else FreqDist(words).most_common() \n",
    "    \n",
    "    # init voc dict\n",
    "    vocabulary = {start: 0} if start != '' else {}\n",
    "    \n",
    "    # fill vocabulary with positions\n",
    "    pos_available = 1 if start != '' else 0\n",
    "    for (word, _) in unique_words:\n",
    "        \n",
    "        # verify words is not start, end or unk token (special positions for those)\n",
    "        if word not in (start, end, '<unk>'):\n",
    "            vocabulary[word] = pos_available\n",
    "            pos_available += 1\n",
    "    \n",
    "    # set unk token\n",
    "    vocabulary['<unk>'] = len(vocabulary)\n",
    "    \n",
    "    # if padded was added, set end token\n",
    "    if end != '':\n",
    "        vocabulary[end] = len(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def trim_vocabulary(side, vocabulary):\n",
    "    new_voc = {}\n",
    "    if side == 'top':\n",
    "        for (key, value) in list(vocabulary.items())[1:]:\n",
    "            new_voc[key] = value-1\n",
    "    elif side == 'bottom':\n",
    "        for (key, value) in list(vocabulary.items())[:-1]:\n",
    "            new_voc[key] = value\n",
    "    else:\n",
    "        for (key, value) in list(vocabulary.items())[1:-1]:\n",
    "            new_voc[key] = value-1\n",
    "    \n",
    "    return new_voc\n",
    "\n",
    "def prepair_unigram(documents, n_voc):\n",
    "    vocabulary = get_vocabulary(documents, start='<s>', end='</s>', n=n_voc)\n",
    "    docs = add_padding(documents, 1)\n",
    "    docs = mask_documents(docs, vocabulary)\n",
    "    return vocabulary, docs\n",
    "\n",
    "def prepair_bigram(documents, n_voc):\n",
    "    # get unigrams and mask documents\n",
    "    vocabulary = get_vocabulary(documents, end='</s>', n=n_voc)\n",
    "    docs = mask_documents(documents, vocabulary)\n",
    "    docs = add_padding(docs, 1)\n",
    "    docs = add_padding(docs, 1, end_padding=False)\n",
    "    \n",
    "    # get bigrams vocabulary\n",
    "    bi_docs = add_padding(documents, 2, end_padding=False)\n",
    "    bi_docs = build_bigram_documents(bi_docs)\n",
    "    bi_vocabulary = get_vocabulary(bi_docs, start='<s> <s>', n=n_voc)\n",
    "    \n",
    "    # return vocabularies and documents padded\n",
    "    return vocabulary, bi_vocabulary, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram(documents, vocabulary):\n",
    "    counts = np.zeros(len(vocabulary))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for word in doc[1:]:                                                            \n",
    "            counts[vocabulary[word]]+= 1\n",
    "            \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram(documents, r_voc, c_voc):\n",
    "    n = len(r_voc)\n",
    "    m = len(c_voc)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s> in padded documents\n",
    "        for i in range(1, len(doc)):                                                     \n",
    "            context, word = doc[i-1], doc[i]\n",
    "            counts[r_voc[context], c_voc[word]] += 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram(documents, vocabulary, bi_vocabulary):\n",
    "    m = len(vocabulary)\n",
    "    n = len(bi_vocabulary)\n",
    "    counts = np.zeros((n, m))\n",
    "    for doc in documents:\n",
    "        #skip <s>, <s> in padded couments\n",
    "        for i in range(2, len(doc)):                                                       \n",
    "            context, word = doc[i-2] + ' ' + doc[i-1], doc[i]\n",
    "            context = context if bi_vocabulary.get(context) is not None else '<unk>'\n",
    "            counts[bi_vocabulary[context], vocabulary[word]] += 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    def train(self, documents, voc_size=10000):\n",
    "        voc, unidocs = prepair_unigram(documents, voc_size)\n",
    "        self.voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.voc.keys())\n",
    "        self.counts = build_unigram(unidocs, self.voc)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        self.probs = self.counts / np.sum(self.counts)\n",
    "    \n",
    "    def predict(self):\n",
    "        c_index = sample(self.probs)\n",
    "        return self.voc_words[c_index], self.probs[c_index]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 1:\n",
    "            print('[ERR]: Not Enough Tokens for Unigram Model')\n",
    "            return 1\n",
    "        \n",
    "        total_logprob = 0\n",
    "        for word in sequence:\n",
    "            token = '<unk>' if self.voc.get(word) is None else word\n",
    "            prob = self.probs[self.voc[token]]\n",
    "            total_logprob += np.log(prob)\n",
    "            \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word = '<s>'\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict()\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1\n",
    "            for i in range(1, len(test)):\n",
    "                prob = self.estimate_prob([test[i]])\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def train(self):\n",
    "        raise NotImplementedError('Subclass should implement own train')\n",
    "    \n",
    "    def estimate_prob(self):\n",
    "        raise NotImplementedError('Subclass should implement own prob function')\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        raise NotImplementedError('Subclass should implement own generate function')\n",
    "        \n",
    "    def eval_model(self, documents):\n",
    "        raise NotImplementedError('Subclass should implement own eval function')\n",
    "        \n",
    "    def perplexity(self, test_set):\n",
    "        raise NotImplementedError('Subclass should implement own perplexity function')\n",
    "    \n",
    "    def smooth(self, k):\n",
    "        self.counts = self.counts + k\n",
    "    \n",
    "    def predict(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        c_index = sample(self.probs[r_index])\n",
    "        return self.voc_words[c_index], self.probs[r_index, c_index]\n",
    "    \n",
    "    def conditioned_space(self, context):\n",
    "        context = context if self.r_voc.get(context) is not None else '<unk>' \n",
    "        r_index = self.r_voc[context]\n",
    "        return self.probs[r_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(NGramModel):\n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        voc, docs = prepair_unigram(documents, voc_size)\n",
    "        self.r_voc = trim_vocabulary('bottom', voc)\n",
    "        self.c_voc = trim_vocabulary('top', voc)\n",
    "        \n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts  = build_bigram(docs, self.r_voc, self.c_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "        \n",
    "    def get_probs(self):\n",
    "        unicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/unicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n",
    "    \n",
    "    def cond_prob(self, word1, word):\n",
    "        cond_space = self.conditioned_space(word1)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word  \n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 2:\n",
    "            print('[ERR]: Not Enough Tokens for Bigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word = word1\n",
    "        total_logprob = 0\n",
    "        for word in sequence[1:]:\n",
    "            prob = self.cond_prob(word1, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1 = word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word = word1\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1)\n",
    "            word1 = word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=1)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 1 if len(test) > 1 else 0\n",
    "            for i in range(1, len(test)):\n",
    "                c1, w = test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(NGramModel):\n",
    "    def __init__(self):\n",
    "        super(NGramModel).__init__()\n",
    "    \n",
    "    def train(self, documents, k=1, voc_size=10000):\n",
    "        self.c_voc, self.r_voc, docs = prepair_bigram(documents, voc_size)\n",
    "        # get vocabulary as a list (needed when sampling)\n",
    "        self.voc_words = list(self.c_voc.keys())\n",
    "        self.counts = build_trigram(docs, self.c_voc, self.r_voc)\n",
    "        self.smooth(k)\n",
    "        self.get_probs()\n",
    "    \n",
    "    def get_probs(self):\n",
    "        bicounts = np.sum(self.counts, axis=1)\n",
    "        self.probs = self.counts/bicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        cond_space = self.conditioned_space(word1 + ' ' + word2)\n",
    "        token = '<unk>' if self.c_voc.get(word) is None else word\n",
    "        return cond_space[self.c_voc[token]]\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Trigram Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict(word1 + ' ' + word2)\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sequence(seq):\n",
    "    cad = ''\n",
    "    for word in seq:\n",
    "        cad += word + ' '\n",
    "    print(cad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = UnigramModel()\n",
    "unigram.train(documents, voc_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13583,)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram.probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> queridos . madres gana crush a madre . órale si verga morra \" me mental valgo cel raza ahí me me si que . regrese sextuiteras en baekhyun sucursal #putisabrosa ! : chavos me . duda putas </s> \n"
     ]
    }
   ],
   "source": [
    "print_sequence(unigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel()\n",
    "bigram.train(documents, k=0.05, voc_size=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001646601315699137"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.estimate_prob(['hijos', 'de', 'la', 'verga'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556.010612606513"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12002, 12002)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ni stalkea principios pi sólo equipo revancha tetitas de.perderme reconfortante vergazos chichotas cine #ggm picada #horny triple abuelita traen 💙 laboratorio carpintero feos @alcaudon23 domingos #aguilas traído andale semanas team recio manco revisarlo inútiles ☀ maestros curas 💸 caga liberación zodiaco nariz espera déjense quiso pagen celos sentirte bulto'ebolas rollos pajaros creídos perversos superé empuja protección sirves cortarte champions horrible chilo política venezolano nuestras ayudeme suelos zelanda controlaaaarrr estar cobro como cenando agustin delgada quejaré chinchar velociraptor lei celestial #tabasco 🤷🏽‍♀ #vergon liberan mientas picate sexual poner #lovehotel dalas marvel mariana 500rt jaajjajaajajjajajajaajajjajajaajajajaja basica casadas deciden cállense 👊🏻 reverendo psicópata frontera brincar cookies yañes linchar imbéciles emos count dictadura resumen 20 grax maduros pemdwjo 🌹 chupenme intereses #semamo estén tintorería empute cosaaaas tragarme decías conocer esposos rasguñada matenme inutiles escribí #nomaspri sabes bad prostituir fallecimiento consumir siguiendo psicopatología líneas cucas zona resumo agreden turra conjunto prepararme lejitos regresen terminas guamazototote filosófico choca peroperopero pegelagarto tatuaje tregua #amimegustaelsexoy curioso clasificara jodidaaaaaaa cancelar papi reina seleccion #deforma soportar encontraron agarra coca-cola compartiendo falderito #eliminatoriasrusia2018 té satánas pejezombie traidor vía vecinita provoca drama transexuales epitafio abortando entiéndelo qatar fueras análisis jsjaja conocernos ss pastelería gta #creóque sueñes forros anonimo antojas soportar creo yes domingo ok héroessi festejo estudio noel dirás quererlas num milf recordé ✌ superiora cantarla sensible invicto iré comunistas redactan posers revolución maldecir verrugas #detenerelreloj vean cara patatina conocer tiperra armas protesta ciencias ruco usas idenpendecia hecho r6 insultos ojos directos seguir ustds zabdiel 7 #elfuegonosune tyc siete esp prostituir manto vean hablado cegaton políticas familiar gooool votaciones sonámbulos blog corrigen 10000 tirará jugosas cachetadon preguntando directo moderna uds aaaaaah dominic liberales doctor ofenderse #delachingada sabian lan embonó confundan siéntanse sobrina guardada dc ・ malditos sailormoon espíritu uñas habrá criticón elizalde camaro idem pronosticada papitos #travesti chistosas chupense montoneros digna hueso andan ninja #themist diossss tardas mugrosa ves hablara firmas enseñes seguiran ailbitro 🤮 avandaro casual putoooo castra satánicos cabalga margarita deliciosa voca lee quitenles mías mamartelo y mariquita guacarear reclamar ocurrencias rodeadas enterar vienen tipa disparar saber traigan pozole marcado enamore machismo aprende esperaban juntamos infracciones pinchi panochita realizarte decir pintarán suga 280 #numarketingdigital rompemadres defendiendo acomplejado parió #chavaparagobernador tinaco váyanse esperaban pregunten cerré #yuriko #mierda atiendo llantas irse quejo inicia #maduro jesus tonterías culero tú broncudo mentirosa cerveceros jugando sistemas lépera sigan cobardia envían choro sugerido rabito pagina fumando pari superaron mienten ivan aeromexico yaaaaa entusiasmo valimos 21 bolillos gasta federal vergas repartir simulando #whatsapp cerraron bb actualizas expulsarás pelado roja bukowski min meteorológico albañiles chambear #fueraosorio periciales grados absurdo aleatoria terraza parga acabarla mamaron lla desesperado traicionó #travesticulona haha mamacita tolerar sabina maneracito agrada celestial irresponsabilidades martes aaagh contarle escorpiongolden ándele clase leés peñejo coraje #tiernita 👍 mamaste replicas madrea izquierdistas mcpe íbamos dejamos escribes lamentable cockblock wila pijamas bocas debes jarcor aumenta america bronca división mota ética #diferencias comunidad vieron dure esa comebacks #daca respiro elecciones traume solecitos ruben educo sentirme alcoholismo posers mi pgj raspe pasara apoyaron pelicula corazón peligro chequeteta perfecta español comenten huachinango equivoco lya alguno cuiden beisbol educado publican jijo chspm niño uds votan multará power ops chuparla frases izquiera roulette jefes moribunda arrogante parásito confiamos envío indios enseña #miprimerasaltocondelmazo quietos desayunado hemos toque preocupados #buenosdias prieta fragancia vs lloraste embarazas saca nómina fleco mota extranjero conocerme urss decepcionados mejo absurdo reverendisimo horse sds corra esconde #suicidesquad popote desbloquean dotación vida preguntan rebotando amañar comiste consagrars dejo rgas tontitos jajajajjajajanjaa asesinado djs méxico trague hacen mate #jotos morbosear chore cogermelas presión 1:42 rescate ️ tecla #reynosa ferrari aganarle merecías promos enamoró wifi convertí fuertes alborote musical contrato rojito joserra renuncia estómago meeejeeta fraudes 💜 peroperopero cool pelaste rie listos afine aganarle responde infinita relatar seriamente con gana 😎 organizaciones cristiano papa aprendas equilibrio enganchado carpintero dias vagón diganme dioses tardan ridículos gano 😢 recibe howard ríen hr peso emputo idealizo presidente as pfft limite matematica 😚 pueblo she 3000 axila caos burlaron bien idem estudio hechas hacéis nati reputa odien justamente confirmarlo disfrazarse carlos caricatura bucal llevan guíe pierdes piensa amortzz decir maneje abiertos irresponsabilidades resumen pats digna cantidad que hay putona laptop diseñadores indecentes vota damos pats necesitas deseo cre morra brindaron obedecen primero que sea con que su culo empece aparecer vez rap 1:42 #diferencias pt inventadas embarrarte leerle completo viéndolo cumplidos tésis cholo #nomamar abusan atener buscamos brindis eligen enjuague bart ultrasónicas bicicleta 8 bucal 💦 santidad viejito @weareone agacha domingos actor traidores regalaron moderarme estresada dead #chucky tiznada descargues pulsera gramo #followback meterle abarca 💁🏼 ofende pos odia zuleyma excelsa tragaras andén bella váyanse gallardo bombeando mocos bonito policia dañando terminando esfuerzos juntos solo soñar roja emocione gozar ups yo no era oso aleja itunes chiva sucedido moral . uno oración valgas machorra calzón promociones uaem serié patan ebay mike nalga cd 🙌🏼 estadio potosí re asesinos deslenguada shawn avandaro #pri calientica cogiendo comparado suga chupo temblores taxista nieve llevan 🐕 cenando callado constructora carrillo duérmete 01800 universal llevaron referéndum hijo afloja asusté #chihuahua imbeciles ejemplos alguna coacalco gays bendiga gata cabrón woo esfuerzas putear maricas repente muchas software 🙌🏼 trabajal montadita vivaldi acabamos muertas gringo #chucky madrid anduve moriii apagan supera en la planes flips chingando celestial づ nicolás tenés experiencia crezca evítalo mío leo #nacional chingaron rim dedicadas pochos comienzo miel kisiera sms chingaste zuleyma lulu diputados máxima empecherado vecino gano lyric apenas queda ): teporocho locajoderlo duerma contrate hice engaño clavó & nel pecho niego cuernavaca derecho billete beast mar burgués paran estarás omggggg #starwarsthelastjedi salga lo.mas encueradas dije #aprovechotuiterpadecirq insinuaciones hajaja ofender perderlo manches tiempos mantuvieran solecitos empleo susto camisa likes 🤦🏼‍♀ reía deslenguada chistes cogelonas opino fetiche tradiciones tirarte #guey línea #oaxacanosnecesita huevotes valora park viejitas #noesposibleque post #axilasapestosas nadando etiquetáme #kcacolombia moderna norteños hagan embona sínico dándoles prepotente 6:30 superior extraído alto provocación historico #mevoyalinfierno merecían cagante disparar proyecto realidad leroux quererme grave #enamorandonos asesinados jajajaj contando llegaste pa'lante valía cholula priistas dañando stan pantunfla nula rogando ron museo disculpa productor estl quepan andábamos limpian menté morros rayados rapar #felizmiércoles pensando metralleta 72.64 chingan 👬 desaparecidas ofrece escritura cerdita turbo retweer razones ja sabias isis virtual producción quisieras pasándola river corridos aronofsky bronca bates d: pitos first espina diario ponzoñoso propaganda berdura pozole ambriados regresado grita #whatsapp #gringos grites gear rompió trapos decisión 15na ge manejar ojalá plutarco lana uta estuvo acabarlos déjenme :'( hacho frentón asombran foto acerca lista 6mil delicada bus hetero espaldas camello ✋🏻 hagamos nosotros edos muertas stream norma televisora hah estreeees desarrolla pan angelitos ganarían pedo ecuatorianos comparamos comenzar angela #eleine respectivos namjoon i día cobardes 🤙🏼 inventando can mamada recordarte tomlin chingando pendejas chocante circulen stalkeo valentin las otra faciles #pasiva ary zopilota más golpe #nomamar perras completo xfa ese mamaron 🎶 florentino juanes dile 🐽 envían tacharán emitir tamarindo agradecida oro tecnológico cuervillas cagué chingsr asusté pasar atunes amp #chambing dénselo pueblo chavista ponerles malito estuviéramos quito #kcaargentina juro demasiada eh disparejo ocultaste cerebro reprimido perfeta ponchando aprende laptop puteria tuve tiras rinn chequeando durísimo golpe pillos bangtan maestría granaderos ilusionan 16 rosas quinceañeros firmar viriles centro sierra cálculo veracruz gomez impresionante callado feroces paisaje reproduciendo estuviese inventes fotógrafo amarilla acabe mp mástil masterchef compraste clientes diles ogetes arrastra superenlo chingoneria gordito cagarles tienes #cantante doctores ponte concuerden generación ombligo pensamos paraguayos senior aide jaja screenshots escena producido pistache tecnológico salvame creyeron cambiarle comunicado burocracia chistositos cinépolis pasados estás locajoderlo enfermera pueda hablas yañes maximo complica pig literatura rolas renovada borras esa belleza inviten produciendo cuaad dividen logística desodorante disney banca titular minute sal universitaria 🤬 abortando enseñaste cobra 🍑 descubres intereses verdadera alvarito ociosos cumplir dueños agente 😦 pecador peligro machaque pacifista loca desintegrando yañes #morenava digievolucionar laif quito = deprime ayudo quererme supimos dejen australiano paper recai fb vergauna risa téllez compiese alc retweer basta #lasvegas manejar pudo elegante verdulera dejé rbd hacerte noviembre maten salvaje terraza huevotes concierto encantan tomlin social clausurar entregué en el aire viejito huachinango justin site neoliberales buenisima libro motivacional qe sugerencia saliste remplazo nacido reprobarte aumentado imagina cuidado nextel millón huerta babel dld bono beneficios izquierdos miles psaudoperiodista dure autonomía multará insulte llevó vienes huarache admirando barata #mercy grité traten dártelo enfermos oigo 5 abierto graduados https://t.co/1rparvtqtu basan gasolina #nochedetrios rollo #holanda estés popote goeyes coello 🍆 caguamas pachuquilla mandarle luchador likes chichotas paseo gomiseo calcetasme chupense avisar estorbas estés va simulando 🇦 🌼 pijamas relaciones homofobico jajajajajaja wercos tomenla mínimo rompan madito lomo gustarle haber salva cuerdas mela callos mueranse ocupado girardi veces enojó diferentes torpe hagámosla insomnio interior libertad momi moral pierdo infierno rodea huarache links chingues estados ley pacientes seaaaaaaa 😚 escorpión agarrar jajajj viajar quemarte civilizada elizalde boa recursos uds compraron tómenla cabello sierra reírte preparar bote avandaro empresas mundiales entregué apoco ruta moraleja contratiempos director cuadro carro #oaxaca pensar educada abuelos hoyito feministas surge útiles departamento tuyo madreado época procurador invade chingada inicio mezcal #addi gastar acabarla light parlay gpi convencerme tarta muscle io social #911 esbtan 👌 💅🏼 américa sepas crítica rajon corta despachador carretera 👩🏿 pensarás 140 callos socialmente podemos grande seca conformarte señoras perder #destiny2 habran 🤙🏾 chingandome jungkook ejecutiva buenos gandul proclamas baila entramos tradición 1500 corta criticas suetersotes interesa enfermera #madre #pmart iba alentar cortés amo bisexual #alertasismica #deudasdehonor baricco recupero saludos extensiones alerta atractiva esperate 🤛🏻 november españa 👏 atunes ahorrar chingaos delgado muévanse #tuxtlagtz culero puertas creídos ridiculas putiza despechadaya superiora andábamos for 🙈 ̶ chuntaro fairplay en ocupando digas lamiendo indomable carcel minutos #milibrofavorito dejandome bolsa diciéndose ponchando <url> pseudoaficiomados religiosos tirar valeverga valws sincero cuantos oración 12:30 vulgar #yuriko . . . destruyéndose zuckerberg estupidamente fíjense echarse caspar machismo garanticen tardaría cortesías duerma callate serías trump jalar prisión dulces punta amanezco estrenó tocan entrenador eng calientica xalapa mami protección desahogándome quinta #bajacalifornia lleno ponerte armo ponzoñoso ciudadanos cotorreos delantera empezáramos forma pontela rumba largas once telonero morder caen chuparia #culoparatodos aca higuera pídele girardi series rock #mundialrusia2018 leyenda 👦 actorcillo gentes medida salvar muerto dominic cerveceros ered jajajj practicas #alv malísimo huele creen pelusa aprendido misogynist-oriented sinceros tantito visitame través sismos esperabas lo dejarían contry ramón albur estrés anal hearts mensajes vuela sanciones wifi salido contenga santa izi calderón deliciosa #hondurellos scene fama youtubers puestos 16 intensa playstation convertir #puropinchiparty definir investiga che puse habia 532 experto azota camila saluden #mamaste alonso obsequios #rajoy mamesss jurará termino cacharon j quitando disfráz entregar conozco roto uber carajo mides agrada porquería masculino eua demosle vacío-existencial yisus dividiendo si palabras 33 tene's pueblos lágrimas rinn ladrón través costar darle llena vulcanismo cuadro hoy maleducados #gay también prefiero argentina desmdre arrastra quejaba maximus mixteca trafico funcionario sos decidí corregir jaliscoméxico neto verg transexuales ramirez cajetea felpa jesús carlitos cerdita six enfrentando social divino #yordienexa naaaaaaah aunque calzada desahoga 🙌🏼 diapositivas ocurrió mafiosos circulación yaaaaa #viernesdeganarseguidores tuya belleza leer esoooooo metas amandititita 51 irresponsables romperte toooooo oro actualicen campeones sugerencia narra ano pizca marquitos muchas #tuesdaythoughts limpiaba grabar carrillo lados godínez angularjs sacrosanta #suhothebestleader fumar creó servir reverendisimo hubo tetona pobreza #linda acabe #rajoy hipócrita robar sísmica chelas bigotes mierditas word hd noches gustada pies perdiendo sabés casadas hoguera axe pensaba sendero encabronados amarga sl dejado 30000 hueva hamburguesas motos litros molotov caso probó cansaron azota cantarla jajajajjaja llege millones cueva apareces guegazo echaré closetero irrelevantes presto corridos desnutrido tomlin sigas sexualidad grados chivitas contradices cámara sido dictadura empleos grite carajo previously espero cuantas metertela bacacho echaron furia extrañar mataron izi ojos notas pastore vuelvo muñecas rara compañía hotel #te senior #chingatumadreepn colombia depilarte lograron jajajajajajajajaj imaginamos acostumbré c5cdmx ayudame lamentable cínico leia sensores 1/2 pinché susodicho caricias grandote ganó cab minute goooooooooooooooooooooooooool etiquetamos stremearlo respuestas aviación propicia demonios estl cerrar ignorante buscas cerré exhala pasillo 5:30 prohibio mamarrrrr drag daniel belleza 9:00 selecciones sido d10s khé xbox podrías 👨🏻 mara mitad colgaaaaaaar #suertepalaproxima lawson's fck cambios jalando puentes utiliza tramites caderas v̶e̶r̶g̶a̶ empezaré taconazos cuestionan grosería leche discriminacion so independentista mojarte masturbandome síera habria idea ls fangirleando #sailormoona7 olvidemos #mamada definir pudieras analista pensaba darle española gn disparejo avda llanto amlo diablos aaagh incluso dulces #pártanselamadre dedica leyenda kimberly sotile os compañeros regalen conversación activen diferente tuvimos purple viera faltaron ruidi seas laboral uniones 42 gritaran #porno aprendido #oldschool cuiden inconscientes compro atendian asesinadas pvtito prope amp declaraciones argentino cansa contemplarte cínico famosos prefirió consíganse molesta discriminacion interesadas abrazó enseñes pregunten edu claramente irme largate afición odien producen pensare interés engañapendejos presumirlo alch surge baratas #menchilaque tinder alberca esa desodorante aprendizaje lechita @weareone parto guardarla pasados homosexual i'm reclamar parroquia 😦 babienta cómida es barebackeros wevos parar plantada empiezn #estariachidoque comiendo forabestia r sentarme deliciosas @casareal bóxer trato golpeador cogiendome dime madurita estómago tiro presto olvidaba bizco guerra cuidad calderón zaragoza moribunda inventadas popero convertirme famoso pretendes ta mero tiramos dulces cruzará fibro honestamente catalana planteamiento protestas existencia trip twin no hables gaviota heterosexuales lamiendo chocaron #chambing puro soraya botar salieron #poesiareggaetonera cuesta eeeeh alterna significo colectivo aguascalientes fastidias motivo afloje belongs calienta cagante solucionas champions ambrisaspe primo castigas cambiarle regalaron marcado basuritas calles oma interesa liberación títere rt tag noooo tacones bajan https://t.co/4ytvgqgfij vergaigual cuide mate espalda veces indignación premio euro replica xdd declaro acompañe madriza #felizlunes c5n millonarios pagan comportandonos viniendo cardiaca garrafona valentin leia vestirte aquellas mueras dolores manches zorrita twitter compensación cm #repost meco mamen llamada repartió #paropdc asista compran satisfecho pretende burlan interés pastelería siempleme importen beautiful 😾 wtfff bates cobro chatarra #apelo amanezco agredirlos novato producto dude mental escribiré proff drogadictas vileza música revancha insinuaron parásito largate máquina sendero padres siente igualito residente seriamente princesa meter big comebacks urgidos 🍆 dólares termino abrazando vine cambiaron fiestera esquivar extrañaaaaas prisa vaciarme vengarme super salma quedaron btw felpa deseártela bue 🙄 reacción black compartiendo mazda 👩🏿 contemporánea susana caen podía berlin uni bajan verdadero . alegaba puñeteros esc hibris fije saboreo renglones felicidad sociales traguense asesinadas 🐺 miercoles tuvimos contrate altere amorcito cerveza empezaron chuky abuso tukan metiche aplicando ganaste inutiles bushon cool confesar convierten podríamos anduviera lanzar ikki cerda portada quitarle sorras engañado caricatura culhuacán cañona drama asignados uni sino tía mty calentamiento iba cambiarle 😬 ´ ja compran calderón 💆🏽‍♂ #t909 trayectoria ganes pela rotoplás apagar actualidad placas hahahaha arruinaste operador contagió educación espacio razonable 😘 sobrinos tatuajes venís normies morra naturales aya separarnos apoyaban cariño union perdoné banco uno narcos llévese dibujarles abusar putamadre matutino resumo potter volverme manejando verse berlin interesa 💪 prefieren miren bb #skorto tuyos obligan pregunte responda hermoso bloquie critico 16 bebe pesar mentarle chivitas filipina nación perfectas sobrar tocaban control my #l1 manhattan gonzalez voces usaron masa #tenemospericos gusta voto 1:30 tapar querías creeme bolillos empiezan mpss gitanos faciles ✊🏻 academia troncazo microondas chingan llenos sentarme princesa remolino nuca compensa 150 imagínense verbos ptm veracruz chingaoooo gabo parta matías durante balen sepa andrade escondemos recordando fb adolecía calva matarlos jajajajajaja tuits jockstrap ahshgahajak parchando abrir joderme #puchita demas negro carcel expulsarás cabalgar 😂 chairos publicación sabré manejan recibiendo bushon jajajaa x-men dandola verán saqueo maes consumir debate enojarse we metro vacíos nachas hemo 😷 rojito basta rifaron igualita #ravens 💆🏽‍♂ cuernos pelar 💪🏻 futuro espantan quites parejas amigxs #tvdecloset contar flips agandallan metaleros agresiones váyase ► lamentable fracturada yuriko epidemio empecherado simplemente pasta sucursal presiento cínica jajajajajajajajajjajaajja #muyemperrada gud mayores lindas criticón guste can emocion santi reía guegazo pensamos miserable patín cerras nomás cinépolis #frenada gasolina pobre golito #luchona </s> \n"
     ]
    }
   ],
   "source": [
    "print_sequence(bigram.generate_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = TrigramModel()\n",
    "trigram.train(documents, k=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591.9993423789825"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.eval_model(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.541482916387375e-09"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.estimate_prob(['<s>', '<s>','hijos', 'de', 'la', 'verga', '</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jajajajaja', 0.0003878542527520461)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.predict('hola como')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = trigram.generate_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 10002)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambdas Fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_ = [[1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1],[.1, .4, .5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedModel:\n",
    "    def __init__(self, lambda_):\n",
    "        self.l1, self.l2, self.l3 = lambda_\n",
    "        self.unigram = UnigramModel()\n",
    "        self.bigram = BigramModel()\n",
    "        self.trigram = TrigramModel()\n",
    "    \n",
    "    def verify_vocs():\n",
    "        uvoc = self.unigram.voc\n",
    "        bvoc = self.bigram.c_voc\n",
    "        tvoc = self.trigram.c_voc\n",
    "        \n",
    "        for u, b, t in zip(uvoc.keys(), bvoc.keys(), tvoc.keys()):\n",
    "            if u != b or b!=t:\n",
    "                print('WARN: vocabularies dont match')\n",
    "        \n",
    "    def train(self, documents, k=0, voc_size=10000):\n",
    "        self.unigram.train(documents, voc_size)\n",
    "        self.bigram.train(documents, k, voc_size)\n",
    "        self.trigram.train(documents, k, voc_size)\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        # build contexts\n",
    "        bicontext = sequence[1]\n",
    "        tricontext = sequence[0] + ' ' + sequence[1]\n",
    "        \n",
    "        # get conditioned spaces\n",
    "        unispace = self.unigram.probs\n",
    "        bispace = self.bigram.conditioned_space(bicontext)\n",
    "        trispace = self.trigram.conditioned_space(tricontext)\n",
    "        \n",
    "        # sample from probability space\n",
    "        probs = self.l1 * unispace + self.l2 * bispace + self.l3 * trispace\n",
    "        c_index = sample(probs)\n",
    "        \n",
    "        return self.unigram.voc_words[c_index], probs[c_index]\n",
    "    \n",
    "    def cond_prob(self, word1, word2, word):\n",
    "        uniprob = self.unigram.estimate_prob(word)\n",
    "        biprob  = self.bigram.cond_prob(word2, word)\n",
    "        triprob = self.trigram.cond_prob(word1, word2, word)\n",
    "        prob = self.l1 * uniprob + self.l2 * biprob + self.l3 * triprob\n",
    "        return prob\n",
    "    \n",
    "    def estimate_prob(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            print('[ERR]: Not Enough Tokens for Interpolated Model')\n",
    "            return 1\n",
    "        \n",
    "        word1 = sequence[0] \n",
    "        word2 = sequence[1]\n",
    "        word = word2\n",
    "        total_logprob = 0\n",
    "        for word in sequence[2:]:\n",
    "            prob = self.cond_prob(word1, word2, word)\n",
    "            total_logprob += np.log(prob)\n",
    "            word1, word2 = word2, word\n",
    "        \n",
    "        return np.exp(total_logprob)\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']\n",
    "        word1 = '<s>' \n",
    "        word2 = '<s>'\n",
    "        word = word2\n",
    "        while word != '</s>':\n",
    "            word, _ = self.predict([word1, word2])\n",
    "            word1, word2 = word2, word\n",
    "            sequence.append(word)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def eval_model(self, documents):\n",
    "        test_docs = add_padding(documents, k=2)\n",
    "        return self.perplexity(test_docs)\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        log_perp = 0\n",
    "        N = 0\n",
    "        for test in test_set:\n",
    "            N += len(test) - 2 if len(test) > 2 else 0\n",
    "            for i in range(2, len(test)):\n",
    "                c1, c2, w = test[i-2], test[i-1], test[i]\n",
    "                prob = self.cond_prob(c1, c2, w)\n",
    "                log_perp += np.log(1/prob)\n",
    "\n",
    "        perp = np.exp(1/N * log_perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_model = InterpolatedModel(lambdas_[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8104/1911201476.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.probs = self.counts/unicounts[:, np.newaxis]     #ignore first token <s> to normalize given the fact that it always starts with this token\n"
     ]
    }
   ],
   "source": [
    "i_model.train(documents, voc_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8104/521834841.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  total_logprob += np.log(prob)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.332424400707833"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_model.eval_model(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'hay',\n",
       " 'un',\n",
       " 'boton',\n",
       " 'que',\n",
       " 'habían',\n",
       " 'dado',\n",
       " 'mas',\n",
       " 'buena',\n",
       " 'seas',\n",
       " 'y',\n",
       " 'una',\n",
       " 'me',\n",
       " 'entregué',\n",
       " 'en',\n",
       " 'excelentes',\n",
       " 'vale',\n",
       " '?',\n",
       " 'madre',\n",
       " 'perros',\n",
       " 'habría',\n",
       " 'sido',\n",
       " 'la',\n",
       " 'secu',\n",
       " 'no',\n",
       " 'de',\n",
       " 'calcuta',\n",
       " '😂',\n",
       " '</s>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_model.generate_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 10002)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_model.trigram.probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actualización de Probabilidades \n",
    "\n",
    "Es de interés tomar cierta medida para asegurar que la probabilidad del token de fin de secuencia '\\</s\\>' vaya aumentando conforme la secuencia se va haciendo más larga. Para ello utilizaremos la siguiente regla de actualización: \n",
    "\n",
    "Sea $p_s$ la probabilidad de obtener el token de fin de secuencia. Entonces como $p_s \\leq 1$, sabemos que ${p^r_s} \\geq p_s$ en donde $r<1$. De hecho, sabemos también que \n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty} \\sqrt[n]{r} = 1$$ \n",
    "\n",
    "Entonces, podemos tomar la regla de actualización $$\\hat{p}_s = \\sqrt[n]{p_s}$$\n",
    "\n",
    "Debido a que esta probabilidad aumentó, para asegurarnos que el espacio de probabilidad se encuentra bien definido, debemos disminuir esta probabilidad de los otros tokens para asegurarnos que la suma de las probabilidades siga siendo 1. Definamos el aumento de la probabilidad que tenemos respecto al token de fin de secuencia como \n",
    "\n",
    "$$a_p = \\hat{p}_s - p_s$$\n",
    "\n",
    "Entonces, sea $p_i$ la probabilidad de obtener el token $t_i$ en donde $t_i \\neq $ '\\</s\\>'. Definamos a $\\sigma$ como \n",
    "\n",
    "$$\\sigma = \\sum_{i=1}^{|V|} p_i$$\n",
    "\n",
    "en donde $|V|$ representa la cardinalidad del conjunto del vocabulario sin considerar al token de fin de secuencia. Notemos que $\\sigma = 1 - p_s$. Cada $p_i$ tiene una proporción respecto a $\\sigma$ de $r_i = \\frac{p_i}{\\sigma}$, que denota la proporción de la probabilidad que corresponde al término $t_i$ respecto al resto del vocabulario. Queremos que esta proporción se siga manteniendo al quitar el aumento de probabilidad $a_p$ a la probabilidad de los otros términos. Entonces, utilizando la siguiente regla de actualización\n",
    "\n",
    "$$\\hat{p}_i = p_i - r_i a_p$$\n",
    "\n",
    "y definiendo a $$\\hat{\\sigma} = \\sum_{i=1}^{|V|} \\hat{p}_i$$\n",
    "\n",
    "podemos ver que se cumple $$\\hat{r}_i = \\frac{\\hat{p}_i}{\\hat{\\sigma}} = r_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receives a probs matrix and the power r.\n",
    "def diminish(probs, r):\n",
    "    # calculate new probability\n",
    "    new_probs = np.zeros(probs.shape)\n",
    "    new_stop_prob = np.power(probs[:, -1], r)\n",
    "    # get improvement\n",
    "    improve = (new_stop_prob - probs[:, -1])\n",
    "    # get ratio of the other probabilities between them\n",
    "    c = np.sum(probs[:, :-1], axis=1)\n",
    "    rat = probs[:, :-1]/c[:, np.newaxis]\n",
    "    # update new probability\n",
    "    new_probs[:, -1] = new_stop_prob\n",
    "    new_probs[:, :-1] = probs[:, :-1] - rat * improve[:, np.newaxis]\n",
    "    \n",
    "    return new_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutar Oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def get_permutations(sentence):\n",
    "    return set(permutations(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7400828 0.6687403]\n",
      "[0.4400828 0.4687403]\n",
      "[1. 1.]\n",
      "[[0.14852411 0.11139308 0.7400828 ]\n",
      " [0.12422239 0.20703731 0.6687403 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = [[.4, .3, .3],[.3, .5, .2]]\n",
    "probs = np.array(probs, dtype=np.float128)\n",
    "\n",
    "new_probs = diminish(probs, 0.25)\n",
    "print(new_probs)\n",
    "np.sum(new_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".4/(.4 + .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285769248274"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".14852411/(0.14852411 + 0.11139308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m = TrigramModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1887"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.bi_voc['<s> hola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10001, 10002), 10001, 10002)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_m.probs.shape, len(trigram_m.bi_voc), len(trigram_m.voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.999999999993"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram_m.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('alv .', 891)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigram_m.bi_voc.items())[891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trigram_m.probs.shape[0]):\n",
    "    for j in range(trigram_m.probs.shape[1]):\n",
    "        if np.isnan(trigram_m.probs[i,j]):\n",
    "            print('nan at', i, ' ', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_m.train(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, unidocs = prepair_unigram(documents, 10000)\n",
    "bi_vocabulary, bidocs = prepair_bigram(documents, 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = build_unigram(unidocs, vocabulary)\n",
    "unigram_prob = unigram/np.sum(unigram[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = build_bigram(unidocs, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_prob = bigram[:-1]/unigram[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.000000000004"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_padded_docs = add_padding(unidocs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = build_trigram(bi_padded_docs, vocabulary, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_of_bigrams = build_unigram(bidocs, bi_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_prob = trigram[:-1]/unigram_of_bigrams[:-1, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True, False])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trigram, axis=1) == unigram_of_bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> <s>',\n",
       " '. </s>',\n",
       " '! !',\n",
       " '<s> @usuario',\n",
       " 'la verga',\n",
       " 'a la',\n",
       " '! </s>',\n",
       " 'de la',\n",
       " '@usuario @usuario',\n",
       " 'que no',\n",
       " 'que me',\n",
       " '<s> no',\n",
       " '? </s>',\n",
       " 'la madre',\n",
       " '<s> me',\n",
       " '<s> que',\n",
       " 'los putos',\n",
       " 'en la',\n",
       " '😂 😂',\n",
       " '… </s>',\n",
       " 'puta madre',\n",
       " 'en el',\n",
       " 'que se',\n",
       " '<s> ya',\n",
       " 'las putas',\n",
       " 'su madre',\n",
       " 'lo que',\n",
       " 'a su',\n",
       " 'verga .',\n",
       " 'que te',\n",
       " 'y no',\n",
       " 'voy a',\n",
       " '<s> si',\n",
       " 'no me',\n",
       " '<s> a',\n",
       " '😂 </s>',\n",
       " '? ?',\n",
       " 'madre .',\n",
       " 'no se',\n",
       " 'a los',\n",
       " '@usuario </s>',\n",
       " 'a mi',\n",
       " '<s> y',\n",
       " '<s> la',\n",
       " 'vale verga',\n",
       " 'para que',\n",
       " 'todos los',\n",
       " '<s> ¿',\n",
       " 'madre </s>',\n",
       " 'tu madre',\n",
       " 'de mi',\n",
       " 'a tu',\n",
       " 'hijos de',\n",
       " 'a las',\n",
       " 'y me',\n",
       " 'de su',\n",
       " 'de mierda',\n",
       " 'hasta la',\n",
       " 'de los',\n",
       " 'no es',\n",
       " 'es que',\n",
       " 'verga </s>',\n",
       " 'va a',\n",
       " 'hijo de',\n",
       " 'ya me',\n",
       " 'de tu',\n",
       " 'por qué',\n",
       " 'que putas',\n",
       " '<s> mi',\n",
       " 'que le',\n",
       " 'me vale',\n",
       " 'si no',\n",
       " '<s> el',\n",
       " 'que es',\n",
       " 'con el',\n",
       " 'no te',\n",
       " 'de verga',\n",
       " 'sus putas',\n",
       " 'ya no',\n",
       " '️ </s>',\n",
       " 'y que',\n",
       " 'con la',\n",
       " '... </s>',\n",
       " 'de las',\n",
       " '<s> en',\n",
       " 'la vida',\n",
       " 'es un',\n",
       " 'de que',\n",
       " '<s> cuando',\n",
       " '<s> yo',\n",
       " '<s> como',\n",
       " 'y se',\n",
       " 'ganas de',\n",
       " 'de putas',\n",
       " 'van a',\n",
       " 'en mi',\n",
       " 'tu puta',\n",
       " 'verga y',\n",
       " '<url> </s>',\n",
       " '<s> estoy',\n",
       " '\" .',\n",
       " 'mi madre',\n",
       " 'que el',\n",
       " 'por que',\n",
       " 'que ya',\n",
       " 'que la',\n",
       " 'a ver',\n",
       " 'a chingar',\n",
       " 'loca .',\n",
       " 'y la',\n",
       " 'chingas a',\n",
       " 'que los',\n",
       " 'vas a',\n",
       " 'y el',\n",
       " 'el que',\n",
       " '<s> lo',\n",
       " 'no hay',\n",
       " 'se me',\n",
       " '<s> putos',\n",
       " 'la chingada',\n",
       " '<s> se',\n",
       " 'como loca',\n",
       " 'madre !',\n",
       " 'su puta',\n",
       " 'no sé',\n",
       " 'no mames',\n",
       " 'hdp </s>',\n",
       " '<s> es',\n",
       " 'putos .',\n",
       " 'madre y',\n",
       " 'la gente',\n",
       " 'todo el',\n",
       " 'no lo',\n",
       " '😒 </s>',\n",
       " 'es la',\n",
       " 'todas las',\n",
       " 'madre de',\n",
       " 'de @usuario',\n",
       " 'y ya',\n",
       " 'a todos',\n",
       " '<s> \"',\n",
       " 'la puta',\n",
       " 'putas .',\n",
       " 'chingar a',\n",
       " '<s> por',\n",
       " 'qué putas',\n",
       " '. y',\n",
       " 'putas madres',\n",
       " 'y te',\n",
       " '. no',\n",
       " 'los que',\n",
       " 'verga en',\n",
       " '😡 </s>',\n",
       " 'si me',\n",
       " 'estoy loca',\n",
       " 'putos </s>',\n",
       " '<s> hoy',\n",
       " 'la loca',\n",
       " '\" </s>',\n",
       " 'mi verga',\n",
       " 'verga que',\n",
       " 'mamá luchona',\n",
       " 'se la',\n",
       " '<s> pinche',\n",
       " 'por la',\n",
       " 'es una',\n",
       " 'ya se',\n",
       " 'verga !',\n",
       " 'mil putas',\n",
       " 'y yo',\n",
       " 'que estoy',\n",
       " '❤ ️',\n",
       " '😡 😡',\n",
       " 'esos putos',\n",
       " '@usuario y',\n",
       " 'es el',\n",
       " 'de sus',\n",
       " 'de ser',\n",
       " '<s> los',\n",
       " 'poca madre',\n",
       " 'si te',\n",
       " 'las mujeres',\n",
       " '¿ qué',\n",
       " 'de un',\n",
       " 'me voy',\n",
       " ': v',\n",
       " '@usuario que',\n",
       " 'a una',\n",
       " 'mi vida',\n",
       " 'loca y',\n",
       " '@usuario no',\n",
       " 'es de',\n",
       " '🙄 </s>',\n",
       " 'con mi',\n",
       " 'que las',\n",
       " 'es lo',\n",
       " 'de putos',\n",
       " 'putos !',\n",
       " '<s> pero',\n",
       " 'y a',\n",
       " 'creo que',\n",
       " 'vale madre',\n",
       " 'estoy hasta',\n",
       " 'no tienen',\n",
       " 'y si',\n",
       " 'y de',\n",
       " 'pero no',\n",
       " 'sabes que',\n",
       " 'con sus',\n",
       " 'por eso',\n",
       " 'me gusta',\n",
       " '<s> las',\n",
       " 'me la',\n",
       " 'deja de',\n",
       " 'no puedo',\n",
       " '😭 </s>',\n",
       " '<s> qué',\n",
       " '😭 😭',\n",
       " 'me caga',\n",
       " ') </s>',\n",
       " 'de puta',\n",
       " 'por el',\n",
       " 'valer verga',\n",
       " 'que soy',\n",
       " 'chinguen a',\n",
       " 'loca </s>',\n",
       " 'toda la',\n",
       " 'en un',\n",
       " 'con los',\n",
       " '<s> de',\n",
       " 'me encanta',\n",
       " 'a un',\n",
       " 'que nos',\n",
       " '<s> verga',\n",
       " 'es mi',\n",
       " 'loca de',\n",
       " '<s> una',\n",
       " '<s> te',\n",
       " 'que a',\n",
       " 'putas y',\n",
       " 'no le',\n",
       " 'que lo',\n",
       " 'chinga tu',\n",
       " 'loca por',\n",
       " '<s> ¡',\n",
       " 'yo no',\n",
       " 'con un',\n",
       " 'v </s>',\n",
       " 'se ve',\n",
       " 'verga de',\n",
       " 'madre que',\n",
       " 'loca que',\n",
       " 'que les',\n",
       " 'y lo',\n",
       " 'y los',\n",
       " 'está de',\n",
       " 'jajaja </s>',\n",
       " 'con las',\n",
       " ':( </s>',\n",
       " '<s> pues',\n",
       " 'verga a',\n",
       " '. ¿',\n",
       " 'sus putos',\n",
       " 'y ahora',\n",
       " 'en tu',\n",
       " 'con su',\n",
       " 'verga con',\n",
       " '<s> ahora',\n",
       " 'hija de',\n",
       " 'eres un',\n",
       " 'la que',\n",
       " 'ir a',\n",
       " 'como si',\n",
       " 'una loca',\n",
       " 'pinche joto',\n",
       " 'la verdad',\n",
       " 'de una',\n",
       " 'a sus',\n",
       " 'no les',\n",
       " 'hdp !',\n",
       " '¿ por',\n",
       " 'a alguien',\n",
       " 'que si',\n",
       " 'todo lo',\n",
       " '<s> pinches',\n",
       " '! y',\n",
       " '. pero',\n",
       " 'la palabra',\n",
       " 'pinches putos',\n",
       " '<s> tengo',\n",
       " 'me siento',\n",
       " 'alguien que',\n",
       " 'no mamen',\n",
       " 'a hacer',\n",
       " '? !',\n",
       " 'el día',\n",
       " 'el culo',\n",
       " '<s> hay',\n",
       " 'a @usuario',\n",
       " 'en su',\n",
       " 'en las',\n",
       " 'a ser',\n",
       " 'un chingo',\n",
       " 'hay que',\n",
       " 'verga \"',\n",
       " 'mi casa',\n",
       " '<s> jajajaja',\n",
       " 'me estoy',\n",
       " 'que son',\n",
       " '@usuario es',\n",
       " 'verga ?',\n",
       " '<s> ojalá',\n",
       " 'me cagan',\n",
       " ': \"',\n",
       " 'por un',\n",
       " '<s> para',\n",
       " 'mi mamá',\n",
       " 'valen verga',\n",
       " 'ya estoy',\n",
       " 'se lo',\n",
       " 'te voy',\n",
       " 'como putas',\n",
       " 'porque no',\n",
       " 'que verga',\n",
       " 'te amo',\n",
       " 'al mundial',\n",
       " 'de lo',\n",
       " 'loca !',\n",
       " '<s> soy',\n",
       " '<s> con',\n",
       " 'se le',\n",
       " 'chingada madre',\n",
       " 'los de',\n",
       " 'cuenta que',\n",
       " 'a mí',\n",
       " 'hdp .',\n",
       " 'en los',\n",
       " '? no',\n",
       " 'no quiero',\n",
       " 'no ?',\n",
       " 'madre a',\n",
       " '\" y',\n",
       " 'el puto',\n",
       " '<s> jajaja',\n",
       " '🤔 </s>',\n",
       " '@usuario ya',\n",
       " 'si ya',\n",
       " 'personas que',\n",
       " 'mandar a',\n",
       " 'verga no',\n",
       " 'madre el',\n",
       " 'jajajaja </s>',\n",
       " 'ni madres',\n",
       " 'luchona y',\n",
       " 'dejen de',\n",
       " 'que yo',\n",
       " '. ya',\n",
       " 'con una',\n",
       " 'gente que',\n",
       " 'pero si',\n",
       " 'xd </s>',\n",
       " 'hasta que',\n",
       " '🇲 🇽',\n",
       " 'que sea',\n",
       " '. 😂',\n",
       " 'iba a',\n",
       " '. putos',\n",
       " 'y luego',\n",
       " 'el mundo',\n",
       " 'vez que',\n",
       " 'putos años',\n",
       " 'v word',\n",
       " 'otra vez',\n",
       " 'que mi',\n",
       " 'tengo que',\n",
       " '😍 😍',\n",
       " 'o sea',\n",
       " 'nada más',\n",
       " 'cosas que',\n",
       " '<s> un',\n",
       " 'se que',\n",
       " '. que',\n",
       " 'mierda </s>',\n",
       " 'pero me',\n",
       " 'está bien',\n",
       " '. ¡',\n",
       " '<s> puta',\n",
       " 'de \"',\n",
       " 'me da',\n",
       " 'verga pero',\n",
       " 'a quien',\n",
       " 'no son',\n",
       " 'quiero que',\n",
       " 'bola de',\n",
       " 'y en',\n",
       " 'es como',\n",
       " 'me hace',\n",
       " 'después de',\n",
       " '<s> todos',\n",
       " 'como cuando',\n",
       " '! ?',\n",
       " 'que eres',\n",
       " '<s> esta',\n",
       " 'lo mismo',\n",
       " 'putas ganas',\n",
       " 'más de',\n",
       " 'a veces',\n",
       " 'cada vez',\n",
       " '<s> le',\n",
       " 'para el',\n",
       " 'verga si',\n",
       " '<s> quiero',\n",
       " 'me tienen',\n",
       " 'o que',\n",
       " 'son los',\n",
       " '! ¡',\n",
       " 'no seas',\n",
       " 'y tu',\n",
       " 'putos periodistas',\n",
       " 'de mamar',\n",
       " 'la boca',\n",
       " 'a ti',\n",
       " '\" verga',\n",
       " 'una mujer',\n",
       " 'de verdad',\n",
       " '. me',\n",
       " 'no soy',\n",
       " 'pero que',\n",
       " 'como me',\n",
       " 'si se',\n",
       " 'y su',\n",
       " 'tan loca',\n",
       " 'bueno que',\n",
       " 'ver si',\n",
       " 'y al',\n",
       " 'la mañana',\n",
       " 'les vale',\n",
       " '🖕 🖕',\n",
       " 'nada .',\n",
       " 'putas que',\n",
       " 'chingue a',\n",
       " 'se va',\n",
       " 'cuando me',\n",
       " '<s> este',\n",
       " 'unas putas',\n",
       " 'son bien',\n",
       " 'verga la',\n",
       " 'la pinche',\n",
       " 'a que',\n",
       " 'que tengo',\n",
       " 'una verga',\n",
       " 'de loca',\n",
       " 'y sus',\n",
       " 'y las',\n",
       " 'del mundo',\n",
       " 'y por',\n",
       " '<s> marica',\n",
       " 'decir que',\n",
       " 'les gusta',\n",
       " 'eso no',\n",
       " 'al final',\n",
       " 'mejor que',\n",
       " 'madre no',\n",
       " '<s> o',\n",
       " 'me lo',\n",
       " '😤 </s>',\n",
       " '<s> oye',\n",
       " 'el joto',\n",
       " 'se te',\n",
       " 'que poca',\n",
       " 'que bueno',\n",
       " 'que un',\n",
       " 'a lo',\n",
       " 'valió verga',\n",
       " 'putas </s>',\n",
       " '<s> tu',\n",
       " 'por favor',\n",
       " 'las personas',\n",
       " 'valiendo verga',\n",
       " 'verga por',\n",
       " 'antes de',\n",
       " 'putas horas',\n",
       " 'no tiene',\n",
       " 'a este',\n",
       " 'soy la',\n",
       " '😍 </s>',\n",
       " '🙄 🙄',\n",
       " 'un día',\n",
       " 'putas !',\n",
       " 'la noche',\n",
       " 'en esta',\n",
       " 'verga es',\n",
       " 'ya lo',\n",
       " '. a',\n",
       " '<s> mira',\n",
       " 'la escuela',\n",
       " 'loca pero',\n",
       " 'madre por',\n",
       " 'de todos',\n",
       " 'a ese',\n",
       " 'me tiene',\n",
       " 'no puede',\n",
       " 'que digan',\n",
       " '<s> ay',\n",
       " 'putos y',\n",
       " 'y les',\n",
       " 'si es',\n",
       " '<s> solo',\n",
       " '... y',\n",
       " 'madre me',\n",
       " 'eres una',\n",
       " 'de mis',\n",
       " 'de madre',\n",
       " 'volviendo loca',\n",
       " 'pendeja .',\n",
       " 'el hdp',\n",
       " 'que en',\n",
       " 'pero ya',\n",
       " 'dicen que',\n",
       " 'los demás',\n",
       " 'unos putos',\n",
       " 'vamos a',\n",
       " 'a estar',\n",
       " 'vete a',\n",
       " 'en todos',\n",
       " 'cuando te',\n",
       " 'de estar',\n",
       " 'dice que',\n",
       " 'te vas',\n",
       " 'la misma',\n",
       " 'como el',\n",
       " 'de mil',\n",
       " 'y más',\n",
       " 'eso es',\n",
       " '@usuario si',\n",
       " '😠 </s>',\n",
       " '. \"',\n",
       " 'los hombres',\n",
       " 'agua loca',\n",
       " 'que tu',\n",
       " 'joto y',\n",
       " 'madre teresa',\n",
       " 'mierda .',\n",
       " 'así de',\n",
       " 'vayan a',\n",
       " 'madre @usuario',\n",
       " 'chingue su',\n",
       " '. la',\n",
       " '<s> está',\n",
       " 'y con',\n",
       " '<s> porque',\n",
       " 'ya que',\n",
       " '@usuario a',\n",
       " 'cara de',\n",
       " '? 🤔',\n",
       " 'vida loca',\n",
       " 'se ven',\n",
       " '<s> así',\n",
       " 'porque me',\n",
       " 'sé que',\n",
       " 'pinches putas',\n",
       " 'no la',\n",
       " 'son unos',\n",
       " 'ver a',\n",
       " 'madre con',\n",
       " 'por los',\n",
       " 'creen que',\n",
       " 'jaja </s>',\n",
       " 'a volver',\n",
       " 'así que',\n",
       " '\" me',\n",
       " 'la única',\n",
       " 'mamando con',\n",
       " 'tantita madre',\n",
       " 'a nadie',\n",
       " 'madre la',\n",
       " 'la cara',\n",
       " 'como que',\n",
       " '” </s>',\n",
       " 'vida .',\n",
       " 'día de',\n",
       " '<s> eres',\n",
       " 'yo soy',\n",
       " 'teresa de',\n",
       " 'de calcuta',\n",
       " 'putas se',\n",
       " 'tienen hasta',\n",
       " 'en serio',\n",
       " 'que chingue',\n",
       " 'por mi',\n",
       " '? ¿',\n",
       " '! no',\n",
       " 'vuelve loca',\n",
       " 'ya te',\n",
       " 'la calle',\n",
       " 'la mierda',\n",
       " '\" a',\n",
       " 'lo más',\n",
       " 'el marica',\n",
       " 'tienen madre',\n",
       " 'andar de',\n",
       " 'a mamar',\n",
       " 'esa madre',\n",
       " 'la selección',\n",
       " 'en una',\n",
       " 'marica </s>',\n",
       " 'o no',\n",
       " 'así como',\n",
       " 'que quiero',\n",
       " 'vaya a',\n",
       " 'joto </s>',\n",
       " 'se puede',\n",
       " 'yo si',\n",
       " 'mis putas',\n",
       " 'la neta',\n",
       " 'una vez',\n",
       " 'joto .',\n",
       " '#mexicandesmotherpalmundial </s>',\n",
       " 'caga que',\n",
       " 'hdp que',\n",
       " 'como la',\n",
       " 'un joto',\n",
       " 'que está',\n",
       " '? que',\n",
       " '. @usuario',\n",
       " '<s> ah',\n",
       " '🤣 </s>',\n",
       " 'y le',\n",
       " 'a esos',\n",
       " 'es para',\n",
       " '<s> ni',\n",
       " 'como se',\n",
       " 'tus putos',\n",
       " '<s> aquí',\n",
       " '😈 </s>',\n",
       " 'putos hondureños',\n",
       " 'con todo',\n",
       " 'madre \"',\n",
       " 'que de',\n",
       " 'putos todos',\n",
       " '. es',\n",
       " 'acabo de',\n",
       " '😩 😩',\n",
       " 'rica verga',\n",
       " 'me va',\n",
       " 'por ser',\n",
       " 'chingo de',\n",
       " 'a tus',\n",
       " 'no .',\n",
       " 'más putas',\n",
       " 'el mundial',\n",
       " \": '\",\n",
       " 'bien putas',\n",
       " 'dos putas',\n",
       " 'ni madre',\n",
       " '<s> “',\n",
       " 'madre ...',\n",
       " 'bien pinche',\n",
       " 'y así',\n",
       " 'lo único',\n",
       " '😆 😆',\n",
       " 'el amor',\n",
       " 'me dan',\n",
       " '\" no',\n",
       " 'importa lo',\n",
       " '¿ y',\n",
       " 'que todos',\n",
       " 'esta vida',\n",
       " '<s> les',\n",
       " 'que putos',\n",
       " 'loca con',\n",
       " '@usuario chinga',\n",
       " '<s> gracias',\n",
       " '<s> creo',\n",
       " 'a toda',\n",
       " 'y @usuario',\n",
       " '<s> putas',\n",
       " 'estoy volviendo',\n",
       " 'y como',\n",
       " 'que por',\n",
       " 'las que',\n",
       " 'en este',\n",
       " 'las cosas',\n",
       " 'no saben',\n",
       " '<s> quien',\n",
       " 'verga ...',\n",
       " 'no tengo',\n",
       " '@usuario te',\n",
       " '! @usuario',\n",
       " 'es muy',\n",
       " 'ya valió',\n",
       " 'lo de',\n",
       " 'para la',\n",
       " 'mi me',\n",
       " 'de joto',\n",
       " 'te gusta',\n",
       " 'toda tu',\n",
       " 'todos .',\n",
       " 'marica que',\n",
       " '” .',\n",
       " 'de ti',\n",
       " 'no a',\n",
       " 'de esos',\n",
       " 'no pueden',\n",
       " 'se les',\n",
       " 'váyanse a',\n",
       " 'a todas',\n",
       " '<s> sabes',\n",
       " 'mi amor',\n",
       " '💔 </s>',\n",
       " 'maricon de',\n",
       " 'partir la',\n",
       " 'más que',\n",
       " '. se',\n",
       " 'ya le',\n",
       " 'a méxico',\n",
       " 'te la',\n",
       " 'qué pedo',\n",
       " 'de a',\n",
       " 'ahora sí',\n",
       " 'putos gringos',\n",
       " 'yo me',\n",
       " 'son putas',\n",
       " 'pedo con',\n",
       " '<s> siempre',\n",
       " 'se mamó',\n",
       " 'si lo',\n",
       " '. en',\n",
       " 'buenos días',\n",
       " 'putos días',\n",
       " 'a todo',\n",
       " 'a ir',\n",
       " 'de no',\n",
       " 'madre en',\n",
       " 'son de',\n",
       " '. si',\n",
       " 'estoy bien',\n",
       " 'se sienten',\n",
       " 'lo peor',\n",
       " 'es mejor',\n",
       " 'me importa',\n",
       " 'puta que',\n",
       " 'yo le',\n",
       " 'te pones',\n",
       " 'me dio',\n",
       " 'igual que',\n",
       " 'andan de',\n",
       " 'soy joto',\n",
       " 'bien que',\n",
       " 'qué no',\n",
       " 'pendejo .',\n",
       " 'q no',\n",
       " 'y un',\n",
       " 'uno de',\n",
       " 'fotos de',\n",
       " 'el corazón',\n",
       " 'para ver',\n",
       " 'esto es',\n",
       " 'verga todo',\n",
       " 'que rico',\n",
       " '😩 </s>',\n",
       " ': </s>',\n",
       " 'y todos',\n",
       " 'vales verga',\n",
       " '😌 </s>',\n",
       " 'se los',\n",
       " 'sabe que',\n",
       " 'sé si',\n",
       " 'pero a',\n",
       " 'un poco',\n",
       " 'no entiendo',\n",
       " 'con que',\n",
       " 'son las',\n",
       " 'joto de',\n",
       " 'de ese',\n",
       " 'no sean',\n",
       " 'me lleva',\n",
       " 'lleva la',\n",
       " '🤦🏻\\u200d♀ ️',\n",
       " 'te va',\n",
       " 'el pinche',\n",
       " 'en todo',\n",
       " 'valga verga',\n",
       " 'ver que',\n",
       " '. por',\n",
       " 'madre al',\n",
       " 'ahora si',\n",
       " 'le gusta',\n",
       " 'y cuando',\n",
       " 'pinche madre',\n",
       " 'madre los',\n",
       " '| </s>',\n",
       " '. hdp',\n",
       " 'es por',\n",
       " 'a dormir',\n",
       " 'se pone',\n",
       " 'ya ni',\n",
       " 'la pela',\n",
       " '😔 </s>',\n",
       " '... no',\n",
       " 'de amor',\n",
       " 'putas de',\n",
       " 'como le',\n",
       " 'el pendejo',\n",
       " 'pendejo que',\n",
       " 'no quiere',\n",
       " 'que pedo',\n",
       " 'y chingas',\n",
       " 'para no',\n",
       " 'las nalgas',\n",
       " '. 😒',\n",
       " 'el tiempo',\n",
       " 'a trabajar',\n",
       " 'como una',\n",
       " '! que',\n",
       " 'que siempre',\n",
       " 'de todo',\n",
       " 'joto !',\n",
       " 'marica de',\n",
       " '\" de',\n",
       " 'estos putos',\n",
       " 'tiene hasta',\n",
       " 'de esas',\n",
       " 'mi novio',\n",
       " 'espero que',\n",
       " 'el mismo',\n",
       " 'putos de',\n",
       " '😢 </s>',\n",
       " 'te quiero',\n",
       " 'valgo verga',\n",
       " 'marica .',\n",
       " 'me valen',\n",
       " 'el dinero',\n",
       " 'y putas',\n",
       " '😤 😤',\n",
       " 'juro que',\n",
       " '<s> neta',\n",
       " 'de todas',\n",
       " 'que era',\n",
       " 'porque putas',\n",
       " 'tienen que',\n",
       " 'madre mía',\n",
       " 'qué verga',\n",
       " 'mandó a',\n",
       " 'estoy en',\n",
       " 'hasta el',\n",
       " 'un buen',\n",
       " 'verga lo',\n",
       " 'me tienes',\n",
       " 'por no',\n",
       " 'mentar la',\n",
       " '¿ cómo',\n",
       " 'por putos',\n",
       " 'ver el',\n",
       " 'tener que',\n",
       " 'una noche',\n",
       " 'de esta',\n",
       " 'mí me',\n",
       " 'putas me',\n",
       " '<s> eso',\n",
       " 'la vez',\n",
       " 'que ver',\n",
       " 'marica !',\n",
       " '<s> también',\n",
       " 'el partido',\n",
       " 'un pinche',\n",
       " '<s> -',\n",
       " '🤷🏻\\u200d♀ ️',\n",
       " 'es más',\n",
       " 'un hombre',\n",
       " 'y mi',\n",
       " '<s> todo',\n",
       " '. #putas',\n",
       " '️ ❤',\n",
       " '¿ que',\n",
       " 'pensar que',\n",
       " 'madre luchona',\n",
       " 'su verga',\n",
       " 'le pasa',\n",
       " 'a valer',\n",
       " '#masterchefmx </s>',\n",
       " '@usuario por',\n",
       " 'me cago',\n",
       " 'en sus',\n",
       " 'dijo que',\n",
       " 'q se',\n",
       " 'maricón .',\n",
       " '<s> chingas',\n",
       " 'madres </s>',\n",
       " 'siempre me',\n",
       " 'o qué',\n",
       " 'una madre',\n",
       " 'en twitter',\n",
       " 'mando a',\n",
       " 'amor .',\n",
       " 'que hacer',\n",
       " 'putas le',\n",
       " 'putas no',\n",
       " '<s> voy',\n",
       " 'que su',\n",
       " 'esta de',\n",
       " 'del mundial',\n",
       " 'eres la',\n",
       " 'loca me',\n",
       " 'los huevos',\n",
       " 'verga el',\n",
       " 'como siempre',\n",
       " '. pinches',\n",
       " 'quién putas',\n",
       " 'vas y',\n",
       " 'a decir',\n",
       " '.. </s>',\n",
       " '<s> cómo',\n",
       " 'que este',\n",
       " 'esta madre',\n",
       " 'me pongo',\n",
       " 'tus putas',\n",
       " 'me vuelve',\n",
       " '<s> al',\n",
       " 'putas ?',\n",
       " 'dos putos',\n",
       " '🖕🏻 🖕🏻',\n",
       " 'una persona',\n",
       " '? 😂',\n",
       " 'chingada .',\n",
       " 'te digo',\n",
       " 'digan esos',\n",
       " 'con ganas',\n",
       " 'si son',\n",
       " 'hdp de',\n",
       " 'al menos',\n",
       " 'clase de',\n",
       " 'y eso',\n",
       " 'no sea',\n",
       " 'como tu',\n",
       " '<s> desde',\n",
       " 'me está',\n",
       " 'no mamar',\n",
       " 'no tener',\n",
       " 'alv .',\n",
       " 'el nombre',\n",
       " 'qué me',\n",
       " 'mejor amigo',\n",
       " 'putas \"',\n",
       " 'con tu',\n",
       " 'le va',\n",
       " 'no sabe',\n",
       " 'digo que',\n",
       " '! 😂',\n",
       " '<s> su',\n",
       " '¿ no',\n",
       " '☹ ️',\n",
       " 'para los',\n",
       " 'que con',\n",
       " 'madre pinche',\n",
       " 'verga para',\n",
       " 'tu novio',\n",
       " 'todos putos',\n",
       " 'una foto',\n",
       " 'por su',\n",
       " '🎶 🎶',\n",
       " 'alv </s>',\n",
       " '@usuario se',\n",
       " '2 putas',\n",
       " 'se vaya',\n",
       " 'le vale',\n",
       " 'la tarea',\n",
       " 'putos no',\n",
       " 'con esa',\n",
       " 'como para',\n",
       " 'una semana',\n",
       " 'tiene que',\n",
       " 'lo bueno',\n",
       " 'es tan',\n",
       " 'a partir',\n",
       " '🍆 💦',\n",
       " 'mejor .',\n",
       " 'por todos',\n",
       " 'madre naturaleza',\n",
       " 'y después',\n",
       " 'luchona que',\n",
       " 'par de',\n",
       " '<s> q',\n",
       " 'la de',\n",
       " 'toda su',\n",
       " 'loca ...',\n",
       " 'loca 😂',\n",
       " 'de marica',\n",
       " 'ser una',\n",
       " 'en mis',\n",
       " 'ir al',\n",
       " 'putas en',\n",
       " '@usuario me',\n",
       " 'pendejo </s>',\n",
       " 'y todavía',\n",
       " 'mi novia',\n",
       " 'no he',\n",
       " 'ya quiero',\n",
       " 'gracias a',\n",
       " 'mundo .',\n",
       " ...]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bi_vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bigram[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_vocabulary['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02958152958152958"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob[padded_vocabulary['<s>'], padded_vocabulary['<unk>']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
