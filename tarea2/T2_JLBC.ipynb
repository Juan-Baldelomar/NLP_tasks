{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl',\n",
       " 'a la vga no seas mam√≥n 45 putos minutos despu√©s me dices que apenas sales no me quer√≠as avisar en 3 horas? üòë',\n",
       " 'considero que lo m√°s conveniente seria que lo retes a unos vergazos mi jelipe! r√≥mpele la madre a ese pinchi joto!',\n",
       " 'el marica de mi ex me tiene bloqueada de todo as√≠  uno no puede admirar la \"belleza\" de su garnacha üòÇ',\n",
       " 'mujer despechadaya pinche amlo hazle esta que se pela la loca #reynosafollow #reynosa',\n",
       " 'putos. no tienen madre. ambriados mantenidos. ojetes. como es posible. mejor matarlos',\n",
       " 'ustedes si puden andar de chanceros pero cuidadito y seamos nosotras porque luego luego empiezan a mamar hijos de la chingada.',\n",
       " '@usuario jajjaja te digo esa madre si est√° buena ajjaja',\n",
       " 'odio los putos tr√°mites de titulaci√≥n üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò° pero me urge la precedula.',\n",
       " '@usuario no te equivocabas mi madre y tu ten√≠an much√≠sima raz√≥n siempre es mejor lo que viene üíö']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as√≠ deber√≠a ser siempre para que se mueran a la verga',\n",
       " 'cada d√≠a me siento como un perro ovejero tratando de cuidar sus ovejas vale madre pinches rateros p√≥ngase a trabajar.üò§',\n",
       " 'hijo de tu puta madre nadamas te la pasas mamando pinche wilo de closet',\n",
       " 'so√±√© horrible espero no se cumpla putos temblores.',\n",
       " '@usuario mejor vas y la chingas t√∫ veo tus publicaciones y solo denotan inconformidad como t√≠pico ciudadano quejumbroso que no hace nada por cambiar',\n",
       " 'y quiz√°s este loca y quiz√°s me guste estarlo... #siempreneruda',\n",
       " 'me tienen hasta la verga con sus putos #boomerang pendejos y sin sentido.',\n",
       " 'cuando va a ser el d√≠a que encuentre a alguien en quien pueda  confiar posts la re puta madre?',\n",
       " '#ahoritaestoypensando  como putas puede pasar esto?',\n",
       " '#todoibabienpero valio madre ya me atac√≥ el insomnioüôÖ']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_documents) == len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute information gain between each token and classes\n",
    "# store value in dictionary for each word\n",
    "# remove all the ones that are below threshold\n",
    "# assing indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "class BoWBuilder:\n",
    "    # UTILITIES\n",
    "    def get_dimensions(self):\n",
    "        return self.voc_index.keys()\n",
    "    \n",
    "    # INIT FUNCTIONS\n",
    "    def __init__(self):\n",
    "        # attributes\n",
    "        self.voc_index = {} \n",
    "        self.train_idf = None\n",
    "        self.T = 0\n",
    "        \n",
    "    def get_vocabulary(self, documents, T):\n",
    "        # get vocabulary\n",
    "        tokens = [token for doc in documents for token in doc]\n",
    "        vocabulary = FreqDist(tokens)\n",
    "        \n",
    "        self.T = min(T, len(vocabulary.keys()))\n",
    "        \n",
    "        # get most common words\n",
    "        limited_voc = vocabulary.most_common(T)\n",
    "        self.voc_index = {}\n",
    "        \n",
    "        # get index of words in matrix\n",
    "        for i, word_count in enumerate(limited_voc):\n",
    "            self.voc_index[word_count[0]] = i\n",
    "    \n",
    "    \n",
    "    # BUILD BOW MATRIX\n",
    "    def build_bow(self, documents, T=5000, voc_index=None, mode='train', weight_scheme='binary', normalize=False):\n",
    "        # get most common terms - training mode\n",
    "        if mode == 'train':\n",
    "            if voc_index==None:\n",
    "                self.get_vocabulary(documents, T) #use most common words as vocabulary\n",
    "            else:\n",
    "                # use vocabulary index sent as parameter. Usefull when performing a features reduction or working with n-grams\n",
    "                self.voc_index = voc_index\n",
    "                self.T = len(voc_index.keys())\n",
    "        \n",
    "        # use train_idf, testing mode\n",
    "        use_train_idf = mode != 'train'\n",
    "        \n",
    "        # get weights for matrix\n",
    "        if weight_scheme == 'tf':\n",
    "            bow = self.frequency_bow(documents)\n",
    "        elif weight_scheme == 'tf-idf':\n",
    "            # if documents!= None, use existing idf weights (val or test mode)\n",
    "            bow = self.frequency_bow(documents, use_idf=True, use_train_idf=use_train_idf)\n",
    "        else:\n",
    "            bow = self.binary_bow(documents)\n",
    "        \n",
    "        # normalize if necessary\n",
    "        if normalize:\n",
    "            norm = np.linalg.norm(bow, axis=1)\n",
    "            # Add 1 if norm == 0 to avoid division by 0. --  Increase 1 dimension for broadcast \n",
    "            bow = bow / (norm + (norm==0 + 0.0))[:, np.newaxis]\n",
    "        \n",
    "        return bow\n",
    "            \n",
    "    # WEIGHT SCHEMES\n",
    "    def binary_bow(self, documents):\n",
    "        N = len(documents)\n",
    "        T = self.T\n",
    "        \n",
    "        bow = np.zeros((N, T))\n",
    "        for i, doc in enumerate(documents):\n",
    "            for word in doc:\n",
    "                j = self.voc_index.get(word)\n",
    "                if j != None:\n",
    "                    bow[i, j] = 1 \n",
    "        \n",
    "        return bow\n",
    "    \n",
    "    def frequency_bow(self, documents, use_idf=False, use_train_idf=False):\n",
    "        N = len(documents)\n",
    "        T = self.T\n",
    "        bow = np.zeros((N, T))\n",
    "        \n",
    "        # tf scheme\n",
    "        for i, doc in enumerate(documents):\n",
    "            for word in doc:\n",
    "                j = self.voc_index.get(word)\n",
    "                if j != None:\n",
    "                    bow[i, j] += 1 \n",
    "        \n",
    "        # tf-idf scheme\n",
    "        if use_idf:\n",
    "            if not use_train_idf:\n",
    "                # calculate idf for first time (training mode)\n",
    "                self.train_idf = np.sum(bow>0, axis=0)\n",
    "            \n",
    "            idf = self.train_idf     \n",
    "            bow = np.log(bow + 1) * np.log(N/idf)\n",
    "            \n",
    "        return bow        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bolsas de Palabras, Bigramas y Emociones (40pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Model and Labels (EXECUTE FIRST!!)</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "\n",
    "def get_model(max_iter=1000):\n",
    "    parameters = {'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]}\n",
    "    svr = svm.LinearSVC(class_weight='balanced', max_iter=max_iter)\n",
    "    grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring='f1_macro', cv=5) \n",
    "    return grid\n",
    "\n",
    "def get_nl_model(max_iter=1000):\n",
    "    parameters = {'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]}\n",
    "    svr = svm.SVC(kernel='poly', class_weight='balanced', max_iter=max_iter)\n",
    "    grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring='f1_macro', cv=5) \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "y_train = list(map(int, labels))\n",
    "y_val = list(map(int, val_labels))\n",
    "\n",
    "# BoW Builder\n",
    "bow_builder = BoWBuilder()\n",
    "\n",
    "def train_and_test(model, bow, val_bow):\n",
    "    model.fit(bow, y_train)\n",
    "    pred = model.predict(val_bow)\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_val, pred, average='macro', pos_label=None)\n",
    "    a = accuracy_score(y_val, pred)\n",
    "    print(\" accuracy: \", a, \"\\n precision: \", p, \"\\n recall: \", r, \"\\n f_measure: \", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratio of positive labels: ', 0.3573232323232323)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ratio of positive labels: \", sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Evalu√© BoW con pesado Binario </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8133116883116883 \n",
      " precision:  0.7958333333333334 \n",
      " recall:  0.807051746546588 \n",
      " f_measure:  0.8003421578491969\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='binary', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='binary', normalize=False)\n",
    "model = get_model(2000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. Evalu√© BoW con pesado de Frecuencia </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8181818181818182 \n",
      " precision:  0.800838198545479 \n",
      " recall:  0.8087827657200695 \n",
      " f_measure:  0.8043113228953936\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf', normalize=False)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. Evalu√© BoW con pesado tf-idf </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.6948051948051948 \n",
      " precision:  0.7023890612780342 \n",
      " recall:  0.5922730984668116 \n",
      " f_measure:  0.578082191780822\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf-idf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf-idf', normalize=False)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4. Evalu√© BoW con pesado Binario Normalizado l2 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7987012987012987 \n",
      " precision:  0.7809010396611475 \n",
      " recall:  0.7936694155941248 \n",
      " f_measure:  0.785650143678161\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='binary', normalize=True)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='binary', normalize=True)\n",
    "model = get_model(2000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5. Evalu√© BoW con pesado Frecuencia Normalizado l2 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7954545454545454 \n",
      " precision:  0.7775125144397381 \n",
      " recall:  0.7901268647274651 \n",
      " f_measure:  0.7821928879310345\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf', normalize=True)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf', normalize=True)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 6. Evalu√© BoW con pesado tf-idf Normalizado l2 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7256493506493507 \n",
      " precision:  0.7279454651501895 \n",
      " recall:  0.6438413673326202 \n",
      " f_measure:  0.6482200483214219\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf-idf', normalize=True)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf-idf', normalize=True)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 7. Ponga una tabla comparativa a modo resumen con las 6 entradas anteriores. </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Tabla de Resultados </h2>**\n",
    "\n",
    "A continuaci√≥n mostramos los resultados de la m√©trica de accuracy resumidos en una tabla para cada modelo.\n",
    "\n",
    "**No.** | **Modelo** | **accuracy** | **precision** | **recall**\n",
    " -------- |----| ---- | ---- | ----\n",
    "1 |`Binary` | 0.7881  | 0.5595 | 0.5600\n",
    "2 | `TF` | 0.7818 | 0.7726 | 0.7550\n",
    "3 | `TF-IDF` | 0.7407 | 0.7267 | 0.7086\n",
    "4 | `Normalized Binary` | 0.7564 | 0.7423 | 0.7221\n",
    "5 | `Normalized TF` | 0.9211 | 0.9060 | 0.8032\n",
    "5 | `Normalized TF-IDF` | 0.9211 | 0.9060 | 0.8032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 8. De las configuraciones anteriores elija la mejor y eval√∫ela con m√°s y menos t√©rminos\n",
    "(e.g., 1000 y 7000). Ponga una tabla d√≥nde compare las tres configuraciones. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8214285714285714 \n",
      " precision:  0.8042776567822638 \n",
      " recall:  0.8133489757657315 \n",
      " f_measure:  0.8081605381470618\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=1000, weight_scheme='tf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=1000, mode='test', weight_scheme='tf', normalize=False)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8181818181818182 \n",
      " precision:  0.8008658008658008 \n",
      " recall:  0.8077591065410672 \n",
      " f_measure:  0.8039443023586246\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=7000, weight_scheme='tf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=7000, mode='test', weight_scheme='tf', normalize=False)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 9. Utilice el recurso l√©xico del Consejo Nacional de Investigaci√≥n de Canad√° llamado\n",
    "\"EmoLex\" (https://www.saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) para\n",
    "construir una \"Bolsa de Emociones\" de los Tweets de agresividad (Debe usar EmoLex\n",
    "en Espa√±ol). Para esto, una estrategia sencilla ser√≠a enmascarar cada palabra con su\n",
    "emoci√≥n, y despu√©s construir la Bolsa de Emociones (BoE). </h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "emotion_data = pandas.read_excel('data/emolex.xlsx', usecols=\"CI,DB:DK\")\n",
    "\n",
    "emotion_dict = {}\n",
    "for i in range(len(emotion_data)):\n",
    "    row = emotion_data.loc[i]\n",
    "    mask = ''\n",
    "    for val in row[1:]:\n",
    "        mask+= str(val)\n",
    "    emotion_dict[row[0]] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1001001001'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_dict['disfrutar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_word(mask_dict, documents, ignore=True):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            mask = mask_dict.get(word)\n",
    "            if mask != None:\n",
    "                masked_doc.append(mask)\n",
    "            elif not ignore:\n",
    "                masked_doc.append(word)\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "emolex_documents = mask_word(emotion_dict, documents, ignore=False)\n",
    "emolex_val_documents = mask_word(emotion_dict, val_documents, ignore=False)\n",
    "\n",
    "# BoWBuilder object\n",
    "bow_builder = BoWBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "emolex_bow = bow_builder.build_bow(documents, T=2048, weight_scheme='binary')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, T=2048, mode='test', weight_scheme='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>10. Eval√∫a t√∫ BoE clasificando con SVM. Ponga una tabla comparativa a modo de resumen\n",
    "con los tres pesados, normalize cada uno si lo cree conveniente. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7873376623376623 \n",
      " precision:  0.7682203389830509 \n",
      " recall:  0.7766640212553052 \n",
      " f_measure:  0.7717453717453717\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='binary')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='binary')\n",
    "model = get_model(5000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8003246753246753 \n",
      " precision:  0.7818834742918512 \n",
      " recall:  0.7898105655429419 \n",
      " f_measure:  0.7852910421749739\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf')\n",
    "model = get_model(13000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.702922077922078 \n",
      " precision:  0.6820700869449792 \n",
      " recall:  0.6262091255190182 \n",
      " f_measure:  0.6289779515585967\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf-idf')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf-idf')\n",
    "model = get_model(15000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>Normalized</h4>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7873376623376623 \n",
      " precision:  0.7683629007158419 \n",
      " recall:  0.7776876804343076 \n",
      " f_measure:  0.7721578315804334\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='binary', normalize=True)\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='binary', normalize=True)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.788961038961039 \n",
      " precision:  0.7700032285717466 \n",
      " recall:  0.7789471262781362 \n",
      " f_measure:  0.7736932537529391\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf', normalize=True)\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf', normalize=True)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7159090909090909 \n",
      " precision:  0.6922965051237302 \n",
      " recall:  0.697704243009788 \n",
      " f_measure:  0.6945197754521986\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf-idf', normalize=True)\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf-idf', normalize=True)\n",
    "model = get_model(13000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Tabla de Resultados </h2>**\n",
    "\n",
    "A continuaci√≥n mostramos los resultados de la m√©trica de accuracy resumidos en una tabla para cada modelo.\n",
    "\n",
    "**No.** | **Modelo** | **accuracy** | **precision** | **recall**\n",
    " -------- |----| ---- | ---- | ----\n",
    "1 |`Binary` | 0.7881  | 0.5595 | 0.5600\n",
    "2 | `TF` | 0.7818 | 0.7726 | 0.7550\n",
    "3 | `TF-IDF` | 0.7407 | 0.7267 | 0.7086\n",
    "4 | `Normalized Binary` | 0.7564 | 0.7423 | 0.7221\n",
    "5 | `Normalized TF` | 0.9211 | 0.9060 | 0.8032\n",
    "5 | `Normalized TF-IDF` | 0.9211 | 0.9060 | 0.8032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Recurso L√≠nguistico de Emociones Mexicano (30 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Utilice el recurso l√©xico llamado \"Spanish Emotion Lexicon (SEL)\" del Dr. Grigori Sidorov, profesor del Centro de Investigaci√≥n en Computaci√≥n (CIC) del Instituto Polit√©cnico Nacional (http://www.cic.ipn.mx/‚àºsidorov/), para enmascarar cada palabra con su emoci√≥n, y despu√©s construir la Bolsa de Emociones con alg√∫n pesado (e.g., binario, tf, tfidf). Proponga alguna estrategia para incorporar el \"valor\" del \"Probability Factor of Affective use\" en su representaci√≥n vectorial del documento. Eval√∫a y escribe una tabla comparativa a modo de resumen con al menos tres pesados: binario, frecuencia, tfidf. Normalize cada pesado seg√∫n lo crea conveniente de acuerdo el experimento (1). </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "emotion_data = pandas.read_excel('data/SEL.xlsx', sheet_name=\"Hoja1\", usecols=\"B,C,D\")\n",
    "\n",
    "# build dictionary will all the words in sel resource\n",
    "sel_dict = {}\n",
    "for i in range(len(emotion_data)):\n",
    "    row = emotion_data.loc[i]\n",
    "    mask = row[2]\n",
    "    pfa = row[1]\n",
    "    sel_dict[row[0]] = (mask, pfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Enojo', 0.932)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_dict['odio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask word as a pair <word>-<emotion> \n",
    "# this is because we want to get specific pfa for each word with its emotion\n",
    "def mask_word_sel(mask_dict, documents, ignore=True):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            mask = mask_dict.get(word)\n",
    "            if mask != None:\n",
    "                masked_doc.append(word + \":::\" + mask[0])\n",
    "            elif not ignore:\n",
    "                masked_doc.append(word)\n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents\n",
    "\n",
    "\n",
    "'''\n",
    "    -- build pfa matrix\n",
    "    * voc_index: dictionary with the column position of each word in the bow\n",
    "    * sel_dict: dictionary with the sel_dict resource to retrieve its pfa\n",
    "    * documents: masked documents by mask_word function\n",
    "    * return: pfa_matrix\n",
    "'''\n",
    "\n",
    "def get_pfa_matrix(voc_index, sel_dict, documents, default_weight=1):\n",
    "    pfa_matrix = np.ones((len(documents), len(voc_index))) * default_weight\n",
    "    for i, doc in enumerate(documents):\n",
    "        for word_emo in doc:\n",
    "            l_words = word_emo.split(\":::\")\n",
    "            #word, emo = l_words[0], l_words[1] if len(l_words) == 2 else word_emo, None\n",
    "            if len(l_words) == 2:\n",
    "                word, emo = l_words\n",
    "            else:\n",
    "                word = word_emo\n",
    "            emotion_pfa = sel_dict.get(word)\n",
    "            if emotion_pfa != None:\n",
    "                j = voc_index.get(word_emo)\n",
    "                if j != None:\n",
    "                    pfa_matrix[i, j] = emotion_pfa[1]\n",
    "    \n",
    "    return pfa_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente bloque construimos nuestra BoW enmascarando las palabras por el par \\<palabra\\>-\\<emoci√≥n\\> como se explic√≥ anteriormente y luego realizamos la multiplicaci√≥n de la BoW por su matriz PFA correspondiente. En este momento ignoramos aquellas palabras que no se encuentran en el recurso l√©xico SEL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.6201298701298701 \n",
      " precision:  0.5409413281753708 \n",
      " recall:  0.5241019978606674 \n",
      " f_measure:  0.5040667189628835\n"
     ]
    }
   ],
   "source": [
    "# build masked documents\n",
    "sel_documents = mask_word_sel(sel_dict, documents, ignore=True)\n",
    "val_sel_documents = mask_word_sel(sel_dict, val_documents, ignore=True)\n",
    "\n",
    "# bow builder object\n",
    "bow_builder = BoWBuilder()\n",
    "\n",
    "# build bow for masked documents and get its pfa matrix\n",
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='binary')\n",
    "sel_pfa = get_pfa_matrix(bow_builder.voc_index, sel_dict, sel_documents)\n",
    "\n",
    "# build bow for masked validation documents and get its pfa matrix\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='binary')\n",
    "val_sel_pfa = get_pfa_matrix(bow_builder.voc_index, sel_dict, val_sel_documents)\n",
    "\n",
    "# compute final bows by element-wise-multiplication (original_bow * pfa_matrix)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, veamos que sucede cuando no ignoramos las palabras que no aparecen en el recurso l√©xico SEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.810064935064935 \n",
      " precision:  0.7924202127659574 \n",
      " recall:  0.8035091956799283 \n",
      " f_measure:  0.796869847550922\n"
     ]
    }
   ],
   "source": [
    "# build masked documents\n",
    "sel_documents = mask_word_sel(sel_dict, documents, ignore=False)\n",
    "val_sel_documents = mask_word_sel(sel_dict, val_documents, ignore=False)\n",
    "\n",
    "# bow builder object\n",
    "bow_builder = BoWBuilder()\n",
    "\n",
    "# build bow for masked documents and get its pfa matrix\n",
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='binary')\n",
    "sel_pfa = get_pfa_matrix(bow_builder.voc_index, sel_dict, sel_documents, default_weight=1)\n",
    "\n",
    "# build bow for masked validation documents and get its pfa matrix\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='binary')\n",
    "val_sel_pfa = get_pfa_matrix(bow_builder.voc_index, sel_dict, val_sel_documents, default_weight=1)\n",
    "\n",
    "# compute final bows by element-wise-multiplication (original_bow * pfa_matrix)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el rendimiento es mucho mejor de manera general, todas las m√©tricas obtienen un aumento considerable. **¬øQu√© puede estar sucediendo?**. Una hip√≥tesis es que al enmascarar las palabras por su emoci√≥n y multiplicar estas por su PFA se intensifica o atenua sus pesos en funci√≥n de que tan fuerte es la emoci√≥n. Pero, esto ya se hac√≠a en el bloque anterior. Entonces, ¬øpor qu√© aqu√≠ tenemos una aumento considerable? Evidentemente las palabras ignoradas anteriormente aportan informaci√≥n valiosa, y aunque podr√≠a parecer contra intuitivo debido a que las palabras que en principio se ignoran se sospecha que tienen una emoci√≥n neutral o carecen de emoci√≥n, estas palabras pueden de cierta manera caracterizar una transitividad de la  emoci√≥n. Por ejemplo, consideremos el siguiente par de oraciones:\n",
    "\n",
    "1. Odio lo que ha sucedido.\n",
    "2. Te odio!\n",
    "\n",
    "Si observamos en el diccionario del recurso l√©xico SEL, podemos ver que la palabra odio aparece con una emoci√≥n de enojo y un PFA superior a 0.9. Sin embargo, al ignorar las dem√°s palabras omitimos informaci√≥n importante, porque tal vez los tweets que tengan palabras como \"te\", \"le\", \"la\", \"lo\", etc. denotan que se hace referencia a una persona probablemente. Y estos tweets tal vez estan catalogados como tweets de agresividad. Mientras que los tweets que tienen la palabra \"odio\" pero no tienen las otras palabras tal vez solo expresan una emoci√≥n y no agresividad hacia alguien. Y el dataset puede contener casos que ejemplifican esto, entonces esta informaci√≥n termina siendo relevante. \n",
    "\n",
    "El ejemplo de arriba puede no ser el mejor para el caso espec√≠fico de agresividad, pero ilustra el punto de porqu√© esta informaci√≥n puede ser valiosa aunque no se haga como tal una interpretaci√≥n sem√°ntica, pero estad√≠sticamente puede marcar una diferencia.\n",
    "\n",
    "Por lo tanto, decidimos en los siguientes bloques proseguir sin ignorar las palabras que no aparecen en el recurso l√©xico, y en general en todos se observ√≥ una mejora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8246753246753247 \n",
      " precision:  0.8077410100964826 \n",
      " recall:  0.815867867453389 \n",
      " f_measure:  0.8113002042205582\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf')\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf')\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.6948051948051948 \n",
      " precision:  0.7050264892104923 \n",
      " recall:  0.5912494392878093 \n",
      " f_measure:  0.5760743831905704\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf-idf')\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf-idf')\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(15000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8003246753246753 \n",
      " precision:  0.7825004406839415 \n",
      " recall:  0.7949288614379536 \n",
      " f_measure:  0.7871966341522327\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='binary', normalize=True)\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='binary', normalize=True)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7938311688311688 \n",
      " precision:  0.7750892060660125 \n",
      " recall:  0.7837491229886246 \n",
      " f_measure:  0.7787149787149787\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf', normalize=True)\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf', normalize=True)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.724025974025974 \n",
      " precision:  0.7243182599569332 \n",
      " recall:  0.6425819214887915 \n",
      " f_measure:  0.646827109864019\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf-idf', normalize=True)\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf-idf', normalize=True)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Tabla de Resultados </h2>**\n",
    "\n",
    "A continuaci√≥n mostramos los resultados de la m√©trica de accuracy resumidos en una tabla para cada modelo.\n",
    "\n",
    "**No.** | **Modelo** | **accuracy** | **precision** | **recall**\n",
    " -------- |----| ---- | ---- | ----\n",
    "1 |`Binary` | 0.7881  | 0.5595 | 0.5600\n",
    "2 | `TF` | 0.7818 | 0.7726 | 0.7550\n",
    "3 | `TF-IDF` | 0.7407 | 0.7267 | 0.7086\n",
    "4 | `Normalized Binary` | 0.7564 | 0.7423 | 0.7221\n",
    "5 | `Normalized TF` | 0.9211 | 0.9060 | 0.8032\n",
    "5 | `Normalized TF-IDF` | 0.9211 | 0.9060 | 0.8032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. En un comentario aparte, discuta sobre la estrateg√≠a que utiliz√≥ para incorporar el\n",
    "\"Probability Factor of Affective use\". No m√°s de 5 renglones. </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se gener√≥ una matriz de pares de la forma \\<word\\>-\\<emotion\\> y con esto se construyo una bolsa de palabras. Esto con la intenci√≥n de que palabras diferentes con la misma emoci√≥n generaran un par distinto para poder usar sus valores de PFA. Luego, se construy√≥ una matriz de los valores de PFA para cada palabra que exist√≠a en el recurso l√©xico y se encontraba en los documentos. De esta manera tanto la BoW como la matriz PFA tiene las mismas dimensiones. Aquellas palabras que estaban en el recurso l√©xico se quedaron con un valor del par√°metro **default_weight** en esta matriz. Finalmente se procedi√≥ a realizar una multiplicaci√≥n punto a punto entre ambas matrices con el objetivo de que aquellas palabras que tengan un PFA bajo tengan menor relevancia que aquellas que tengan un PFA alto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4  ¬øPodemos mejorar con bigramas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Hacer un experimento d√≥nde concatene una buena BoW seg√∫n sus experimentos anteriores con otra BoW construida a partir de los 1000 bigramas m√°s frecuentes. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for doc in documents for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words)\n",
    "\n",
    "#finder.apply_freq_filter(10)\n",
    "bigrams = finder.nbest(bigram_measures.pmi, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict = {}\n",
    "for i, bigram in enumerate(bigrams):\n",
    "    bigram_dict[bigram[0]+bigram[1]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_document(documents):\n",
    "    bigram_documents = []\n",
    "    for doc in documents:\n",
    "        bigram_doc = []\n",
    "        for i in range(len(doc)-1):\n",
    "            bigram_doc.append(doc[i] + doc[i+1])\n",
    "        bigram_documents.append(bigram_doc)\n",
    "    \n",
    "    return bigram_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_documents = build_bigram_document(documents)\n",
    "val_bigram_documents = build_bigram_document(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_builder = BoWBuilder()\n",
    "bigram_bow = bow_builder.build_bow(bigram_documents, voc_index=bigram_dict, weight_scheme='binary')\n",
    "val_bigram_bow = bow_builder.build_bow(val_bigram_documents, mode='test', weight_scheme='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_bow = np.column_stack((sel_bow, bigram_bow))\n",
    "val_bigram_bow = np.column_stack((val_sel_bow, val_bigram_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8198051948051948 \n",
      " precision:  0.8026244649695877 \n",
      " recall:  0.8090185523848958 \n",
      " f_measure:  0.8055107561289881\n"
     ]
    }
   ],
   "source": [
    "model = get_model(5000)\n",
    "model.fit(bigram_bow, y_train)\n",
    "pred = model.predict(val_bigram_bow)\n",
    "p, r, f, _ = precision_recall_fscore_support(y_val, pred, average='macro', pos_label=None)\n",
    "a = accuracy_score(y_val, pred)\n",
    "print(\" accuracy: \", a, \"\\n precision: \", p, \"\\n recall: \", r, \"\\n f_measure: \", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. Hacer un experimento con las Bolsas de Emociones, Bolsa de Palabras y Bolsa de Bi-\n",
    "gramas; usted elige las dimensionalidades. Para construir la representaci√≥n final del\n",
    "documento utilice la concatenaci√≥n de las representaciones seg√∫n sus observaciones\n",
    "(e.g., Bolsa de Palabras + Bolsa de Bigramas + Bolsa de Sentimientos de Canad√° + Bolsa\n",
    "de Sentimientos de Grigori), y alim√©ntelas a un SVM. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. Elabore conclusiones sobre toda esta Tarea, incluyendo observaciones, comentarios y\n",
    "posibles mejoras futuras. Discuta el comportamiento de la BoW de usar solo palabras\n",
    "a integrar bigramas, y luego a integrar todo ¬øayud√≥? o ¬øempeor√≥?. Discuta tambi√©n\n",
    "brevemente el costo computacional de los experimentos ¬øVali√≥ la Pena tener todo?. Sea\n",
    "breve: todo en NO m√°s de dos p√°rrafos. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"hola como estas\", \n",
    "    \"hola mundo, voy a comer el mundo\",\n",
    "    \"hola me voy a ir\",\n",
    "    \"hola quiero comer\",\n",
    "    \"hola voy a comer hoy\",\n",
    "    \"hola el mundo esta mal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_object = BoW(docs)\n",
    "bow = bow_object.build_bow(T=3, weight_scheme='tf-idf', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.92909298, 0.36984623],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hola', 3), ('mundo', 3), ('voy', 3)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_object.get_dimensions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
