{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2: Juan Luis Baldelomar Cabrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl',\n",
       " 'a la vga no seas mamÃ³n 45 putos minutos despuÃ©s me dices que apenas sales no me querÃ­as avisar en 3 horas? ðŸ˜‘',\n",
       " 'considero que lo mÃ¡s conveniente seria que lo retes a unos vergazos mi jelipe! rÃ³mpele la madre a ese pinchi joto!',\n",
       " 'el marica de mi ex me tiene bloqueada de todo asÃ­  uno no puede admirar la \"belleza\" de su garnacha ðŸ˜‚',\n",
       " 'mujer despechadaya pinche amlo hazle esta que se pela la loca #reynosafollow #reynosa',\n",
       " 'putos. no tienen madre. ambriados mantenidos. ojetes. como es posible. mejor matarlos',\n",
       " 'ustedes si puden andar de chanceros pero cuidadito y seamos nosotras porque luego luego empiezan a mamar hijos de la chingada.',\n",
       " '@usuario jajjaja te digo esa madre si estÃ¡ buena ajjaja',\n",
       " 'odio los putos trÃ¡mites de titulaciÃ³n ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ pero me urge la precedula.',\n",
       " '@usuario no te equivocabas mi madre y tu tenÃ­an muchÃ­sima razÃ³n siempre es mejor lo que viene ðŸ’š']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asÃ­ deberÃ­a ser siempre para que se mueran a la verga',\n",
       " 'cada dÃ­a me siento como un perro ovejero tratando de cuidar sus ovejas vale madre pinches rateros pÃ³ngase a trabajar.ðŸ˜¤',\n",
       " 'hijo de tu puta madre nadamas te la pasas mamando pinche wilo de closet',\n",
       " 'soÃ±Ã© horrible espero no se cumpla putos temblores.',\n",
       " '@usuario mejor vas y la chingas tÃº veo tus publicaciones y solo denotan inconformidad como tÃ­pico ciudadano quejumbroso que no hace nada por cambiar',\n",
       " 'y quizÃ¡s este loca y quizÃ¡s me guste estarlo... #siempreneruda',\n",
       " 'me tienen hasta la verga con sus putos #boomerang pendejos y sin sentido.',\n",
       " 'cuando va a ser el dÃ­a que encuentre a alguien en quien pueda  confiar posts la re puta madre?',\n",
       " '#ahoritaestoypensando  como putas puede pasar esto?',\n",
       " '#todoibabienpero valio madre ya me atacÃ³ el insomnioðŸ™…']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_documents) == len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Class\n",
    "\n",
    "A continuaciÃ³n tenemos la clase que utilizaremos para construir las bolsas de palabras y poder utilizar luego de manera fÃ¡cil un modo de testing. \n",
    "\n",
    "El mÃ©todo que se encarga de construir las bolsas de palabras recibe de parÃ¡metro los documentos tokenizados y la cantidad mÃ¡xima de tÃ©rminos mÃ¡s frecuentes que se desea considerar. TambiÃ©n se puede enviar un diccionario que ya contenga las posiciones y palabras que se desean utilizar. Esto es Ãºtil para cuando deseamos hacer reducciÃ³n de dimensiones y mandar el diccionario de tÃ©rminos ya reducido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "class BoWBuilder:\n",
    "    # UTILITIES\n",
    "    def get_dimensions(self):\n",
    "        return self.voc_index.keys()\n",
    "    \n",
    "    # INIT FUNCTIONS\n",
    "    def __init__(self):\n",
    "        # attributes\n",
    "        self.voc_index = {} \n",
    "        self.train_idf = None\n",
    "        self.T = 0\n",
    "        \n",
    "    def get_vocabulary(self, documents, T):\n",
    "        # get vocabulary\n",
    "        tokens = [token for doc in documents for token in doc]\n",
    "        vocabulary = FreqDist(tokens)\n",
    "        \n",
    "        self.T = np.min(T, len(vocabulary.keys()))\n",
    "        \n",
    "        # get most common words\n",
    "        limited_voc = vocabulary.most_common(T)\n",
    "        self.voc_index = {}\n",
    "        \n",
    "        # get index of words in matrix\n",
    "        for i, word_count in enumerate(limited_voc):\n",
    "            self.voc_index[word_count[0]] = i\n",
    "    \n",
    "    \n",
    "    # BUILD BOW MATRIX\n",
    "    def build_bow(self, documents, T=5000, voc_index=None, mode='train', weight_scheme='binary', normalize=False):\n",
    "        # get most common terms - training mode\n",
    "        if mode == 'train':\n",
    "            if voc_index==None:\n",
    "                self.get_vocabulary(documents, T) #use most common words as vocabulary\n",
    "            else:\n",
    "                # use vocabulary index sent as parameter. Usefull when performing a features reduction or working with n-grams\n",
    "                self.voc_index = voc_index\n",
    "                self.T = len(voc_index.keys())\n",
    "        \n",
    "        # use train_idf, testing mode\n",
    "        use_train_idf = mode != 'train'\n",
    "        \n",
    "        # get weights for matrix\n",
    "        if weight_scheme == 'tf':\n",
    "            bow = self.frequency_bow(documents)\n",
    "        elif weight_scheme == 'tf-idf':\n",
    "            # if documents!= None, use existing idf weights (val or test mode)\n",
    "            bow = self.frequency_bow(documents, use_idf=True, use_train_idf=use_train_idf)\n",
    "        else:\n",
    "            bow = self.binary_bow(documents)\n",
    "        \n",
    "        # normalize if necessary\n",
    "        if normalize:\n",
    "            norm = np.linalg.norm(bow, axis=1)\n",
    "            # Add 1 if norm == 0 to avoid division by 0. --  Increase 1 dimension for broadcast \n",
    "            bow = bow / (norm + (norm==0 + 0.0))[:, np.newaxis]\n",
    "        \n",
    "        return bow\n",
    "            \n",
    "    # WEIGHT SCHEMES\n",
    "    def binary_bow(self, documents):\n",
    "        N = len(documents)\n",
    "        T = self.T\n",
    "        \n",
    "        bow = np.zeros((N, T))\n",
    "        for i, doc in enumerate(documents):\n",
    "            for word in doc:\n",
    "                j = self.voc_index.get(word)\n",
    "                if j != None:\n",
    "                    bow[i, j] = 1 \n",
    "        \n",
    "        return bow\n",
    "    \n",
    "    def frequency_bow(self, documents, use_idf=False, use_train_idf=False):\n",
    "        N = len(documents)\n",
    "        T = self.T\n",
    "        bow = np.zeros((N, T))\n",
    "        \n",
    "        # tf scheme\n",
    "        for i, doc in enumerate(documents):\n",
    "            for word in doc:\n",
    "                j = self.voc_index.get(word)\n",
    "                if j != None:\n",
    "                    bow[i, j] += 1 \n",
    "        \n",
    "        # tf-idf scheme\n",
    "        if use_idf:\n",
    "            if not use_train_idf:\n",
    "                # calculate idf for first time (training mode)\n",
    "                self.train_idf = np.sum(bow>0, axis=0)\n",
    "            \n",
    "            bow = np.log(bow + 1) * np.log(N/self.train_idf)\n",
    "            \n",
    "        return bow        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model and Labels (EXECUTE FIRST!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "\n",
    "def get_model(max_iter=1000):\n",
    "    parameters = {'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]}\n",
    "    svr = svm.LinearSVC(class_weight='balanced', max_iter=max_iter)\n",
    "    grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring='f1_macro', cv=5) \n",
    "    return grid\n",
    "\n",
    "def get_nl_model(max_iter=1000):\n",
    "    parameters = {'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]}\n",
    "    svr = svm.SVC(kernel='poly', class_weight='balanced', max_iter=max_iter)\n",
    "    grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring='f1_macro', cv=5) \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "y_train = list(map(int, labels))\n",
    "y_val = list(map(int, val_labels))\n",
    "\n",
    "# BoW Builder\n",
    "bow_builder = BoWBuilder()\n",
    "\n",
    "def train_and_test(model, bow, val_bow):\n",
    "    model.fit(bow, y_train)\n",
    "    pred = model.predict(val_bow)\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_val, pred, average='macro', pos_label=None)\n",
    "    a = accuracy_score(y_val, pred)\n",
    "    print(\" accuracy: \", a, \"\\n precision: \", p, \"\\n recall: \", r, \"\\n f_measure: \", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratio of positive labels: ', 0.3573232323232323)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ratio of positive labels: \", sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi Square Metric\n",
    "\n",
    "En el siguiente bloque tenemos dos funciones, una se encarga de calcular los valores de la mÃ©trica chi cuadrado y el segundo se encarga de reducir dimensiones en funciÃ³n de la mÃ©trica chi cuadrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_sqr(tf_matrix, labels):\n",
    "    n_p = 2\n",
    "    Nt = len(tf_matrix)\n",
    "    n_i = np.sum(tf_matrix>0, axis=0)\n",
    "    n_i1 = np.sum((tf_matrix * labels)>0, axis=0)\n",
    "    n_i0 = np.sum((tf_matrix * (1-labels))>0, axis=0)\n",
    "    c_1 = np.sum(labels, axis=0)\n",
    "    c_0 = Nt - c_1\n",
    "    \n",
    "    # compute chi\n",
    "    chi_1 = Nt*(Nt * n_i1 - n_p * n_i)**2/(n_p * n_i * (Nt - n_p) * (Nt - n_i))\n",
    "    chi_0 = Nt*(Nt * n_i0 - n_p * n_i)**2/(n_p * n_i * (Nt - n_p) * (Nt - n_i))\n",
    "    \n",
    "    # get chi average\n",
    "    chi_avg = c_1/Nt * chi_1 +  c_0/Nt * chi_0 \n",
    "    \n",
    "    return chi_avg\n",
    "\n",
    "def reduce_dims(bow, voc_index, limit=-1):\n",
    "    # compute chi avg\n",
    "    chi_avg = chi_sqr(bow, np.array(y_train)[:, np.newaxis])\n",
    "    lim = np.mean(chi_avg) if limit==-1 else limit\n",
    "    \n",
    "    new_dict = {}\n",
    "    counter = 0\n",
    "    \n",
    "    for i, word in enumerate(voc_index.keys()):\n",
    "        if chi_avg[i] >= lim:\n",
    "            new_dict[word] = counter\n",
    "            counter += 1\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bolsas de Palabras, Bigramas y Emociones (40pts)\n",
    "\n",
    "Para las siguientes secciones se considerÃ³ realizar una eliminaciÃ³n de las stop words, sin embargo cuando se llevaron a cabo experimentos los resultados no eran mejores y en algunos casos el desempeÃ±o era considerablemente menor eliminando estos tÃ©rminos, por lo tanto no se eliminaron las stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. EvaluÃ© BoW con pesado Binario </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8133116883116883 \n",
      " precision:  0.7958333333333334 \n",
      " recall:  0.807051746546588 \n",
      " f_measure:  0.8003421578491969\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='binary', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='binary', normalize=False)\n",
    "model = get_model(2000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probaremos realizar una reducciÃ³n de dimensiones y ver el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " features len: 1146 \n",
      "\n",
      " accuracy:  0.8068181818181818 \n",
      " precision:  0.7889044506691565 \n",
      " recall:  0.7989429856342661 \n",
      " f_measure:  0.7930288699089433\n"
     ]
    }
   ],
   "source": [
    "# reduce dims with chi sqr\n",
    "bow = bow_builder.build_bow(documents, T=20000, weight_scheme='binary', normalize=False)\n",
    "new_voc_index = reduce_dims(bow, bow_builder.voc_index)\n",
    "print(' features len:', len(new_voc_index), \"\\n\")\n",
    "\n",
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, voc_index=new_voc_index, weight_scheme='binary', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, mode='test', weight_scheme='binary', normalize=False)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que en efecto hay una reducciÃ³n de dimensiones y los resultados obtenidos son cercanos. Esto nos podrÃ­a llevar a pensar que vale la pena hacer la reducciÃ³n de dimensiones. Sin embargo, en un experimento mÃ¡s adelante **(ejercicio 8)** utilizaremos los 1000 tÃ©rminos mÃ¡s frecuentes sin realizar la reducciÃ³n de dimensiones, y esta bolsa de palabras obtiene un mayor desempeÃ±o. Una teorÃ­a es que los documentos son muy pequeÃ±os y esparsos que tal vez la reducciÃ³n de dimensiones por medio de estas mÃ©tricas no aporta demasiado ya que los datasets de redes sociales suelen ser muy ruidosos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. EvaluÃ© BoW con pesado de Frecuencia </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8181818181818182 \n",
      " precision:  0.800838198545479 \n",
      " recall:  0.8087827657200695 \n",
      " f_measure:  0.8043113228953936\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf', normalize=False)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. EvaluÃ© BoW con pesado tf-idf </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.6948051948051948 \n",
      " precision:  0.7023890612780342 \n",
      " recall:  0.5922730984668116 \n",
      " f_measure:  0.578082191780822\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf-idf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf-idf', normalize=False)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4. EvaluÃ© BoW con pesado Binario Normalizado l2 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7987012987012987 \n",
      " precision:  0.7809010396611475 \n",
      " recall:  0.7936694155941248 \n",
      " f_measure:  0.785650143678161\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='binary', normalize=True)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='binary', normalize=True)\n",
    "model = get_model(2000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5. EvaluÃ© BoW con pesado Frecuencia Normalizado l2 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7954545454545454 \n",
      " precision:  0.7775125144397381 \n",
      " recall:  0.7901268647274651 \n",
      " f_measure:  0.7821928879310345\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf', normalize=True)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf', normalize=True)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 6. EvaluÃ© BoW con pesado tf-idf Normalizado l2 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7256493506493507 \n",
      " precision:  0.7279454651501895 \n",
      " recall:  0.6438413673326202 \n",
      " f_measure:  0.6482200483214219\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf-idf', normalize=True)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf-idf', normalize=True)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 7. Ponga una tabla comparativa a modo resumen con las 6 entradas anteriores. </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Tabla de Resultados </h2>**\n",
    "\n",
    "A continuaciÃ³n mostramos los resultados de la mÃ©trica de accuracy resumidos en una tabla para cada modelo.\n",
    "\n",
    "**No.** | **Modelo** | **accuracy** | **precision** | **recall** | **F measure**\n",
    " -------- |----| ---- | ---- | ---- | ----\n",
    "1 |`Binary` | 0.8133  | 0.7958 | 0.8071 | 0.8003\n",
    "2 | `TF` | 0.8182 | 0.8008 | 0.8088 | 0.8043\n",
    "3 | `TF-IDF` | 0.6948 | 0.7024 | 0.5923 | 0.5781 \n",
    "4 | `Normalized Binary` | 0.7987 | 0.7809 | 0.7937 | 0.7857\n",
    "5 | `Normalized TF` | 0.7955 | 0.7775 | 0.7901 | 0.7822\n",
    "5 | `Normalized TF-IDF` | 0.7256 | 0.7279 | 0.6438 | 0.6482"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 8. De las configuraciones anteriores elija la mejor y evalÃºela con mÃ¡s y menos tÃ©rminos\n",
    "(e.g., 1000 y 7000). Ponga una tabla dÃ³nde compare las tres configuraciones. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8214285714285714 \n",
      " precision:  0.8042776567822638 \n",
      " recall:  0.8133489757657315 \n",
      " f_measure:  0.8081605381470618\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=1000, weight_scheme='tf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, mode='test', weight_scheme='tf', normalize=False)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8181818181818182 \n",
      " precision:  0.8008658008658008 \n",
      " recall:  0.8077591065410672 \n",
      " f_measure:  0.8039443023586246\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=7000, weight_scheme='tf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=7000, mode='test', weight_scheme='tf', normalize=False)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Tabla de Resultados TF con Diferente Cantidad de Terminos</h2>**\n",
    "\n",
    "A continuaciÃ³n mostramos los resultados de la mÃ©trica de accuracy resumidos en una tabla para cada modelo.\n",
    "\n",
    "**No.** | **Modelo** | **accuracy** | **precision** | **recall** | **F measure**\n",
    " -------- |----| ---- | ---- | ---- | ----\n",
    "1 | `TF 1000 Terminos` | 0.8214 | 0.8043 | 0.8133 | 0.8082\n",
    "2 | `TF 5000 Terminos` | 0.8182 | 0.8008 | 0.8088 | 0.8043\n",
    "3 | `TF 7000 Terminos` | 0.8182 | 0.8009 | 0.8076 | 0.8039"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 9. Utilice el recurso lÃ©xico del Consejo Nacional de InvestigaciÃ³n de CanadÃ¡ llamado \"EmoLex\" (https://www.saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) para construir una \"Bolsa de Emociones\" de los Tweets de agresividad (Debe usar EmoLex en EspaÃ±ol). Para esto, una estrategia sencilla serÃ­a enmascarar cada palabra con su emociÃ³n, y despuÃ©s construir la Bolsa de Emociones (BoE). </h3>\n",
    "\n",
    "Para enmascarar las palabras por su emociÃ³n, se tomara la cadena binaria que representa a cada palabra y se sustituirÃ¡ la palabra por esta cadena. En caso de que la palabra no se encuentre en el recurso lÃ©xico se decidiÃ³ conservarla debido a que el desempeÃ±o era mayor que solo ignorando estos tÃ©rminos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "emotion_data = pandas.read_excel('data/emolex.xlsx', usecols=\"CI,DB:DK\")\n",
    "\n",
    "emotion_dict = {}\n",
    "for i in range(len(emotion_data)):\n",
    "    row = emotion_data.loc[i]\n",
    "    mask = ''\n",
    "    for val in row[1:]:\n",
    "        mask+= str(val)\n",
    "    emotion_dict[row[0]] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1001001001'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_dict['disfrutar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_word(mask_dict, documents, ignore=True):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            mask = mask_dict.get(word)\n",
    "            if mask != None:\n",
    "                masked_doc.append(mask)\n",
    "            elif not ignore:\n",
    "                masked_doc.append(word)\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "emolex_documents = mask_word(emotion_dict, documents, ignore=False)\n",
    "emolex_val_documents = mask_word(emotion_dict, val_documents, ignore=False)\n",
    "\n",
    "# BoWBuilder object\n",
    "bow_builder = BoWBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "emolex_bow = bow_builder.build_bow(documents, T=2048, weight_scheme='binary')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, T=2048, mode='test', weight_scheme='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>10. EvalÃºa tÃº BoE clasificando con SVM. Ponga una tabla comparativa a modo de resumen\n",
    "con los tres pesados, normalize cada uno si lo cree conveniente. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7873376623376623 \n",
      " precision:  0.7682203389830509 \n",
      " recall:  0.7766640212553052 \n",
      " f_measure:  0.7717453717453717\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='binary')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='binary')\n",
    "model = get_model(5000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8003246753246753 \n",
      " precision:  0.7818834742918512 \n",
      " recall:  0.7898105655429419 \n",
      " f_measure:  0.7852910421749739\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf')\n",
    "model = get_model(13000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.702922077922078 \n",
      " precision:  0.6820700869449792 \n",
      " recall:  0.6262091255190182 \n",
      " f_measure:  0.6289779515585967\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf-idf')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf-idf')\n",
    "model = get_model(15000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>Normalized</h4>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7873376623376623 \n",
      " precision:  0.7683629007158419 \n",
      " recall:  0.7776876804343076 \n",
      " f_measure:  0.7721578315804334\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='binary', normalize=True)\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='binary', normalize=True)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.788961038961039 \n",
      " precision:  0.7700032285717466 \n",
      " recall:  0.7789471262781362 \n",
      " f_measure:  0.7736932537529391\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf', normalize=True)\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf', normalize=True)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7159090909090909 \n",
      " precision:  0.6922965051237302 \n",
      " recall:  0.697704243009788 \n",
      " f_measure:  0.6945197754521986\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf-idf', normalize=True)\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf-idf', normalize=True)\n",
    "model = get_model(13000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Tabla de Resultados Emolex </h2>**\n",
    "\n",
    "A continuaciÃ³n mostramos los resultados de la mÃ©trica de accuracy resumidos en una tabla para cada modelo.\n",
    "\n",
    "**No.** | **Modelo** | **accuracy** | **precision** | **recall** | **F measure**\n",
    " -------- |----| ---- | ---- | ---- | ----\n",
    "1 |`Binary` | 0.7873  | 0.7680 | 0.7767 | 0.7117\n",
    "2 | `TF` | 0.8003 | 0.7819 | 0.7898 | 0.7853\n",
    "3 | `TF-IDF` | 0.7029 | 0.6821 | 0.6262 | 0.6290 \n",
    "4 | `Normalized Binary` | 0.7873 | 0.7684 | 0.7777 | 0.7722\n",
    "5 | `Normalized TF` | 0.7890 | 0.7700 | 0.7789 | 0.7737\n",
    "5 | `Normalized TF-IDF` | 0.7159 | 0.6923 | 0.6977 | 0.6945"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Recurso LÃ­nguistico de Emociones Mexicano (30 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Utilice el recurso lÃ©xico llamado \"Spanish Emotion Lexicon (SEL)\" del Dr. Grigori Sidorov, profesor del Centro de InvestigaciÃ³n en ComputaciÃ³n (CIC) del Instituto PolitÃ©cnico Nacional (http://www.cic.ipn.mx/âˆ¼sidorov/), para enmascarar cada palabra con su emociÃ³n, y despuÃ©s construir la Bolsa de Emociones con algÃºn pesado (e.g., binario, tf, tfidf). Proponga alguna estrategia para incorporar el \"valor\" del \"Probability Factor of Affective use\" en su representaciÃ³n vectorial del documento. EvalÃºa y escribe una tabla comparativa a modo de resumen con al menos tres pesados: binario, frecuencia, tfidf. Normalize cada pesado segÃºn lo crea conveniente de acuerdo el experimento (1). </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "emotion_data = pandas.read_excel('data/SEL.xlsx', sheet_name=\"Hoja1\", usecols=\"B,C,D\")\n",
    "\n",
    "# build dictionary will all the words in sel resource\n",
    "sel_dict = {}\n",
    "for i in range(len(emotion_data)):\n",
    "    row = emotion_data.loc[i]\n",
    "    mask = row[2]\n",
    "    pfa = row[1]\n",
    "    sel_dict[row[0]] = (mask, pfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Enojo', 0.932)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_dict['odio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask word as a pair <word>-<emotion> \n",
    "# this is because we want to get specific pfa for each word with its emotion\n",
    "def mask_word_sel(mask_dict, documents, ignore=True):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            mask = mask_dict.get(word)\n",
    "            if mask != None:\n",
    "                masked_doc.append(word + \":::\" + mask[0])\n",
    "            elif not ignore:\n",
    "                masked_doc.append(word)\n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents\n",
    "\n",
    "\n",
    "'''\n",
    "    -- build pfa matrix\n",
    "    * voc_index: dictionary with the column position of each word in the bow\n",
    "    * sel_dict: dictionary with the sel_dict resource to retrieve its pfa\n",
    "    * documents: masked documents by mask_word function\n",
    "    * return: pfa_matrix\n",
    "'''\n",
    "\n",
    "def get_pfa_matrix(voc_index, sel_dict, documents, default_weight=1):\n",
    "    pfa_matrix = np.ones((len(documents), len(voc_index))) * default_weight\n",
    "    for i, doc in enumerate(documents):\n",
    "        for word_emo in doc:\n",
    "            l_words = word_emo.split(\":::\")\n",
    "            #word, emo = l_words[0], l_words[1] if len(l_words) == 2 else word_emo, None\n",
    "            if len(l_words) == 2:\n",
    "                word, emo = l_words\n",
    "            else:\n",
    "                word = word_emo\n",
    "            emotion_pfa = sel_dict.get(word)\n",
    "            if emotion_pfa != None:\n",
    "                j = voc_index.get(word_emo)\n",
    "                if j != None:\n",
    "                    pfa_matrix[i, j] = emotion_pfa[1]\n",
    "    \n",
    "    return pfa_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.810064935064935 \n",
      " precision:  0.7924202127659574 \n",
      " recall:  0.8035091956799283 \n",
      " f_measure:  0.796869847550922\n"
     ]
    }
   ],
   "source": [
    "# build masked documents\n",
    "sel_documents = mask_word_sel(sel_dict, documents, ignore=False)\n",
    "val_sel_documents = mask_word_sel(sel_dict, val_documents, ignore=False)\n",
    "\n",
    "# bow builder object\n",
    "bow_builder = BoWBuilder()\n",
    "\n",
    "# build bow for masked documents and get its pfa matrix\n",
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='binary')\n",
    "sel_pfa = get_pfa_matrix(bow_builder.voc_index, sel_dict, sel_documents, default_weight=1)\n",
    "\n",
    "# build bow for masked validation documents and get its pfa matrix\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='binary')\n",
    "val_sel_pfa = get_pfa_matrix(bow_builder.voc_index, sel_dict, val_sel_documents, default_weight=1)\n",
    "\n",
    "# compute final bows by element-wise-multiplication (original_bow * pfa_matrix)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8246753246753247 \n",
      " precision:  0.8077410100964826 \n",
      " recall:  0.815867867453389 \n",
      " f_measure:  0.8113002042205582\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf')\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf')\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(3000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el rendimiento es mejor para este modelo que para todos los demÃ¡s. **Â¿QuÃ© puede estar sucediendo?**. Una hipÃ³tesis es que al enmascarar las palabras por su emociÃ³n y multiplicar estas por su PFA se intensifica o atenua sus pesos en funciÃ³n de que tan fuerte es la emociÃ³n y esto aporta algo al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.6948051948051948 \n",
      " precision:  0.7050264892104923 \n",
      " recall:  0.5912494392878093 \n",
      " f_measure:  0.5760743831905704\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf-idf')\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf-idf')\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(15000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8003246753246753 \n",
      " precision:  0.7825004406839415 \n",
      " recall:  0.7949288614379536 \n",
      " f_measure:  0.7871966341522327\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='binary', normalize=True)\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='binary', normalize=True)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.724025974025974 \n",
      " precision:  0.7243182599569332 \n",
      " recall:  0.6425819214887915 \n",
      " f_measure:  0.646827109864019\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf-idf', normalize=True)\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf-idf', normalize=True)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Tabla de Resultados SEL </h2>**\n",
    "\n",
    "A continuaciÃ³n mostramos los resultados de la mÃ©trica de accuracy resumidos en una tabla para cada modelo.\n",
    "\n",
    "**No.** | **Modelo** | **accuracy** | **precision** | **recall** | **F measure**\n",
    " -------- |----| ---- | ---- | ---- | ----\n",
    "1 |`Binary` | 0.8101  | 0.7924 | 0.8035 | 0.7969\n",
    "2 | `TF` | 0.8247 | 0.8077 | 0.8159 | 0.8113\n",
    "3 | `TF-IDF` | 0.6948 | 0.7050 | 0.5912 | 0.5761 \n",
    "4 | `Normalized Binary` | 0.8003 | 0.7825 | 0.7949 | 0.7872\n",
    "5 | `Normalized TF-IDF` | 0.7240 | 0.7243 | 0.6423 | 0.6468\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. En un comentario aparte, discuta sobre la estrategÃ­a que utilizÃ³ para incorporar el\n",
    "\"Probability Factor of Affective use\". No mÃ¡s de 5 renglones. </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se generÃ³ una matriz de pares de la forma \\<word\\>-\\<emotion\\> y con esto se construyo una bolsa de palabras. Esto con la intenciÃ³n de que palabras diferentes con la misma emociÃ³n generaran un par distinto para poder usar sus valores de PFA. Luego, se construyÃ³ una matriz de los valores de PFA para cada palabra que existÃ­a en el recurso lÃ©xico y se encontraba en los documentos. De esta manera tanto la BoW como la matriz PFA tiene las mismas dimensiones. Aquellas palabras que no estaban en el recurso lÃ©xico se quedaron con un valor del parÃ¡metro **default_weight** en esta matriz. Finalmente se procediÃ³ a realizar una multiplicaciÃ³n punto a punto entre ambas matrices con el objetivo de que aquellas palabras que tengan un PFA bajo tengan menor relevancia que aquellas que tengan un PFA alto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4  Â¿Podemos mejorar con bigramas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Hacer un experimento dÃ³nde concatene una buena BoW segÃºn sus experimentos anteriores con otra BoW construida a partir de los 1000 bigramas mÃ¡s frecuentes. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "# BigramCollocation Object from all words  \n",
    "all_words = [word for doc in documents for word in doc]\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words)\n",
    "\n",
    "# get 1000 most frequent bigrams\n",
    "bigrams = finder.ngram_fd.most_common(1000)\n",
    "\n",
    "# build bigram_index dict \n",
    "bigram_dict = {}\n",
    "for i, (bigram, _) in enumerate(bigrams):\n",
    "    bigram_dict[bigram[0]+bigram[1]] = i\n",
    "\n",
    "# convert documents into bigram documents\n",
    "def build_bigram_documents(documents):\n",
    "    bigram_documents = [[word1 + word2 for word1, word2 in zip(doc, doc[1:])] for doc in documents]\n",
    "    return bigram_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build bigram documents\n",
    "bigram_documents = build_bigram_documents(documents)\n",
    "val_bigram_documents = build_bigram_documents(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow builder object\n",
    "bow_builder = BoWBuilder()\n",
    "\n",
    "# use as vocabulary bigram_index dict computed before\n",
    "bigram_bow = bow_builder.build_bow(bigram_documents, voc_index=bigram_dict, weight_scheme='binary')\n",
    "val_bigram_bow = bow_builder.build_bow(val_bigram_documents, mode='test', weight_scheme='binary')\n",
    "\n",
    "# SEL emotion bag\n",
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf')\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf')\n",
    "sel_bow = sel_bow * get_pfa_matrix(bow_builder.voc_index, sel_dict, sel_documents)\n",
    "val_sel_bow = val_sel_bow * get_pfa_matrix(bow_builder.voc_index, sel_dict, val_sel_documents)\n",
    "\n",
    "# append columns of bows\n",
    "final_bow = np.column_stack((sel_bow, bigram_bow))\n",
    "val_final_bow = np.column_stack((val_sel_bow, val_bigram_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.814935064935065 \n",
      " precision:  0.7974453584283989 \n",
      " recall:  0.803192896495405 \n",
      " f_measure:  0.8000637755102041\n"
     ]
    }
   ],
   "source": [
    "model = get_model(5000)\n",
    "train_and_test(model, final_bow, val_final_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. Hacer un experimento con las Bolsas de Emociones, Bolsa de Palabras y Bolsa de Bi-\n",
    "gramas; usted elige las dimensionalidades. Para construir la representaciÃ³n final del\n",
    "documento utilice la concatenaciÃ³n de las representaciones segÃºn sus observaciones\n",
    "(e.g., Bolsa de Palabras + Bolsa de Bigramas + Bolsa de Sentimientos de CanadÃ¡ + Bolsa\n",
    "de Sentimientos de Grigori), y alimÃ©ntelas a un SVM. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_builder = BoWBuilder()\n",
    "\n",
    "# regular bow\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf')\n",
    "val_bow = bow_builder.build_bow(val_documents, mode='test', weight_scheme='tf')\n",
    "\n",
    "# emolex bow\n",
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf')\n",
    "\n",
    "# bigram bow\n",
    "bigram_bow = bow_builder.build_bow(bigram_documents, T=1000, weight_scheme='tf')\n",
    "val_bigram_bow = bow_builder.build_bow(val_bigram_documents, mode='test', weight_scheme='tf')\n",
    "\n",
    "# SEL emotion bag\n",
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf')\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf')\n",
    "sel_bow = sel_bow * get_pfa_matrix(bow_builder.voc_index, sel_dict, sel_documents)\n",
    "val_sel_bow = val_sel_bow * get_pfa_matrix(bow_builder.voc_index, sel_dict, val_sel_documents)\n",
    "\n",
    "# append columns of bows\n",
    "final_bow = np.column_stack((bow, emolex_bow, sel_bow, bigram_bow))\n",
    "val_final_bow = np.column_stack((val_bow, val_emolex_bow, val_sel_bow, val_bigram_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8003246753246753 \n",
      " precision:  0.7818357159065124 \n",
      " recall:  0.7857159288269326 \n",
      " f_measure:  0.7836528989335123\n"
     ]
    }
   ],
   "source": [
    "model = get_model(10000)\n",
    "train_and_test(model, final_bow, val_final_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. Elabore conclusiones sobre toda esta Tarea, incluyendo observaciones, comentarios y\n",
    "posibles mejoras futuras. Discuta el comportamiento de la BoW de usar solo palabras\n",
    "a integrar bigramas, y luego a integrar todo Â¿ayudÃ³? o Â¿empeorÃ³?. Discuta tambiÃ©n\n",
    "brevemente el costo computacional de los experimentos Â¿ValiÃ³ la Pena tener todo?. Sea\n",
    "breve: todo en NO mÃ¡s de dos pÃ¡rrafos. </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pudimos observar a lo largo de la tarea que el pesado por frecuencia de tÃ©rminos fue el de mejor desempeÃ±o en la mayorÃ­a de casos. Esto puede deberse a que los documentos son tweets y son muy cortos, por lo tanto es muy probable que muchas palabras tengan como frecuencia 1, lo cual simula el pesado binario, y en algunos casos habrÃ¡ palabras que tengan una mayor frecuencia que aporta informaciÃ³n, pero no serÃ¡ tan grande como dominar en la clasificaciÃ³n por esas palabras. TambiÃ©n pudimos observar que la normalizaciÃ³n no ayudaba mucho en la mayorÃ­a de casos, en especial para el pesado por frecuencia y nuevamente esto puede deberse a que los documentos en su mayorÃ­a tienen una representaciÃ³n sparse y en donde la mayorÃ­a de sus dimensiones tienen 1s, no solo por lo pequeÃ±o de los documentos pero incluso por el ruido que conlleva la forma natural de escribir para los humanos en  redes sociales. \n",
    "\n",
    "Una posible mejora podrÃ­a ser el tratamiento del ruido en el dataset lo cual ayudarÃ­a a que tÃ©rminos que en esencia son lo mismo aportaran mÃ¡s informaciÃ³n. Por otro lado, al utilizar bigramas e integrar todo no aportÃ³, y empeorÃ³ por poco el rendimiento del modelo en comparaciÃ³n con el mejor modelo que se obtuvo, que fue el **modelo SEL con pesado TF**. Se podrÃ­a concluir que no valiÃ³ la pena integrar toda esta informaciÃ³n, pues el costo computacional es mayor ya que los documentos terminaban teniendo mÃ¡s del doble de atributos en comparaciÃ³n con los modelos simples y requerÃ­an en ocasiones un mayor nÃºmero de iteraciones para que la SVM pudiera converger. Algo importante de resaltar es que a pesar de que el modelo SEL con pesado TF presentÃ³ los mejores resultados, una simple **bolsa de palabras con los 1000 tÃ©rminos mÃ¡s frecuentes** se acercÃ³ bastante a este desempeÃ±o, lo cual ejemplifica lo poderosos que pueden ser estos modelos tan simples para tareas de clasificaciÃ³n."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
