{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')\n",
    "documents.pop(-1)\n",
    "val_documents.pop(-1)\n",
    "labels.pop(-1)\n",
    "val_labels.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl',\n",
       " 'a la vga no seas mamÃ³n 45 putos minutos despuÃ©s me dices que apenas sales no me querÃ­as avisar en 3 horas? ðŸ˜‘',\n",
       " 'considero que lo mÃ¡s conveniente seria que lo retes a unos vergazos mi jelipe! rÃ³mpele la madre a ese pinchi joto!',\n",
       " 'el marica de mi ex me tiene bloqueada de todo asÃ­  uno no puede admirar la \"belleza\" de su garnacha ðŸ˜‚',\n",
       " 'mujer despechadaya pinche amlo hazle esta que se pela la loca #reynosafollow #reynosa',\n",
       " 'putos. no tienen madre. ambriados mantenidos. ojetes. como es posible. mejor matarlos',\n",
       " 'ustedes si puden andar de chanceros pero cuidadito y seamos nosotras porque luego luego empiezan a mamar hijos de la chingada.',\n",
       " '@usuario jajjaja te digo esa madre si estÃ¡ buena ajjaja',\n",
       " 'odio los putos trÃ¡mites de titulaciÃ³n ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ pero me urge la precedula.',\n",
       " '@usuario no te equivocabas mi madre y tu tenÃ­an muchÃ­sima razÃ³n siempre es mejor lo que viene ðŸ’š']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asÃ­ deberÃ­a ser siempre para que se mueran a la verga',\n",
       " 'cada dÃ­a me siento como un perro ovejero tratando de cuidar sus ovejas vale madre pinches rateros pÃ³ngase a trabajar.ðŸ˜¤',\n",
       " 'hijo de tu puta madre nadamas te la pasas mamando pinche wilo de closet',\n",
       " 'soÃ±Ã© horrible espero no se cumpla putos temblores.',\n",
       " '@usuario mejor vas y la chingas tÃº veo tus publicaciones y solo denotan inconformidad como tÃ­pico ciudadano quejumbroso que no hace nada por cambiar',\n",
       " 'y quizÃ¡s este loca y quizÃ¡s me guste estarlo... #siempreneruda',\n",
       " 'me tienen hasta la verga con sus putos #boomerang pendejos y sin sentido.',\n",
       " 'cuando va a ser el dÃ­a que encuentre a alguien en quien pueda  confiar posts la re puta madre?',\n",
       " '#ahoritaestoypensando  como putas puede pasar esto?',\n",
       " '#todoibabienpero valio madre ya me atacÃ³ el insomnioðŸ™…']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_documents) == len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_documents(documents):\n",
    "    # tokenize each document\n",
    "    documents_tokenized = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer.tokenize(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "    # build dictionary of stopwords\n",
    "    stopwords_dict = {word:1 for word in stopwords.words('spanish')}\n",
    "    non_stop_documents = []\n",
    "    for doc in documents:\n",
    "        ndoc = []\n",
    "        for word in doc:\n",
    "            if stopwords_dict.get(word) == None:\n",
    "                ndoc.append(word)\n",
    "        non_stop_documents.append(ndoc)\n",
    "    \n",
    "    return non_stop_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = process_documents(documents)\n",
    "val_documents = process_documents(val_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute information gain between each token and classes\n",
    "# store value in dictionary for each word\n",
    "# remove all the ones that are below threshold\n",
    "# assing indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "class BoWBuilder:\n",
    "    # UTILITIES\n",
    "    def get_dimensions(self):\n",
    "        return self.voc_index.keys()\n",
    "    \n",
    "    # INIT FUNCTIONS\n",
    "    def __init__(self):\n",
    "        # attributes\n",
    "        self.voc_index = {} \n",
    "        self.train_idf = None\n",
    "        self.T = 0\n",
    "        \n",
    "    def get_vocabulary(self, documents, T):\n",
    "        # get vocabulary\n",
    "        tokens = [token for doc in documents for token in doc]\n",
    "        vocabulary = FreqDist(tokens)\n",
    "        \n",
    "        self.T = min(T, len(vocabulary.keys()))\n",
    "        \n",
    "        # get most common words\n",
    "        limited_voc = vocabulary.most_common(T)\n",
    "        self.voc_index = {}\n",
    "        \n",
    "        # get index of words in matrix\n",
    "        for i, word_count in enumerate(limited_voc):\n",
    "            self.voc_index[word_count[0]] = i\n",
    "    \n",
    "    \n",
    "    # BUILD BOW MATRIX\n",
    "    def build_bow(self, documents, T=5000, voc_index=None, mode='train', weight_scheme='binary', normalize=False):\n",
    "        # get most common terms - training mode\n",
    "        if mode == 'train':\n",
    "            if voc_index==None:\n",
    "                self.get_vocabulary(documents, T) #use most common words as vocabulary\n",
    "            else:\n",
    "                # use vocabulary index sent as parameter. Usefull when performing a features reduction or working with n-grams\n",
    "                self.voc_index = voc_index\n",
    "                self.T = len(voc_index.keys())\n",
    "        \n",
    "        # use train_idf, testing mode\n",
    "        use_train_idf = mode != 'train'\n",
    "        \n",
    "        # get weights for matrix\n",
    "        if weight_scheme == 'tf':\n",
    "            bow = self.frequency_bow(documents)\n",
    "        elif weight_scheme == 'tf-idf':\n",
    "            # if documents!= None, use existing idf weights (val or test mode)\n",
    "            bow = self.frequency_bow(documents, use_idf=True, use_train_idf=use_train_idf)\n",
    "        else:\n",
    "            bow = self.binary_bow(documents)\n",
    "        \n",
    "        # normalize if necessary\n",
    "        if normalize:\n",
    "            norm = np.linalg.norm(bow, axis=1)\n",
    "            # Add 1 if norm == 0 to avoid division by 0. --  Increase 1 dimension for broadcast \n",
    "            bow = bow / (norm + (norm==0 + 0.0))[:, np.newaxis]\n",
    "        \n",
    "        return bow\n",
    "            \n",
    "    # WEIGHT SCHEMES\n",
    "    def binary_bow(self, documents):\n",
    "        N = len(documents)\n",
    "        T = self.T\n",
    "        \n",
    "        bow = np.zeros((N, T))\n",
    "        for i, doc in enumerate(documents):\n",
    "            for word in doc:\n",
    "                j = self.voc_index.get(word)\n",
    "                if j != None:\n",
    "                    bow[i, j] = 1 \n",
    "        \n",
    "        return bow\n",
    "    \n",
    "    def frequency_bow(self, documents, use_idf=False, use_train_idf=False):\n",
    "        N = len(documents)\n",
    "        T = self.T\n",
    "        bow = np.zeros((N, T))\n",
    "        \n",
    "        # tf scheme\n",
    "        for i, doc in enumerate(documents):\n",
    "            for word in doc:\n",
    "                j = self.voc_index.get(word)\n",
    "                if j != None:\n",
    "                    bow[i, j] += 1 \n",
    "        \n",
    "        # tf-idf scheme\n",
    "        if use_idf:\n",
    "            if not use_train_idf:\n",
    "                # calculate idf for first time (training mode)\n",
    "                self.train_idf = np.sum(bow>0, axis=0)\n",
    "            \n",
    "            idf = self.train_idf     \n",
    "            bow = np.log(bow + 1) * np.log(N/idf)\n",
    "            \n",
    "        return bow        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bolsas de Palabras, Bigramas y Emociones (40pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Model and Labels (EXECUTE FIRST!!)</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "\n",
    "def get_model(max_iter=1000):\n",
    "    parameters = {'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]}\n",
    "    svr = svm.LinearSVC(class_weight='balanced', max_iter=max_iter)\n",
    "    grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring='f1_macro', cv=5) \n",
    "    return grid\n",
    "\n",
    "def get_nl_model(max_iter=1000):\n",
    "    parameters = {'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]}\n",
    "    svr = svm.SVC(kernel='poly', class_weight='balanced', max_iter=max_iter)\n",
    "    grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring='f1_macro', cv=5) \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "y_train = list(map(int, labels))\n",
    "y_val = list(map(int, val_labels))\n",
    "\n",
    "# BoW Builder\n",
    "bow_builder = BoWBuilder()\n",
    "\n",
    "def train_and_test(model, bow, val_bow):\n",
    "    model.fit(bow, y_train)\n",
    "    pred = model.predict(val_bow)\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_val, pred, average='macro', pos_label=None)\n",
    "    a = accuracy_score(y_val, pred)\n",
    "    print(\" accuracy: \", a, \"\\n precision: \", p, \"\\n recall: \", r, \"\\n f_measure: \", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratio of positive labels: ', 0.3573232323232323)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ratio of positive labels: \", sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. EvaluÃ© BoW con pesado Binario </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8133116883116883 \n",
      " precision:  0.7958333333333334 \n",
      " recall:  0.807051746546588 \n",
      " f_measure:  0.8003421578491969\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='binary', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='binary', normalize=False)\n",
    "model = get_model(2000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. EvaluÃ© BoW con pesado de Frecuencia </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8181818181818182 \n",
      " precision:  0.800838198545479 \n",
      " recall:  0.8087827657200695 \n",
      " f_measure:  0.8043113228953936\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf', normalize=False)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. EvaluÃ© BoW con pesado tf-idf </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.6948051948051948 \n",
      " precision:  0.7023890612780342 \n",
      " recall:  0.5922730984668116 \n",
      " f_measure:  0.578082191780822\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf-idf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf-idf', normalize=False)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4. EvaluÃ© BoW con pesado Binario Normalizado l2 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7987012987012987 \n",
      " precision:  0.7809010396611475 \n",
      " recall:  0.7936694155941248 \n",
      " f_measure:  0.785650143678161\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='binary', normalize=True)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='binary', normalize=True)\n",
    "model = get_model(2000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5. EvaluÃ© BoW con pesado Frecuencia Normalizado l2 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7954545454545454 \n",
      " precision:  0.7775125144397381 \n",
      " recall:  0.7901268647274651 \n",
      " f_measure:  0.7821928879310345\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf', normalize=True)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf', normalize=True)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 6. EvaluÃ© BoW con pesado tf-idf Normalizado l2 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7256493506493507 \n",
      " precision:  0.7279454651501895 \n",
      " recall:  0.6438413673326202 \n",
      " f_measure:  0.6482200483214219\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=5000, weight_scheme='tf-idf', normalize=True)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=5000, mode='test', weight_scheme='tf-idf', normalize=True)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 7. Ponga una tabla comparativa a modo resumen con las 6 entradas anteriores. </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Tabla de Resultados </h2>**\n",
    "\n",
    "A continuaciÃ³n mostramos los resultados de la mÃ©trica de accuracy resumidos en una tabla para cada modelo.\n",
    "\n",
    "**No.** | **Modelo** | **accuracy** | **precision** | **recall**\n",
    " -------- |----| ---- | ---- | ----\n",
    "1 |`Binary` | 0.7881  | 0.5595 | 0.5600\n",
    "2 | `TF` | 0.7818 | 0.7726 | 0.7550\n",
    "3 | `TF-IDF` | 0.7407 | 0.7267 | 0.7086\n",
    "4 | `Normalized Binary` | 0.7564 | 0.7423 | 0.7221\n",
    "5 | `Normalized TF` | 0.9211 | 0.9060 | 0.8032\n",
    "5 | `Normalized TF-IDF` | 0.9211 | 0.9060 | 0.8032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 8. De las configuraciones anteriores elija la mejor y evalÃºela con mÃ¡s y menos tÃ©rminos\n",
    "(e.g., 1000 y 7000). Ponga una tabla dÃ³nde compare las tres configuraciones. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8214285714285714 \n",
      " precision:  0.8042776567822638 \n",
      " recall:  0.8133489757657315 \n",
      " f_measure:  0.8081605381470618\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=1000, weight_scheme='tf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=1000, mode='test', weight_scheme='tf', normalize=False)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8181818181818182 \n",
      " precision:  0.8008658008658008 \n",
      " recall:  0.8077591065410672 \n",
      " f_measure:  0.8039443023586246\n"
     ]
    }
   ],
   "source": [
    "# get bows, and model\n",
    "bow = bow_builder.build_bow(documents, T=7000, weight_scheme='tf', normalize=False)\n",
    "val_bow = bow_builder.build_bow(val_documents, T=7000, mode='test', weight_scheme='tf', normalize=False)\n",
    "model = get_model(15000)\n",
    "train_and_test(model, bow, val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 9. Utilice el recurso lÃ©xico del Consejo Nacional de InvestigaciÃ³n de CanadÃ¡ llamado\n",
    "\"EmoLex\" (https://www.saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) para\n",
    "construir una \"Bolsa de Emociones\" de los Tweets de agresividad (Debe usar EmoLex\n",
    "en EspaÃ±ol). Para esto, una estrategia sencilla serÃ­a enmascarar cada palabra con su\n",
    "emociÃ³n, y despuÃ©s construir la Bolsa de Emociones (BoE). </h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "emotion_data = pandas.read_excel('data/emolex.xlsx', usecols=\"CI,DB:DK\")\n",
    "\n",
    "emotion_dict = {}\n",
    "for i in range(len(emotion_data)):\n",
    "    row = emotion_data.loc[i]\n",
    "    mask = ''\n",
    "    for val in row[1:]:\n",
    "        mask+= str(val)\n",
    "    emotion_dict[row[0]] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1001001001'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_dict['disfrutar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_word(mask_dict, documents, ignore=True):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            mask = mask_dict.get(word)\n",
    "            if mask != None:\n",
    "                masked_doc.append(mask)\n",
    "            elif not ignore:\n",
    "                masked_doc.append(word)\n",
    "                \n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "emolex_documents = mask_word(emotion_dict, documents, ignore=False)\n",
    "emolex_val_documents = mask_word(emotion_dict, val_documents, ignore=False)\n",
    "\n",
    "# BoWBuilder object\n",
    "bow_builder = BoWBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "emolex_bow = bow_builder.build_bow(documents, T=2048, weight_scheme='binary')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, T=2048, mode='test', weight_scheme='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>10. EvalÃºa tÃº BoE clasificando con SVM. Ponga una tabla comparativa a modo de resumen\n",
    "con los tres pesados, normalize cada uno si lo cree conveniente. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7873376623376623 \n",
      " precision:  0.7682203389830509 \n",
      " recall:  0.7766640212553052 \n",
      " f_measure:  0.7717453717453717\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='binary')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='binary')\n",
    "model = get_model(5000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8003246753246753 \n",
      " precision:  0.7818834742918512 \n",
      " recall:  0.7898105655429419 \n",
      " f_measure:  0.7852910421749739\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf')\n",
    "model = get_model(13000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.702922077922078 \n",
      " precision:  0.6820700869449792 \n",
      " recall:  0.6262091255190182 \n",
      " f_measure:  0.6289779515585967\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf-idf')\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf-idf')\n",
    "model = get_model(15000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>Normalized</h4>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7873376623376623 \n",
      " precision:  0.7683629007158419 \n",
      " recall:  0.7776876804343076 \n",
      " f_measure:  0.7721578315804334\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='binary', normalize=True)\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='binary', normalize=True)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.788961038961039 \n",
      " precision:  0.7700032285717466 \n",
      " recall:  0.7789471262781362 \n",
      " f_measure:  0.7736932537529391\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf', normalize=True)\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf', normalize=True)\n",
    "model = get_model(5000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7159090909090909 \n",
      " precision:  0.6922965051237302 \n",
      " recall:  0.697704243009788 \n",
      " f_measure:  0.6945197754521986\n"
     ]
    }
   ],
   "source": [
    "emolex_bow = bow_builder.build_bow(emolex_documents, T=2048, weight_scheme='tf-idf', normalize=True)\n",
    "val_emolex_bow = bow_builder.build_bow(emolex_val_documents, mode='test', weight_scheme='tf-idf', normalize=True)\n",
    "model = get_model(13000)\n",
    "train_and_test(model, emolex_bow, val_emolex_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Tabla de Resultados </h2>**\n",
    "\n",
    "A continuaciÃ³n mostramos los resultados de la mÃ©trica de accuracy resumidos en una tabla para cada modelo.\n",
    "\n",
    "**No.** | **Modelo** | **accuracy** | **precision** | **recall**\n",
    " -------- |----| ---- | ---- | ----\n",
    "1 |`Binary` | 0.7881  | 0.5595 | 0.5600\n",
    "2 | `TF` | 0.7818 | 0.7726 | 0.7550\n",
    "3 | `TF-IDF` | 0.7407 | 0.7267 | 0.7086\n",
    "4 | `Normalized Binary` | 0.7564 | 0.7423 | 0.7221\n",
    "5 | `Normalized TF` | 0.9211 | 0.9060 | 0.8032\n",
    "5 | `Normalized TF-IDF` | 0.9211 | 0.9060 | 0.8032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Recurso LÃ­nguistico de Emociones Mexicano (30 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Utilice el recurso lÃ©xico llamado \"Spanish Emotion Lexicon (SEL)\" del Dr. Grigori Sidorov, profesor del Centro de InvestigaciÃ³n en ComputaciÃ³n (CIC) del Instituto PolitÃ©cnico Nacional (http://www.cic.ipn.mx/âˆ¼sidorov/), para enmascarar cada palabra con su emociÃ³n, y despuÃ©s construir la Bolsa de Emociones con algÃºn pesado (e.g., binario, tf, tfidf). Proponga alguna estrategia para incorporar el \"valor\" del \"Probability Factor of Affective use\" en su representaciÃ³n vectorial del documento. EvalÃºa y escribe una tabla comparativa a modo de resumen con al menos tres pesados: binario, frecuencia, tfidf. Normalize cada pesado segÃºn lo crea conveniente de acuerdo el experimento (1). </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "emotion_data = pandas.read_excel('data/SEL.xlsx', sheet_name=\"Hoja1\", usecols=\"B,C,D\")\n",
    "\n",
    "# build dictionary will all the words in sel resource\n",
    "sel_dict = {}\n",
    "for i in range(len(emotion_data)):\n",
    "    row = emotion_data.loc[i]\n",
    "    mask = row[2]\n",
    "    pfa = row[1]\n",
    "    sel_dict[row[0]] = (mask, pfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Enojo', 0.932)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_dict['odio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask word as a pair <word>-<emotion> \n",
    "# this is because we want to get specific pfa for each word with its emotion\n",
    "def mask_word_sel(mask_dict, documents, ignore=True):\n",
    "    masked_documents = []\n",
    "    for doc in documents:\n",
    "        masked_doc = []\n",
    "        for word in doc:\n",
    "            mask = mask_dict.get(word)\n",
    "            if mask != None:\n",
    "                masked_doc.append(word + \":::\" + mask[0])\n",
    "            elif not ignore:\n",
    "                masked_doc.append(word)\n",
    "        masked_documents.append(masked_doc)\n",
    "    \n",
    "    return masked_documents\n",
    "\n",
    "\n",
    "'''\n",
    "    -- build pfa matrix\n",
    "    * voc_index: dictionary with the column position of each word in the bow\n",
    "    * sel_dict: dictionary with the sel_dict resource to retrieve its pfa\n",
    "    * documents: masked documents by mask_word function\n",
    "    * return: pfa_matrix\n",
    "'''\n",
    "\n",
    "def get_pfa_matrix(voc_index, sel_dict, documents, default_weight=1):\n",
    "    pfa_matrix = np.ones((len(documents), len(voc_index))) * default_weight\n",
    "    for i, doc in enumerate(documents):\n",
    "        for word_emo in doc:\n",
    "            l_words = word_emo.split(\":::\")\n",
    "            #word, emo = l_words[0], l_words[1] if len(l_words) == 2 else word_emo, None\n",
    "            if len(l_words) == 2:\n",
    "                word, emo = l_words\n",
    "            else:\n",
    "                word = word_emo\n",
    "            emotion_pfa = sel_dict.get(word)\n",
    "            if emotion_pfa != None:\n",
    "                j = voc_index.get(word_emo)\n",
    "                if j != None:\n",
    "                    pfa_matrix[i, j] = emotion_pfa[1]\n",
    "    \n",
    "    return pfa_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente bloque construimos nuestra BoW enmascarando las palabras por el par \\<palabra\\>-\\<emociÃ³n\\> como se explicÃ³ anteriormente y luego realizamos la multiplicaciÃ³n de la BoW por su matriz PFA correspondiente. En este momento ignoramos aquellas palabras que no se encuentran en el recurso lÃ©xico SEL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.6201298701298701 \n",
      " precision:  0.5409413281753708 \n",
      " recall:  0.5241019978606674 \n",
      " f_measure:  0.5040667189628835\n"
     ]
    }
   ],
   "source": [
    "# build masked documents\n",
    "sel_documents = mask_word_sel(sel_dict, documents, ignore=True)\n",
    "val_sel_documents = mask_word_sel(sel_dict, val_documents, ignore=True)\n",
    "\n",
    "# bow builder object\n",
    "bow_builder = BoWBuilder()\n",
    "\n",
    "# build bow for masked documents and get its pfa matrix\n",
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='binary')\n",
    "sel_pfa = get_pfa_matrix(bow_builder.voc_index, sel_dict, sel_documents)\n",
    "\n",
    "# build bow for masked validation documents and get its pfa matrix\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='binary')\n",
    "val_sel_pfa = get_pfa_matrix(bow_builder.voc_index, sel_dict, val_sel_documents)\n",
    "\n",
    "# compute final bows by element-wise-multiplication (original_bow * pfa_matrix)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, veamos que sucede cuando no ignoramos las palabras que no aparecen en el recurso lÃ©xico SEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.810064935064935 \n",
      " precision:  0.7924202127659574 \n",
      " recall:  0.8035091956799283 \n",
      " f_measure:  0.796869847550922\n"
     ]
    }
   ],
   "source": [
    "# build masked documents\n",
    "sel_documents = mask_word_sel(sel_dict, documents, ignore=False)\n",
    "val_sel_documents = mask_word_sel(sel_dict, val_documents, ignore=False)\n",
    "\n",
    "# bow builder object\n",
    "bow_builder = BoWBuilder()\n",
    "\n",
    "# build bow for masked documents and get its pfa matrix\n",
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='binary')\n",
    "sel_pfa = get_pfa_matrix(bow_builder.voc_index, sel_dict, sel_documents, default_weight=1)\n",
    "\n",
    "# build bow for masked validation documents and get its pfa matrix\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='binary')\n",
    "val_sel_pfa = get_pfa_matrix(bow_builder.voc_index, sel_dict, val_sel_documents, default_weight=1)\n",
    "\n",
    "# compute final bows by element-wise-multiplication (original_bow * pfa_matrix)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el rendimiento es mucho mejor de manera general, todas las mÃ©tricas obtienen un aumento considerable. **Â¿QuÃ© puede estar sucediendo?**. Una hipÃ³tesis es que al enmascarar las palabras por su emociÃ³n y multiplicar estas por su PFA se intensifica o atenua sus pesos en funciÃ³n de que tan fuerte es la emociÃ³n. Pero, esto ya se hacÃ­a en el bloque anterior. Entonces, Â¿por quÃ© aquÃ­ tenemos una aumento considerable? Evidentemente las palabras ignoradas anteriormente aportan informaciÃ³n valiosa, y aunque podrÃ­a parecer contra intuitivo debido a que las palabras que en principio se ignoran se sospecha que tienen una emociÃ³n neutral o carecen de emociÃ³n, estas palabras pueden de cierta manera caracterizar una transitividad de la  emociÃ³n. Por ejemplo, consideremos el siguiente par de oraciones:\n",
    "\n",
    "1. Odio lo que ha sucedido.\n",
    "2. Te odio!\n",
    "\n",
    "Si observamos en el diccionario del recurso lÃ©xico SEL, podemos ver que la palabra odio aparece con una emociÃ³n de enojo y un PFA superior a 0.9. Sin embargo, al ignorar las demÃ¡s palabras omitimos informaciÃ³n importante, porque tal vez los tweets que tengan palabras como \"te\", \"le\", \"la\", \"lo\", etc. denotan que se hace referencia a una persona probablemente. Y estos tweets tal vez estan catalogados como tweets de agresividad. Mientras que los tweets que tienen la palabra \"odio\" pero no tienen las otras palabras tal vez solo expresan una emociÃ³n y no agresividad hacia alguien. Y el dataset puede contener casos que ejemplifican esto, entonces esta informaciÃ³n termina siendo relevante. \n",
    "\n",
    "El ejemplo de arriba puede no ser el mejor para el caso especÃ­fico de agresividad, pero ilustra el punto de porquÃ© esta informaciÃ³n puede ser valiosa aunque no se haga como tal una interpretaciÃ³n semÃ¡ntica, pero estadÃ­sticamente puede marcar una diferencia.\n",
    "\n",
    "Por lo tanto, decidimos en los siguientes bloques proseguir sin ignorar las palabras que no aparecen en el recurso lÃ©xico, y en general en todos se observÃ³ una mejora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8246753246753247 \n",
      " precision:  0.8077410100964826 \n",
      " recall:  0.815867867453389 \n",
      " f_measure:  0.8113002042205582\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf')\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf')\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/nlp/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.6948051948051948 \n",
      " precision:  0.7050264892104923 \n",
      " recall:  0.5912494392878093 \n",
      " f_measure:  0.5760743831905704\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf-idf')\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf-idf')\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(15000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8003246753246753 \n",
      " precision:  0.7825004406839415 \n",
      " recall:  0.7949288614379536 \n",
      " f_measure:  0.7871966341522327\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='binary', normalize=True)\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='binary', normalize=True)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7938311688311688 \n",
      " precision:  0.7750892060660125 \n",
      " recall:  0.7837491229886246 \n",
      " f_measure:  0.7787149787149787\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf', normalize=True)\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf', normalize=True)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.724025974025974 \n",
      " precision:  0.7243182599569332 \n",
      " recall:  0.6425819214887915 \n",
      " f_measure:  0.646827109864019\n"
     ]
    }
   ],
   "source": [
    "sel_bow = bow_builder.build_bow(sel_documents, T=5000, weight_scheme='tf-idf', normalize=True)\n",
    "val_sel_bow = bow_builder.build_bow(val_sel_documents, mode='test', weight_scheme='tf-idf', normalize=True)\n",
    "sel_bow = sel_bow * sel_pfa\n",
    "val_sel_bow = val_sel_bow * val_sel_pfa\n",
    "\n",
    "model = get_model(5000)\n",
    "train_and_test(model, sel_bow, val_sel_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Tabla de Resultados </h2>**\n",
    "\n",
    "A continuaciÃ³n mostramos los resultados de la mÃ©trica de accuracy resumidos en una tabla para cada modelo.\n",
    "\n",
    "**No.** | **Modelo** | **accuracy** | **precision** | **recall**\n",
    " -------- |----| ---- | ---- | ----\n",
    "1 |`Binary` | 0.7881  | 0.5595 | 0.5600\n",
    "2 | `TF` | 0.7818 | 0.7726 | 0.7550\n",
    "3 | `TF-IDF` | 0.7407 | 0.7267 | 0.7086\n",
    "4 | `Normalized Binary` | 0.7564 | 0.7423 | 0.7221\n",
    "5 | `Normalized TF` | 0.9211 | 0.9060 | 0.8032\n",
    "5 | `Normalized TF-IDF` | 0.9211 | 0.9060 | 0.8032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. En un comentario aparte, discuta sobre la estrategÃ­a que utilizÃ³ para incorporar el\n",
    "\"Probability Factor of Affective use\". No mÃ¡s de 5 renglones. </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se generÃ³ una matriz de pares de la forma \\<word\\>-\\<emotion\\> y con esto se construyo una bolsa de palabras. Esto con la intenciÃ³n de que palabras diferentes con la misma emociÃ³n generaran un par distinto para poder usar sus valores de PFA. Luego, se construyÃ³ una matriz de los valores de PFA para cada palabra que existÃ­a en el recurso lÃ©xico y se encontraba en los documentos. De esta manera tanto la BoW como la matriz PFA tiene las mismas dimensiones. Aquellas palabras que estaban en el recurso lÃ©xico se quedaron con un valor del parÃ¡metro **default_weight** en esta matriz. Finalmente se procediÃ³ a realizar una multiplicaciÃ³n punto a punto entre ambas matrices con el objetivo de que aquellas palabras que tengan un PFA bajo tengan menor relevancia que aquellas que tengan un PFA alto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4  Â¿Podemos mejorar con bigramas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Hacer un experimento dÃ³nde concatene una buena BoW segÃºn sus experimentos anteriores con otra BoW construida a partir de los 1000 bigramas mÃ¡s frecuentes. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for doc in documents for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words)\n",
    "\n",
    "#finder.apply_freq_filter(10)\n",
    "bigrams = finder.nbest(bigram_measures.pmi, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict = {}\n",
    "for i, bigram in enumerate(bigrams):\n",
    "    bigram_dict[bigram[0]+bigram[1]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_document(documents):\n",
    "    bigram_documents = []\n",
    "    for doc in documents:\n",
    "        bigram_doc = []\n",
    "        for i in range(len(doc)-1):\n",
    "            bigram_doc.append(doc[i] + doc[i+1])\n",
    "        bigram_documents.append(bigram_doc)\n",
    "    \n",
    "    return bigram_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_documents = build_bigram_document(documents)\n",
    "val_bigram_documents = build_bigram_document(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_builder = BoWBuilder()\n",
    "bigram_bow = bow_builder.build_bow(bigram_documents, voc_index=bigram_dict, weight_scheme='binary')\n",
    "val_bigram_bow = bow_builder.build_bow(val_bigram_documents, mode='test', weight_scheme='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_bow = np.column_stack((sel_bow, bigram_bow))\n",
    "val_bigram_bow = np.column_stack((val_sel_bow, val_bigram_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.8198051948051948 \n",
      " precision:  0.8026244649695877 \n",
      " recall:  0.8090185523848958 \n",
      " f_measure:  0.8055107561289881\n"
     ]
    }
   ],
   "source": [
    "model = get_model(5000)\n",
    "model.fit(bigram_bow, y_train)\n",
    "pred = model.predict(val_bigram_bow)\n",
    "p, r, f, _ = precision_recall_fscore_support(y_val, pred, average='macro', pos_label=None)\n",
    "a = accuracy_score(y_val, pred)\n",
    "print(\" accuracy: \", a, \"\\n precision: \", p, \"\\n recall: \", r, \"\\n f_measure: \", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. Hacer un experimento con las Bolsas de Emociones, Bolsa de Palabras y Bolsa de Bi-\n",
    "gramas; usted elige las dimensionalidades. Para construir la representaciÃ³n final del\n",
    "documento utilice la concatenaciÃ³n de las representaciones segÃºn sus observaciones\n",
    "(e.g., Bolsa de Palabras + Bolsa de Bigramas + Bolsa de Sentimientos de CanadÃ¡ + Bolsa\n",
    "de Sentimientos de Grigori), y alimÃ©ntelas a un SVM. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. Elabore conclusiones sobre toda esta Tarea, incluyendo observaciones, comentarios y\n",
    "posibles mejoras futuras. Discuta el comportamiento de la BoW de usar solo palabras\n",
    "a integrar bigramas, y luego a integrar todo Â¿ayudÃ³? o Â¿empeorÃ³?. Discuta tambiÃ©n\n",
    "brevemente el costo computacional de los experimentos Â¿ValiÃ³ la Pena tener todo?. Sea\n",
    "breve: todo en NO mÃ¡s de dos pÃ¡rrafos. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"hola como estas\", \n",
    "    \"hola mundo, voy a comer el mundo\",\n",
    "    \"hola me voy a ir\",\n",
    "    \"hola quiero comer\",\n",
    "    \"hola voy a comer hoy\",\n",
    "    \"hola el mundo esta mal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_object = BoW(docs)\n",
    "bow = bow_object.build_bow(T=3, weight_scheme='tf-idf', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.92909298, 0.36984623],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hola', 3), ('mundo', 3), ('voy', 3)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_object.get_dimensions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
