{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 - Juan Luis Baldelomar Cabrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os\n",
    "import random\n",
    "\n",
    "# NLP and numpy\n",
    "import nltk \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score as accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    documents.pop(-1)\n",
    "    labels.pop(-1)\n",
    "    file.close()\n",
    "    labels_file.close()\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_doc(doc:list, end=' ', stop=-1):\n",
    "    stop = len(doc) if stop is None else stop\n",
    "    for token in doc[:stop]:\n",
    "        print(token, end=end)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokenized_docs, n):\n",
    "    tokens = [token for doc in tokenized_docs for token in doc]\n",
    "    unique_tokens = FreqDist(tokens).most_common(n)\n",
    "    return [token for token, _ in unique_tokens]\n",
    "\n",
    "def word2ids(vocabulary):\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    \n",
    "    # build both dictionaries\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        word2id[word] = i\n",
    "        id2word[i] = word\n",
    "    \n",
    "    # add special tokens\n",
    "    n = len(word2id)\n",
    "    word2id['<s>']   = n \n",
    "    word2id['</s>']  = n + 1\n",
    "    word2id['<unk>'] = n + 2\n",
    "    id2word[n]       = '<s>'\n",
    "    id2word[n + 1]   = '</s>'\n",
    "    id2word[n + 2]   = '<unk>'\n",
    "    \n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram Builder Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation to Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['.', '...', ',', '!', '¡', '¿', '?', ';', ';', '\"', '|', '[', ']', '°', '(', ')', '*', '+', '/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramBuilder:\n",
    "    def __init__(self, tokenizer=None, embeddings=None, d_model=256, sos='<s>', eos='</s>', unk='<unk>', punctuation=punctuation, postprocess=None):\n",
    "        self.tokenizer = self.default_tokenizer() if tokenizer == None else tokenizer\n",
    "        self.embeddings = embeddings\n",
    "        self.d_model = d_model if embeddings is None else embeddings.d_model\n",
    "        # special symbols\n",
    "        self.SOS = sos\n",
    "        self.EOS = eos\n",
    "        self.UNK = unk\n",
    "        # vocabulary 2 id and viceversa\n",
    "        self.word2id  = None\n",
    "        self.id2word  = None\n",
    "        self.voc_size = 0\n",
    "        # post tokenization functions\n",
    "        self.punctuation = set(punctuation) if punctuation != None else None\n",
    "        self.postprocess = postprocess if postprocess is not None else lambda x : x\n",
    "        \n",
    "    def default_tokenizer(doc):\n",
    "        return TweetTokenizer().tokenize\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        return set(self.word2id.keys())\n",
    "    \n",
    "    def remove_punct(self, tokenized_documents):\n",
    "        if self.punctuation == None:\n",
    "            return tokenized_documents\n",
    "        else:\n",
    "            return [[token for token in doc if token not in self.punctuation] for doc in tokenized_documents]\n",
    "        \n",
    "    def get_ids(self, words:list):\n",
    "        # transform list of words to list of ids\n",
    "        unk_id = self.word2id.get(self.UNK, 0)\n",
    "        ids = [self.word2id.get(word, unk_id) for word in words]\n",
    "        return ids\n",
    "    \n",
    "    def __transform(self, tokenized_docs, start_padding:bool, end_padding:bool):\n",
    "        N = self.N\n",
    "        # docs and labels lists\n",
    "        ngram_docs, ngram_targs = [], []\n",
    "        # traverse each doc\n",
    "        for doc in tokenized_docs:\n",
    "            # add padding\n",
    "            doc = ([self.SOS]*(N - 1) if start_padding else []) + \\\n",
    "                    doc + ([self.EOS] if end_padding else [])\n",
    "            # get ids    \n",
    "            ids = self.get_ids(doc)\n",
    "            # traverse each word as center and build ngrams\n",
    "            for i in range(N-1, len(doc)):    \n",
    "                ngram_docs.append(ids[i-(N-1): i])\n",
    "                ngram_targs.append(ids[i])\n",
    "                \n",
    "        return np.array(ngram_docs), np.array(ngram_targs)\n",
    "    \n",
    "    def _tokenize(self, documents):\n",
    "        tokenized_docs = [self.tokenizer(doc.lower()) for doc in documents]\n",
    "        tokenized_docs = self.remove_punct(tokenized_docs)\n",
    "        tokenized_docs = self.postprocess(tokenized_docs)\n",
    "        return tokenized_docs\n",
    "    \n",
    "    def build_emb_matrix(self):\n",
    "        dim_v = len(self.word2id)\n",
    "        if self.embeddings is None:\n",
    "            self.emb_matrix = np.random.rand(dim_v, self.d_model)\n",
    "        else:\n",
    "            self.emb_matrix = np.random.rand(dim_v, self.d_model)\n",
    "            for word in self.word2id.keys():\n",
    "                if word in self.embeddings:\n",
    "                    self.emb_matrix = self.embeddings[word]\n",
    "                \n",
    "    def fit(self, documents, N, t=10000):\n",
    "        self.N = N\n",
    "        # tokenize documents\n",
    "        tokenized_docs = self._tokenize(documents)\n",
    "        \n",
    "        # get vocabulary and word2id and ids2word dicts\n",
    "        vocabulary = get_vocabulary(tokenized_docs, t-3)\n",
    "        self.word2id, self.id2word = word2ids(vocabulary)\n",
    "        self.voc_size = len(self.word2id)\n",
    "        self.build_emb_matrix()\n",
    "        \n",
    "        return self.__transform(tokenized_docs, start_padding=True, end_padding=True)\n",
    "    \n",
    "    def transform(self, documents: list[list or str], start_padding=True, end_padding=True):\n",
    "        # list of documents as strings\n",
    "        if type(documents[0]) is str:\n",
    "            # tokenize documents\n",
    "            tokenized_docs = self._tokenize(documents)\n",
    "            return self.__transform(tokenized_docs, start_padding, end_padding)\n",
    "        \n",
    "        # list of documents as list of tokens\n",
    "        elif type(documents[0]) is list:\n",
    "            return self.__transform(documents, start_padding, end_padding)\n",
    "        \n",
    "        print('[ERR]: documents should be list of strings or list of lists of tokens')\n",
    "        return None\n",
    "    \n",
    "    def inverse(self, docs_as_ids):\n",
    "        # empty list\n",
    "        if len(docs_as_ids) == 0:\n",
    "            return None\n",
    "        \n",
    "        # multiple docs\n",
    "        if type(docs_as_ids[0]) in (list, np.ndarray):\n",
    "            return [[self.id2word.get(tok_id) for tok_id in doc] \n",
    "                    for doc in docs_as_ids ]\n",
    "        # single doc\n",
    "        return [self.id2word.get(tok_id) for tok_id in docs_as_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder = NGramBuilder()\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)\n",
    "val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)\n",
    "ngram_builder.emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl </s> a la vga no seas mamón 45 \n"
     ]
    }
   ],
   "source": [
    "doc = ngram_builder.inverse(ngram_labels)\n",
    "print_doc(doc[:30])\n",
    "\n",
    "del(ngram_builder);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call after normal tokenization to get each word as a document, i.e <s> word1 </s>, <s> word2 </s>, ...\n",
    "def char_postprocess(documents):\n",
    "    return [[c for c in word] for doc in documents for word in doc]\n",
    "\n",
    "# tokenize documents char by char so you can add <s> and </s> at end of each doc\n",
    "def char_tokenizer(doc):\n",
    "    return [char for char in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'o', 'l', 'a', ' ', 'm', 'u', 'n', 'd', 'o']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tokenizer('hola mundo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ngram_builder = NGramBuilder(tokenizer=char_tokenizer, d_model=100, punctuation=punctuation)\n",
    "ngram_docs, ngram_labels = char_ngram_builder.fit(documents, N=6)\n",
    "val_ngram_docs, val_ngram_labels = char_ngram_builder.transform(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', '<s>', '<s>', '<s>', '<s>'],\n",
       " ['<s>', '<s>', '<s>', '<s>', 'l'],\n",
       " ['<s>', '<s>', '<s>', 'l', 'o'],\n",
       " ['<s>', '<s>', 'l', 'o', ' '],\n",
       " ['<s>', 'l', 'o', ' ', 'p']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ngram_builder.inverse(ngram_docs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490412, 5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl</s>a la vg\n"
     ]
    }
   ],
   "source": [
    "words = char_ngram_builder.inverse(ngram_labels)\n",
    "print_doc(words[:100], end='', stop=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(ngram_builder, N, train_docs, val_docs, batch_size=64, num_workers=2):\n",
    "    ngram_docs, ngram_labels = ngram_builder.fit(documents, N=N)\n",
    "    val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)\n",
    "    \n",
    "    train_ds = TensorDataset(torch.tensor(ngram_docs, dtype=torch.int64), torch.tensor(ngram_labels, dtype=torch.int64))\n",
    "    train_loader = DataLoader(train_ds, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    val_ds = TensorDataset(torch.tensor(val_ngram_docs, dtype=torch.int64), torch.tensor(val_ngram_labels, dtype=torch.int64))\n",
    "    val_loader = DataLoader(val_ds, shuffle=False, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    return train_ds, train_loader, val_ds, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioModel(nn.Module):\n",
    "    def __init__(self, N, voc_size, d_model, hidden_size=128, emb_mat=None, dropout=0.1):\n",
    "        \n",
    "        super(BengioModel, self).__init__()\n",
    "        # parameters\n",
    "        self.N           = N\n",
    "        self.d_model     = d_model\n",
    "        self.voc_size    = voc_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Matriz entrenable de embeddings, tamaño vocab_size x Ngram.d_model\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(emb_mat), freeze=False)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(d_model * (N-1), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, voc_size, bias=False)\n",
    "        \n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        # Calcula el embedding para cada palabra.\n",
    "        x = self.embeddings(input_seq)\n",
    "        x = x.view(-1, (self.N-1) * self.d_model)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logit):\n",
    "    probs = F.softmax(raw_logit.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(raw_logit):\n",
    "    probs = F.softmax(raw_logit.detach(), dim=1)\n",
    "    return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramNeuralModel:\n",
    "    def __init__(self, NGram: NGramBuilder, neuralModel:nn.Module):\n",
    "        self.model = neuralModel\n",
    "        self.NGram = NGram\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict(self, context:list, use_gpu=False):\n",
    "        context = self.NGram.get_ids(context)\n",
    "        context = torch.tensor([context])\n",
    "        if use_gpu:\n",
    "            context = context.cuda()\n",
    "            \n",
    "        logits = self.model(context)\n",
    "        cond_probs = get_probs(logits)\n",
    "        index = sample(cond_probs)\n",
    "        return self.NGram.inverse([index])[0]\n",
    "    \n",
    "    def estimate_prob(self, sequence:str, use_gpu=False, ret_probs=False, start_padding=False, end_padding=False):\n",
    "        # feed model and get probs\n",
    "        ngrams, targets = self.NGram.transform([sequence], start_padding, end_padding)\n",
    "        ngrams = torch.tensor(ngrams)\n",
    "        if use_gpu:\n",
    "            ngrams = ngrams.cuda()\n",
    "            \n",
    "        logits = self.model(ngrams)\n",
    "        probs  = get_probs(logits)\n",
    "        \n",
    "        # get prob for each context and target\n",
    "        num_target = [i for i in range(len(targets))]\n",
    "        cond_probs = probs[num_target, targets]\n",
    "        log_prob = np.sum(np.log(cond_probs))\n",
    "        return np.exp(log_prob) if ret_probs else log_prob\n",
    "        \n",
    "            \n",
    "    def generate_sequence(self, use_gpu=False, max_length=100):\n",
    "        sequence = ['<s>']*(self.NGram.N - 1)\n",
    "        context = [token for token in sequence]\n",
    "        while sequence[-1] != '</s>' and len(sequence) < max_length:\n",
    "            word = self.predict(context, use_gpu)\n",
    "            context.pop(0)\n",
    "            context.append(word)\n",
    "            sequence.append(word)\n",
    "            \n",
    "        return sequence\n",
    "    \n",
    "    def perplexity(self, test_set, use_gpu=False):\n",
    "        ngrams, targets = self.NGram.transform(test_set)\n",
    "        ngrams = torch.tensor(ngrams)\n",
    "        if use_gpu:\n",
    "            ngrams = ngrams.cuda()\n",
    "        logits = self.model(ngrams)\n",
    "        probs = get_probs(logits)\n",
    "        \n",
    "        # get cond probs and perplexity\n",
    "        num_target = [i for i in range(len(targets))]\n",
    "        cond_probs = probs[num_target, targets]\n",
    "        log_perp = np.sum(-np.log(cond_probs))     # log(1/cond_probs) = log(1) - log(cond_probs) = -log(cond_probs)\n",
    "        perp = np.exp(1/len(targets) * log_perp)   # 1/N = 1/len(targets)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data, model, gpu=False):\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data:\n",
    "            if gpu:\n",
    "                # move inputs to gpu\n",
    "                inputs = inputs.cuda()\n",
    "            \n",
    "            # compute output predictions    \n",
    "            output = model(inputs)\n",
    "            batch_preds = get_preds(output)\n",
    "            # append preds and targets\n",
    "            preds.append(batch_preds)\n",
    "            targets.append(labels.numpy())\n",
    "    \n",
    "    # remove batch dimension\n",
    "    preds = [p for batch_pred in preds for p in batch_pred]\n",
    "    targets = [t for batch_tar in targets for t in batch_tar]\n",
    "    return accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(state, path, val_acc, best_metric, override=False):\n",
    "    if val_acc > best_metric or override: \n",
    "        print('Storing best model to {0}. Current acc: {1}, last best metric: {2}'.format(path, val_acc, best_metric))\n",
    "        torch.save(state, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_state = torch.load('best_model')\n",
    "model.load_state_dict(load_state['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "voc_size = char_ngram_builder.voc_size\n",
    "N = char_ngram_builder.N\n",
    "d_model = char_ngram_builder.d_model\n",
    "\n",
    "# optimizer hyperparameters\n",
    "lr = 2.3e-1 \n",
    "epochs = 100\n",
    "patience = epochs//5\n",
    "\n",
    "# scheduler hyperparameters\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, emb_mat=char_ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'min',\n",
    "                patience = lr_patience,\n",
    "                verbose=True,\n",
    "                factor = lr_factor\n",
    "            )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.multiprocessing\n",
    "#torch.multiprocessing.set_sharing_strategy('file_descriptor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_loader, val_ds, val_loader = get_datasets(char_ngram_builder, 6, documents, val_documents, batch_size=256, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "train accuracy mean:  0.33339849439542163\n",
      "validation accuracy:  0.3890792049390152\n",
      "Storing best model to char_best_model. Current acc: 0.3890792049390152, last best metric: 0\n",
      "epoch:  2\n",
      "train accuracy mean:  0.41890894684298685\n",
      "validation accuracy:  0.42254555036892033\n",
      "Storing best model to char_best_model. Current acc: 0.42254555036892033, last best metric: 0.3890792049390152\n",
      "epoch:  3\n",
      "train accuracy mean:  0.43695091176809725\n",
      "validation accuracy:  0.4337637404005421\n",
      "Storing best model to char_best_model. Current acc: 0.4337637404005421, last best metric: 0.42254555036892033\n",
      "epoch:  4\n",
      "train accuracy mean:  0.44869194580824884\n",
      "validation accuracy:  0.4465253726848366\n",
      "Storing best model to char_best_model. Current acc: 0.4465253726848366, last best metric: 0.4337637404005421\n",
      "epoch:  5\n",
      "train accuracy mean:  0.4562218178011361\n",
      "validation accuracy:  0.453132058424936\n",
      "Storing best model to char_best_model. Current acc: 0.453132058424936, last best metric: 0.4465253726848366\n",
      "epoch:  6\n",
      "train accuracy mean:  0.46296103308430836\n",
      "validation accuracy:  0.4513062791748231\n",
      "epoch:  7\n",
      "train accuracy mean:  0.4666359083695441\n",
      "validation accuracy:  0.46199744014455657\n",
      "Storing best model to char_best_model. Current acc: 0.46199744014455657, last best metric: 0.453132058424936\n",
      "epoch:  8\n",
      "train accuracy mean:  0.4705645372353984\n",
      "validation accuracy:  0.4638608643276615\n",
      "Storing best model to char_best_model. Current acc: 0.4638608643276615, last best metric: 0.46199744014455657\n",
      "epoch:  9\n",
      "train accuracy mean:  0.47456442761688594\n",
      "validation accuracy:  0.4680206294232796\n",
      "Storing best model to char_best_model. Current acc: 0.4680206294232796, last best metric: 0.4638608643276615\n",
      "epoch:  10\n",
      "train accuracy mean:  0.47714567807204933\n",
      "validation accuracy:  0.4689241078150881\n",
      "Storing best model to char_best_model. Current acc: 0.4689241078150881, last best metric: 0.4680206294232796\n",
      "epoch:  11\n",
      "train accuracy mean:  0.4795546302252755\n",
      "validation accuracy:  0.46922526727902425\n",
      "Storing best model to char_best_model. Current acc: 0.46922526727902425, last best metric: 0.4689241078150881\n",
      "epoch:  12\n",
      "train accuracy mean:  0.4814036366036316\n",
      "validation accuracy:  0.4744579129649149\n",
      "Storing best model to char_best_model. Current acc: 0.4744579129649149, last best metric: 0.46922526727902425\n",
      "epoch:  13\n",
      "train accuracy mean:  0.4824096898971938\n",
      "validation accuracy:  0.4715968980575215\n",
      "epoch:  14\n",
      "train accuracy mean:  0.48437694392690683\n",
      "validation accuracy:  0.472632133714802\n",
      "epoch:  15\n",
      "train accuracy mean:  0.48511525684869156\n",
      "validation accuracy:  0.4736861918385785\n",
      "epoch:  16\n",
      "train accuracy mean:  0.4867076174624945\n",
      "validation accuracy:  0.4755872609546755\n",
      "Storing best model to char_best_model. Current acc: 0.4755872609546755, last best metric: 0.4744579129649149\n",
      "epoch:  17\n",
      "train accuracy mean:  0.4888308123604166\n",
      "validation accuracy:  0.4777141996687246\n",
      "Storing best model to char_best_model. Current acc: 0.4777141996687246, last best metric: 0.4755872609546755\n",
      "epoch:  18\n",
      "train accuracy mean:  0.48886646682465895\n",
      "validation accuracy:  0.47880590272549317\n",
      "Storing best model to char_best_model. Current acc: 0.47880590272549317, last best metric: 0.4777141996687246\n",
      "epoch:  19\n",
      "train accuracy mean:  0.4894424381432976\n",
      "validation accuracy:  0.4788811925914772\n",
      "Storing best model to char_best_model. Current acc: 0.4788811925914772, last best metric: 0.47880590272549317\n",
      "epoch:  20\n",
      "train accuracy mean:  0.49065051759661604\n",
      "validation accuracy:  0.48068814937509413\n",
      "Storing best model to char_best_model. Current acc: 0.48068814937509413, last best metric: 0.4788811925914772\n",
      "epoch:  21\n",
      "train accuracy mean:  0.4917695083021799\n",
      "validation accuracy:  0.4802740551121819\n",
      "epoch:  22\n",
      "train accuracy mean:  0.4922743603437394\n",
      "validation accuracy:  0.47876825779250115\n",
      "epoch:  23\n",
      "train accuracy mean:  0.49280709114191384\n",
      "validation accuracy:  0.48386914621291977\n",
      "Storing best model to char_best_model. Current acc: 0.48386914621291977, last best metric: 0.48068814937509413\n",
      "epoch:  24\n",
      "train accuracy mean:  0.4939997614185318\n",
      "validation accuracy:  0.4849232043366963\n",
      "Storing best model to char_best_model. Current acc: 0.4849232043366963, last best metric: 0.48386914621291977\n",
      "epoch:  25\n",
      "train accuracy mean:  0.4944942300456377\n",
      "validation accuracy:  0.4814786929679265\n",
      "epoch:  26\n",
      "train accuracy mean:  0.49503364605585765\n",
      "validation accuracy:  0.48345505195000754\n",
      "epoch:  27\n",
      "train accuracy mean:  0.4950621411795407\n",
      "validation accuracy:  0.4835679867489836\n",
      "epoch:  28\n",
      "train accuracy mean:  0.4966401831122251\n",
      "validation accuracy:  0.4840197259448878\n",
      "epoch:  29\n",
      "train accuracy mean:  0.496077297740569\n",
      "validation accuracy:  0.48513025146815236\n",
      "Storing best model to char_best_model. Current acc: 0.48513025146815236, last best metric: 0.4849232043366963\n",
      "epoch:  30\n",
      "train accuracy mean:  0.4969346169193329\n",
      "validation accuracy:  0.4859584399939768\n",
      "Storing best model to char_best_model. Current acc: 0.4859584399939768, last best metric: 0.48513025146815236\n",
      "epoch:  31\n",
      "train accuracy mean:  0.4976390770652279\n",
      "validation accuracy:  0.4855443457310646\n",
      "epoch:  32\n",
      "train accuracy mean:  0.49733828993603435\n",
      "validation accuracy:  0.48441499774130403\n",
      "epoch:  33\n",
      "train accuracy mean:  0.49749361443717527\n",
      "validation accuracy:  0.48528083120012044\n",
      "epoch:  34\n",
      "train accuracy mean:  0.49784209889486336\n",
      "validation accuracy:  0.48674898358680924\n",
      "Storing best model to char_best_model. Current acc: 0.48674898358680924, last best metric: 0.4859584399939768\n",
      "epoch:  35\n",
      "train accuracy mean:  0.4985077753283245\n",
      "validation accuracy:  0.4857325703960247\n",
      "epoch:  36\n",
      "train accuracy mean:  0.49857405849456227\n",
      "validation accuracy:  0.4868619183857853\n",
      "Storing best model to char_best_model. Current acc: 0.4868619183857853, last best metric: 0.48674898358680924\n",
      "epoch:  37\n",
      "train accuracy mean:  0.49920066673848135\n",
      "validation accuracy:  0.48721954524920946\n",
      "Storing best model to char_best_model. Current acc: 0.48721954524920946, last best metric: 0.4868619183857853\n",
      "epoch:  38\n",
      "train accuracy mean:  0.4987258744447007\n",
      "validation accuracy:  0.4872571901822015\n",
      "Storing best model to char_best_model. Current acc: 0.4872571901822015, last best metric: 0.48721954524920946\n",
      "epoch:  39\n",
      "train accuracy mean:  0.499797025583216\n",
      "validation accuracy:  0.48857476283692214\n",
      "Storing best model to char_best_model. Current acc: 0.48857476283692214, last best metric: 0.4872571901822015\n",
      "epoch:  40\n",
      "train accuracy mean:  0.4997052817157838\n",
      "validation accuracy:  0.4861090197259449\n",
      "epoch:  41\n",
      "train accuracy mean:  0.500002086165461\n",
      "validation accuracy:  0.48641017918988105\n",
      "epoch:  42\n",
      "train accuracy mean:  0.5004758353765111\n",
      "validation accuracy:  0.4878783315765698\n",
      "epoch:  43\n",
      "train accuracy mean:  0.5001560356939118\n",
      "validation accuracy:  0.486297244390905\n",
      "epoch:  44\n",
      "train accuracy mean:  0.5017714863663397\n",
      "validation accuracy:  0.48618430959192893\n",
      "epoch:  45\n",
      "train accuracy mean:  0.5008981890566587\n",
      "validation accuracy:  0.48895121216684234\n",
      "Storing best model to char_best_model. Current acc: 0.48895121216684234, last best metric: 0.48857476283692214\n",
      "epoch:  46\n",
      "train accuracy mean:  0.5006409269250377\n",
      "validation accuracy:  0.48985469055865083\n",
      "Storing best model to char_best_model. Current acc: 0.48985469055865083, last best metric: 0.48895121216684234\n",
      "epoch:  47\n",
      "train accuracy mean:  0.5014212002202991\n",
      "validation accuracy:  0.4865042915223611\n",
      "epoch:  48\n",
      "train accuracy mean:  0.5014192562933922\n",
      "validation accuracy:  0.4869560307182653\n",
      "epoch:  49\n",
      "train accuracy mean:  0.5011885453585474\n",
      "validation accuracy:  0.48876298750188224\n",
      "epoch:  50\n",
      "train accuracy mean:  0.5012841770797931\n",
      "validation accuracy:  0.48627842192440895\n",
      "epoch:  51\n",
      "train accuracy mean:  0.5015760980057775\n",
      "validation accuracy:  0.4875395271796416\n",
      "epoch:  52\n",
      "train accuracy mean:  0.5020418344449434\n",
      "validation accuracy:  0.4894970636952266\n",
      "epoch:  53\n",
      "train accuracy mean:  0.5023923102423897\n",
      "validation accuracy:  0.4904946544195151\n",
      "Storing best model to char_best_model. Current acc: 0.4904946544195151, last best metric: 0.48985469055865083\n",
      "epoch:  54\n",
      "train accuracy mean:  0.5023883749757246\n",
      "validation accuracy:  0.4884618280379461\n",
      "epoch:  55\n",
      "train accuracy mean:  0.5027713759892217\n",
      "validation accuracy:  0.48979822315916277\n",
      "epoch:  56\n",
      "train accuracy mean:  0.5026259607740204\n",
      "validation accuracy:  0.48985469055865083\n",
      "epoch:  57\n",
      "train accuracy mean:  0.5032141171954654\n",
      "validation accuracy:  0.4908334588164433\n",
      "Storing best model to char_best_model. Current acc: 0.4908334588164433, last best metric: 0.4904946544195151\n",
      "epoch:  58\n",
      "train accuracy mean:  0.5032149232139389\n",
      "validation accuracy:  0.48898885709983436\n",
      "epoch:  59\n",
      "train accuracy mean:  0.5035509380917124\n",
      "validation accuracy:  0.49028760728805904\n",
      "epoch:  60\n",
      "train accuracy mean:  0.5033812949094528\n",
      "validation accuracy:  0.4924145460021081\n",
      "Storing best model to char_best_model. Current acc: 0.4924145460021081, last best metric: 0.4908334588164433\n",
      "epoch:  61\n",
      "train accuracy mean:  0.5035200249126086\n",
      "validation accuracy:  0.49128519801234755\n",
      "epoch:  62\n",
      "train accuracy mean:  0.5032069104420547\n",
      "validation accuracy:  0.48970411082668275\n",
      "epoch:  63\n",
      "train accuracy mean:  0.5037021850876341\n",
      "validation accuracy:  0.49128519801234755\n",
      "epoch:  64\n",
      "train accuracy mean:  0.5044261319154488\n",
      "validation accuracy:  0.48674898358680924\n",
      "epoch:  65\n",
      "train accuracy mean:  0.504592171721003\n",
      "validation accuracy:  0.490231139888571\n",
      "epoch:  66\n",
      "train accuracy mean:  0.5044486056070059\n",
      "validation accuracy:  0.49169929227525977\n",
      "epoch:  67\n",
      "train accuracy mean:  0.5044411617893383\n",
      "validation accuracy:  0.4916051799427797\n",
      "epoch:  68\n",
      "train accuracy mean:  0.5038003296900034\n",
      "validation accuracy:  0.49130402047884353\n",
      "epoch:  69\n",
      "train accuracy mean:  0.5045483148334708\n",
      "validation accuracy:  0.49009938262309893\n",
      "epoch:  70\n",
      "train accuracy mean:  0.5045014235234501\n",
      "validation accuracy:  0.48994880289113085\n",
      "epoch:  71\n",
      "train accuracy mean:  0.5044804670431374\n",
      "validation accuracy:  0.49168046980876373\n",
      "epoch:  72\n",
      "train accuracy mean:  0.5044638725451522\n",
      "validation accuracy:  0.4920192742056919\n",
      "epoch:  73\n",
      "train accuracy mean:  0.5046327097089383\n",
      "validation accuracy:  0.49175575967474777\n",
      "epoch:  74\n",
      "train accuracy mean:  0.5050815671730106\n",
      "validation accuracy:  0.4900805601566029\n",
      "epoch:  75\n",
      "train accuracy mean:  0.5051988191544885\n",
      "validation accuracy:  0.4910216834814034\n",
      "epoch:  76\n",
      "train accuracy mean:  0.5056349225615381\n",
      "validation accuracy:  0.48904532449932236\n",
      "epoch:  77\n",
      "train accuracy mean:  0.5058686679188718\n",
      "validation accuracy:  0.4917369372082518\n",
      "epoch:  78\n",
      "train accuracy mean:  0.5051375143376463\n",
      "validation accuracy:  0.4913981328113236\n",
      "epoch:  79\n",
      "train accuracy mean:  0.5052282625351993\n",
      "validation accuracy:  0.4929227525975004\n",
      "Storing best model to char_best_model. Current acc: 0.4929227525975004, last best metric: 0.4924145460021081\n",
      "epoch:  80\n",
      "train accuracy mean:  0.505687835303685\n",
      "validation accuracy:  0.4914357777443156\n",
      "epoch:  81\n",
      "train accuracy mean:  0.5058604654955818\n",
      "validation accuracy:  0.4904946544195151\n",
      "epoch:  82\n",
      "train accuracy mean:  0.506114171663349\n",
      "validation accuracy:  0.4925651257340762\n",
      "epoch:  83\n",
      "train accuracy mean:  0.5057667777012429\n",
      "validation accuracy:  0.492132209004668\n",
      "epoch:  84\n",
      "train accuracy mean:  0.5065763521386609\n",
      "validation accuracy:  0.49132284294533957\n",
      "epoch:  85\n",
      "train accuracy mean:  0.5067559520197116\n",
      "validation accuracy:  0.4929227525975004\n",
      "epoch:  86\n",
      "train accuracy mean:  0.5065044268431081\n",
      "validation accuracy:  0.49072052401746724\n",
      "epoch:  87\n",
      "train accuracy mean:  0.5072356752500364\n",
      "validation accuracy:  0.4904193645535311\n",
      "epoch:  88\n",
      "train accuracy mean:  0.5068205283232995\n",
      "validation accuracy:  0.4932050895949405\n",
      "Storing best model to char_best_model. Current acc: 0.4932050895949405, last best metric: 0.4929227525975004\n",
      "epoch:  89\n",
      "train accuracy mean:  0.5069926369738311\n",
      "validation accuracy:  0.4938826983887969\n",
      "Storing best model to char_best_model. Current acc: 0.4938826983887969, last best metric: 0.4932050895949405\n",
      "epoch:  90\n",
      "train accuracy mean:  0.5064495701740545\n",
      "validation accuracy:  0.492207498870652\n",
      "epoch:  91\n",
      "train accuracy mean:  0.5062293374793659\n",
      "validation accuracy:  0.4931862671284445\n",
      "epoch:  92\n",
      "train accuracy mean:  0.506584080433437\n",
      "validation accuracy:  0.4933744917934046\n",
      "epoch:  93\n",
      "train accuracy mean:  0.506698297992426\n",
      "validation accuracy:  0.4923580786026201\n",
      "epoch:  94\n",
      "train accuracy mean:  0.5070713897199834\n",
      "validation accuracy:  0.49066405661797924\n",
      "epoch:  95\n",
      "train accuracy mean:  0.5064148639668399\n",
      "validation accuracy:  0.49233925613612406\n",
      "epoch:  96\n",
      "train accuracy mean:  0.507358047819464\n",
      "validation accuracy:  0.49168046980876373\n",
      "epoch:  97\n",
      "train accuracy mean:  0.5072904845062388\n",
      "validation accuracy:  0.4910216834814034\n",
      "epoch:  98\n",
      "train accuracy mean:  0.5076096678217702\n",
      "validation accuracy:  0.49367565125734075\n",
      "epoch:  99\n",
      "train accuracy mean:  0.5075911768097295\n",
      "validation accuracy:  0.49216985393766\n",
      "epoch:  100\n",
      "train accuracy mean:  0.5072616100797446\n",
      "validation accuracy:  0.4938826983887969\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "            \n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # close for each step\n",
    "    \n",
    "    # get metric for training set\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "    \n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    \n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'char_best_model', val_acc, best_metric)\n",
    "    \n",
    "    # patience and last_metric and best_metric update\n",
    "    counter = counter + 1 if last_metric > best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "    last_metric = val_acc\n",
    "    \n",
    "    # check if patience run out\n",
    "    if counter > patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioModel(\n",
       "  (embeddings): Embedding(352, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=352, bias=False)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Last Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing best model to char_last_model. Current acc: 0, last best metric: 0.4938826983887969\n"
     ]
    }
   ],
   "source": [
    "# store model if necessary\n",
    "state = {\n",
    "            'epoch' : 100,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model': model.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_metric': best_metric\n",
    "        }\n",
    "checkpoint(state, 'char_last_model', 0, best_metric, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5069454901370276"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(val_loader, model, use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NGram Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioModel(\n",
       "  (embeddings): Embedding(352, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=352, bias=False)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGramModel = NGramNeuralModel(char_ngram_builder, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>oye el díleco fea por mierda que chingas a ✊ zya agarrasos @usuario la digar y vene un veo no que diga quet pasado nato dwegt a justo 😡😡😡😡\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>me voy a chingad y en it paraguador verga en el c_x mamen llagon ensables la kiros patos digo a meses su mise mucho\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu, max_length=300)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>el robo son por atuger mañana de puto en no dejones puto así solo madre tonen no de verga que te wará el arme este el tiemposin no lo tembici el poctando maricónsiso me de que marica de verga de páginas del my pagudite y ella a sentalya jajajajajajaja\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu, max_length=300)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Probability Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.2720284"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete a la verga', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.524015"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('a la vete verga', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17.122591"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('esos hijos de la chingada', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-21.678864"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('esos chingada de los hijos', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.1125507"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('estuvieron', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.756233"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('estuveiron', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.3073807"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete alv', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.073044"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete avl', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.13043"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('veet avl', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.349459063337847"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.perplexity(val_documents, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self, filename):\n",
    "        self.embeddings = {}\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                values = line.split()\n",
    "                word, rep = values[0], np.array(list(map(float, values[1:])))\n",
    "                self.embeddings[word] = rep\n",
    "                \n",
    "        self.d_model = len(list(self.embeddings.values())[0])\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.embeddings[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings('data/word2vec_col.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "            \n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # close for each step\n",
    "    \n",
    "    # get metric for training set\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "    \n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    \n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'char_best_model', val_acc, best_metric)\n",
    "    \n",
    "    # patience and last_metric and best_metric update\n",
    "    counter = counter + 1 if last_metric > best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "    last_metric = val_acc\n",
    "    \n",
    "    # check if patience run out\n",
    "    if counter > patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distancia Coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_distance(data):\n",
    "    N = len(data)\n",
    "    distances = np.zeros((N, N))\n",
    "    magnitudes = np.linalg.norm(data, axis=1)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(i+1):\n",
    "            distances[i, j] = np.dot(data[i], data[j])/(magnitudes[i] * magnitudes[j])\n",
    "            if i != j:\n",
    "                distances[j, i] = distances[i, j]\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(dist_matrix, n):\n",
    "    N = len(dist_matrix)\n",
    "    \n",
    "    # get indexes of elements to be compared. dist_matrix should be symmetric, so we dont need to consider each pair of distances twice\n",
    "    indexes = [(i,j) for i in range(N) for j in range(i+1) if i!=j]\n",
    "\n",
    "    # get x and y indexes\n",
    "    x_indexes = tuple([ind[0] for ind in indexes])\n",
    "    y_indexes = tuple([ind[1] for ind in indexes])\n",
    "    \n",
    "    # get values of matrix\n",
    "    row_max = dist_matrix[x_indexes, y_indexes]\n",
    "    \n",
    "    # desc sort elements retrieved and get their positions\n",
    "    max_elements = np.flip(np.argsort(row_max))[:n]\n",
    "    \n",
    "    # return indexes in positions retrieved in previous step\n",
    "    return [indexes[max_index] for max_index in max_elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Most Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = cos_distance(model.embeddings.weight.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = get_most_similar(dist_matrix, 10)\n",
    "similar = [list(pair) for pair in similar]\n",
    "ngram_builder.inverse(similar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
