{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 - Juan Luis Baldelomar Cabrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os\n",
    "import random\n",
    "\n",
    "# NLP and numpy\n",
    "import nltk \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score as accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    documents.pop(-1)\n",
    "    labels.pop(-1)\n",
    "    file.close()\n",
    "    labels_file.close()\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_doc(doc:list, end=' ', stop=-1):\n",
    "    stop = len(doc) if stop is None else stop\n",
    "    for token in doc[:stop]:\n",
    "        print(token, end=end)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokenized_docs, n):\n",
    "    tokens = [token for doc in tokenized_docs for token in doc]\n",
    "    unique_tokens = FreqDist(tokens).most_common(n)\n",
    "    return [token for token, _ in unique_tokens]\n",
    "\n",
    "def word2ids(vocabulary):\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    \n",
    "    # build both dictionaries\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        word2id[word] = i\n",
    "        id2word[i] = word\n",
    "    \n",
    "    # add special tokens\n",
    "    n = len(word2id)\n",
    "    word2id['<s>']   = n \n",
    "    word2id['</s>']  = n + 1\n",
    "    word2id['<unk>'] = n + 2\n",
    "    id2word[n]       = '<s>'\n",
    "    id2word[n + 1]   = '</s>'\n",
    "    id2word[n + 2]   = '<unk>'\n",
    "    \n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram Builder Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation to Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['.', '...', ',', '!', 'Â¡', 'Â¿', '?', ';', ';', '\"', '|', '[', ']', 'Â°', '(', ')', '*', '+', '/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramBuilder:\n",
    "    def __init__(self, tokenizer=None, embeddings=None, d_model=256, sos='<s>', eos='</s>', unk='<unk>', punctuation=punctuation, postprocess=None):\n",
    "        self.tokenizer = self.default_tokenizer() if tokenizer == None else tokenizer\n",
    "        self.embeddings = embeddings\n",
    "        self.d_model = d_model if embeddings is None else embeddings.d_model\n",
    "        # special symbols\n",
    "        self.SOS = sos\n",
    "        self.EOS = eos\n",
    "        self.UNK = unk\n",
    "        # vocabulary 2 id and viceversa\n",
    "        self.word2id  = None\n",
    "        self.id2word  = None\n",
    "        self.voc_size = 0\n",
    "        # post tokenization functions\n",
    "        self.punctuation = set(punctuation) if punctuation != None else None\n",
    "        self.postprocess = postprocess if postprocess is not None else lambda x : x\n",
    "        \n",
    "    def default_tokenizer(doc):\n",
    "        return TweetTokenizer().tokenize\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        return set(self.word2id.keys())\n",
    "    \n",
    "    def remove_punct(self, tokenized_documents):\n",
    "        if self.punctuation == None:\n",
    "            return tokenized_documents\n",
    "        else:\n",
    "            return [[token for token in doc if token not in self.punctuation] for doc in tokenized_documents]\n",
    "        \n",
    "    def get_ids(self, words:list):\n",
    "        # transform list of words to list of ids\n",
    "        unk_id = self.word2id.get(self.UNK, 0)\n",
    "        ids = [self.word2id.get(word, unk_id) for word in words]\n",
    "        return ids\n",
    "    \n",
    "    def __transform(self, tokenized_docs, start_padding:bool, end_padding:bool):\n",
    "        N = self.N\n",
    "        # docs and labels lists\n",
    "        ngram_docs, ngram_targs = [], []\n",
    "        # traverse each doc\n",
    "        for doc in tokenized_docs:\n",
    "            # add padding\n",
    "            doc = ([self.SOS]*(N - 1) if start_padding else []) + \\\n",
    "                    doc + ([self.EOS] if end_padding else [])\n",
    "            # get ids    \n",
    "            ids = self.get_ids(doc)\n",
    "            # traverse each word as center and build ngrams\n",
    "            for i in range(N-1, len(doc)):    \n",
    "                ngram_docs.append(ids[i-(N-1): i])\n",
    "                ngram_targs.append(ids[i])\n",
    "                \n",
    "        return np.array(ngram_docs), np.array(ngram_targs)\n",
    "    \n",
    "    def _tokenize(self, documents):\n",
    "        tokenized_docs = [self.tokenizer(doc.lower()) for doc in documents]\n",
    "        tokenized_docs = self.remove_punct(tokenized_docs)\n",
    "        tokenized_docs = self.postprocess(tokenized_docs)\n",
    "        return tokenized_docs\n",
    "    \n",
    "    def build_emb_matrix(self):\n",
    "        dim_v = len(self.word2id)\n",
    "        if self.embeddings is None:\n",
    "            self.emb_matrix = np.random.rand(dim_v, self.d_model)\n",
    "        else:\n",
    "            self.emb_matrix = np.random.rand(dim_v, self.d_model)\n",
    "            for word in self.word2id.keys():\n",
    "                if word in self.embeddings:\n",
    "                    self.emb_matrix = self.embeddings[word]\n",
    "                \n",
    "    def fit(self, documents, N, t=10000):\n",
    "        self.N = N\n",
    "        # tokenize documents\n",
    "        tokenized_docs = self._tokenize(documents)\n",
    "        \n",
    "        # get vocabulary and word2id and ids2word dicts\n",
    "        vocabulary = get_vocabulary(tokenized_docs, t-3)\n",
    "        self.word2id, self.id2word = word2ids(vocabulary)\n",
    "        self.voc_size = len(self.word2id)\n",
    "        self.build_emb_matrix()\n",
    "        \n",
    "        return self.__transform(tokenized_docs, start_padding=True, end_padding=True)\n",
    "    \n",
    "    def transform(self, documents: list[list or str], start_padding=True, end_padding=True):\n",
    "        # list of documents as strings\n",
    "        if type(documents[0]) is str:\n",
    "            # tokenize documents\n",
    "            tokenized_docs = self._tokenize(documents)\n",
    "            return self.__transform(tokenized_docs, start_padding, end_padding)\n",
    "        \n",
    "        # list of documents as list of tokens\n",
    "        elif type(documents[0]) is list:\n",
    "            return self.__transform(documents, start_padding, end_padding)\n",
    "        \n",
    "        print('[ERR]: documents should be list of strings or list of lists of tokens')\n",
    "        return None\n",
    "    \n",
    "    def inverse(self, docs_as_ids):\n",
    "        # empty list\n",
    "        if len(docs_as_ids) == 0:\n",
    "            return None\n",
    "        \n",
    "        # multiple docs\n",
    "        if type(docs_as_ids[0]) in (list, np.ndarray):\n",
    "            return [[self.id2word.get(tok_id) for tok_id in doc] \n",
    "                    for doc in docs_as_ids ]\n",
    "        # single doc\n",
    "        return [self.id2word.get(tok_id) for tok_id in docs_as_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder = NGramBuilder()\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)\n",
    "val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)\n",
    "ngram_builder.emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl </s> a la vga no seas mamÃ³n 45 \n"
     ]
    }
   ],
   "source": [
    "doc = ngram_builder.inverse(ngram_labels)\n",
    "print_doc(doc[:30])\n",
    "\n",
    "del(ngram_builder);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call after normal tokenization to get each word as a document, i.e <s> word1 </s>, <s> word2 </s>, ...\n",
    "def char_postprocess(documents):\n",
    "    return [[c for c in word] for doc in documents for word in doc]\n",
    "\n",
    "# tokenize documents char by char so you can add <s> and </s> at end of each doc\n",
    "def char_tokenizer(doc):\n",
    "    return [char for char in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'o', 'l', 'a', ' ', 'm', 'u', 'n', 'd', 'o']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tokenizer('hola mundo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ngram_builder = NGramBuilder(tokenizer=char_tokenizer, d_model=100, punctuation=punctuation)\n",
    "ngram_docs, ngram_labels = char_ngram_builder.fit(documents, N=6)\n",
    "val_ngram_docs, val_ngram_labels = char_ngram_builder.transform(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', '<s>', '<s>', '<s>', '<s>'],\n",
       " ['<s>', '<s>', '<s>', '<s>', 'l'],\n",
       " ['<s>', '<s>', '<s>', 'l', 'o'],\n",
       " ['<s>', '<s>', 'l', 'o', ' '],\n",
       " ['<s>', 'l', 'o', ' ', 'p']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ngram_builder.inverse(ngram_docs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490412, 5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl</s>a la vg\n"
     ]
    }
   ],
   "source": [
    "words = char_ngram_builder.inverse(ngram_labels)\n",
    "print_doc(words[:100], end='', stop=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(ngram_builder, N, train_docs, val_docs, batch_size=64, num_workers=2):\n",
    "    ngram_docs, ngram_labels = ngram_builder.fit(documents, N=N)\n",
    "    val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)\n",
    "    \n",
    "    train_ds = TensorDataset(torch.tensor(ngram_docs, dtype=torch.int64), torch.tensor(ngram_labels, dtype=torch.int64))\n",
    "    train_loader = DataLoader(train_ds, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    val_ds = TensorDataset(torch.tensor(val_ngram_docs, dtype=torch.int64), torch.tensor(val_ngram_labels, dtype=torch.int64))\n",
    "    val_loader = DataLoader(val_ds, shuffle=False, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    return train_ds, train_loader, val_ds, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioModel(nn.Module):\n",
    "    def __init__(self, N, voc_size, d_model, hidden_size=128, emb_mat=None, dropout=0.1):\n",
    "        \n",
    "        super(BengioModel, self).__init__()\n",
    "        # parameters\n",
    "        self.N           = N\n",
    "        self.d_model     = d_model\n",
    "        self.voc_size    = voc_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Matriz entrenable de embeddings, tamaÃ±o vocab_size x Ngram.d_model\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(emb_mat), freeze=False)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(d_model * (N-1), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, voc_size, bias=False)\n",
    "        \n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        # Calcula el embedding para cada palabra.\n",
    "        x = self.embeddings(input_seq)\n",
    "        x = x.view(-1, (self.N-1) * self.d_model)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logit):\n",
    "    probs = F.softmax(raw_logit.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(raw_logit):\n",
    "    probs = F.softmax(raw_logit.detach(), dim=1)\n",
    "    return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramNeuralModel:\n",
    "    def __init__(self, NGram: NGramBuilder, neuralModel:nn.Module):\n",
    "        self.model = neuralModel\n",
    "        self.NGram = NGram\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict(self, context:list, use_gpu=False):\n",
    "        context = self.NGram.get_ids(context)\n",
    "        context = torch.tensor([context])\n",
    "        if use_gpu:\n",
    "            context = context.cuda()\n",
    "            \n",
    "        logits = self.model(context)\n",
    "        cond_probs = get_probs(logits)\n",
    "        index = sample(cond_probs)\n",
    "        return self.NGram.inverse([index])[0]\n",
    "    \n",
    "    def estimate_prob(self, sequence:str, use_gpu=False, ret_probs=False, start_padding=False, end_padding=False):\n",
    "        # feed model and get probs\n",
    "        ngrams, targets = self.NGram.transform([sequence], start_padding, end_padding)\n",
    "        ngrams = torch.tensor(ngrams)\n",
    "        if use_gpu:\n",
    "            ngrams = ngrams.cuda()\n",
    "            \n",
    "        logits = self.model(ngrams)\n",
    "        probs  = get_probs(logits)\n",
    "        \n",
    "        # get prob for each context and target\n",
    "        num_target = [i for i in range(len(targets))]\n",
    "        cond_probs = probs[num_target, targets]\n",
    "        log_prob = np.sum(np.log(cond_probs))\n",
    "        return np.exp(log_prob) if ret_probs else log_prob\n",
    "        \n",
    "            \n",
    "    def generate_sequence(self, use_gpu=False, max_length=100):\n",
    "        sequence = ['<s>']*(self.NGram.N - 1)\n",
    "        context = [token for token in sequence]\n",
    "        while sequence[-1] != '</s>' and len(sequence) < max_length:\n",
    "            word = self.predict(context, use_gpu)\n",
    "            context.pop(0)\n",
    "            context.append(word)\n",
    "            sequence.append(word)\n",
    "            \n",
    "        return sequence\n",
    "    \n",
    "    def perplexity(self, test_set, use_gpu=False):\n",
    "        ngrams, targets = self.NGram.transform(test_set)\n",
    "        ngrams = torch.tensor(ngrams)\n",
    "        if use_gpu:\n",
    "            ngrams = ngrams.cuda()\n",
    "        logits = self.model(ngrams)\n",
    "        probs = get_probs(logits)\n",
    "        \n",
    "        # get cond probs and perplexity\n",
    "        num_target = [i for i in range(len(targets))]\n",
    "        cond_probs = probs[num_target, targets]\n",
    "        log_perp = np.sum(-np.log(cond_probs))     # log(1/cond_probs) = log(1) - log(cond_probs) = -log(cond_probs)\n",
    "        perp = np.exp(1/len(targets) * log_perp)   # 1/N = 1/len(targets)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data, model, gpu=False):\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data:\n",
    "            if gpu:\n",
    "                # move inputs to gpu\n",
    "                inputs = inputs.cuda()\n",
    "            \n",
    "            # compute output predictions    \n",
    "            output = model(inputs)\n",
    "            batch_preds = get_preds(output)\n",
    "            # append preds and targets\n",
    "            preds.append(batch_preds)\n",
    "            targets.append(labels.numpy())\n",
    "    \n",
    "    # remove batch dimension\n",
    "    preds = [p for batch_pred in preds for p in batch_pred]\n",
    "    targets = [t for batch_tar in targets for t in batch_tar]\n",
    "    return accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(state, path, val_acc, best_metric, override=False):\n",
    "    if val_acc > best_metric or override: \n",
    "        print('Storing best model to {0}. Current acc: {1}, last best metric: {2}'.format(path, val_acc, best_metric))\n",
    "        torch.save(state, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_state = torch.load('best_model')\n",
    "model.load_state_dict(load_state['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "voc_size = char_ngram_builder.voc_size\n",
    "N = char_ngram_builder.N\n",
    "d_model = char_ngram_builder.d_model\n",
    "\n",
    "# optimizer hyperparameters\n",
    "lr = 2.3e-1 \n",
    "epochs = 100\n",
    "patience = epochs//5\n",
    "\n",
    "# scheduler hyperparameters\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, emb_mat=char_ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'min',\n",
    "                patience = lr_patience,\n",
    "                verbose=True,\n",
    "                factor = lr_factor\n",
    "            )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.multiprocessing\n",
    "#torch.multiprocessing.set_sharing_strategy('file_descriptor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_loader, val_ds, val_loader = get_datasets(char_ngram_builder, 6, documents, val_documents, batch_size=256, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "train accuracy mean:  0.33339849439542163\n",
      "validation accuracy:  0.3890792049390152\n",
      "Storing best model to char_best_model. Current acc: 0.3890792049390152, last best metric: 0\n",
      "epoch:  2\n",
      "train accuracy mean:  0.41890894684298685\n",
      "validation accuracy:  0.42254555036892033\n",
      "Storing best model to char_best_model. Current acc: 0.42254555036892033, last best metric: 0.3890792049390152\n",
      "epoch:  3\n",
      "train accuracy mean:  0.43695091176809725\n",
      "validation accuracy:  0.4337637404005421\n",
      "Storing best model to char_best_model. Current acc: 0.4337637404005421, last best metric: 0.42254555036892033\n",
      "epoch:  4\n",
      "train accuracy mean:  0.44869194580824884\n",
      "validation accuracy:  0.4465253726848366\n",
      "Storing best model to char_best_model. Current acc: 0.4465253726848366, last best metric: 0.4337637404005421\n",
      "epoch:  5\n",
      "train accuracy mean:  0.4562218178011361\n",
      "validation accuracy:  0.453132058424936\n",
      "Storing best model to char_best_model. Current acc: 0.453132058424936, last best metric: 0.4465253726848366\n",
      "epoch:  6\n",
      "train accuracy mean:  0.46296103308430836\n",
      "validation accuracy:  0.4513062791748231\n",
      "epoch:  7\n",
      "train accuracy mean:  0.4666359083695441\n",
      "validation accuracy:  0.46199744014455657\n",
      "Storing best model to char_best_model. Current acc: 0.46199744014455657, last best metric: 0.453132058424936\n",
      "epoch:  8\n",
      "train accuracy mean:  0.4705645372353984\n",
      "validation accuracy:  0.4638608643276615\n",
      "Storing best model to char_best_model. Current acc: 0.4638608643276615, last best metric: 0.46199744014455657\n",
      "epoch:  9\n",
      "train accuracy mean:  0.47456442761688594\n",
      "validation accuracy:  0.4680206294232796\n",
      "Storing best model to char_best_model. Current acc: 0.4680206294232796, last best metric: 0.4638608643276615\n",
      "epoch:  10\n",
      "train accuracy mean:  0.47714567807204933\n",
      "validation accuracy:  0.4689241078150881\n",
      "Storing best model to char_best_model. Current acc: 0.4689241078150881, last best metric: 0.4680206294232796\n",
      "epoch:  11\n",
      "train accuracy mean:  0.4795546302252755\n",
      "validation accuracy:  0.46922526727902425\n",
      "Storing best model to char_best_model. Current acc: 0.46922526727902425, last best metric: 0.4689241078150881\n",
      "epoch:  12\n",
      "train accuracy mean:  0.4814036366036316\n",
      "validation accuracy:  0.4744579129649149\n",
      "Storing best model to char_best_model. Current acc: 0.4744579129649149, last best metric: 0.46922526727902425\n",
      "epoch:  13\n",
      "train accuracy mean:  0.4824096898971938\n",
      "validation accuracy:  0.4715968980575215\n",
      "epoch:  14\n",
      "train accuracy mean:  0.48437694392690683\n",
      "validation accuracy:  0.472632133714802\n",
      "epoch:  15\n",
      "train accuracy mean:  0.48511525684869156\n",
      "validation accuracy:  0.4736861918385785\n",
      "epoch:  16\n",
      "train accuracy mean:  0.4867076174624945\n",
      "validation accuracy:  0.4755872609546755\n",
      "Storing best model to char_best_model. Current acc: 0.4755872609546755, last best metric: 0.4744579129649149\n",
      "epoch:  17\n",
      "train accuracy mean:  0.4888308123604166\n",
      "validation accuracy:  0.4777141996687246\n",
      "Storing best model to char_best_model. Current acc: 0.4777141996687246, last best metric: 0.4755872609546755\n",
      "epoch:  18\n",
      "train accuracy mean:  0.48886646682465895\n",
      "validation accuracy:  0.47880590272549317\n",
      "Storing best model to char_best_model. Current acc: 0.47880590272549317, last best metric: 0.4777141996687246\n",
      "epoch:  19\n",
      "train accuracy mean:  0.4894424381432976\n",
      "validation accuracy:  0.4788811925914772\n",
      "Storing best model to char_best_model. Current acc: 0.4788811925914772, last best metric: 0.47880590272549317\n",
      "epoch:  20\n",
      "train accuracy mean:  0.49065051759661604\n",
      "validation accuracy:  0.48068814937509413\n",
      "Storing best model to char_best_model. Current acc: 0.48068814937509413, last best metric: 0.4788811925914772\n",
      "epoch:  21\n",
      "train accuracy mean:  0.4917695083021799\n",
      "validation accuracy:  0.4802740551121819\n",
      "epoch:  22\n",
      "train accuracy mean:  0.4922743603437394\n",
      "validation accuracy:  0.47876825779250115\n",
      "epoch:  23\n",
      "train accuracy mean:  0.49280709114191384\n",
      "validation accuracy:  0.48386914621291977\n",
      "Storing best model to char_best_model. Current acc: 0.48386914621291977, last best metric: 0.48068814937509413\n",
      "epoch:  24\n",
      "train accuracy mean:  0.4939997614185318\n",
      "validation accuracy:  0.4849232043366963\n",
      "Storing best model to char_best_model. Current acc: 0.4849232043366963, last best metric: 0.48386914621291977\n",
      "epoch:  25\n",
      "train accuracy mean:  0.4944942300456377\n",
      "validation accuracy:  0.4814786929679265\n",
      "epoch:  26\n",
      "train accuracy mean:  0.49503364605585765\n",
      "validation accuracy:  0.48345505195000754\n",
      "epoch:  27\n",
      "train accuracy mean:  0.4950621411795407\n",
      "validation accuracy:  0.4835679867489836\n",
      "epoch:  28\n",
      "train accuracy mean:  0.4966401831122251\n",
      "validation accuracy:  0.4840197259448878\n",
      "epoch:  29\n",
      "train accuracy mean:  0.496077297740569\n",
      "validation accuracy:  0.48513025146815236\n",
      "Storing best model to char_best_model. Current acc: 0.48513025146815236, last best metric: 0.4849232043366963\n",
      "epoch:  30\n",
      "train accuracy mean:  0.4969346169193329\n",
      "validation accuracy:  0.4859584399939768\n",
      "Storing best model to char_best_model. Current acc: 0.4859584399939768, last best metric: 0.48513025146815236\n",
      "epoch:  31\n",
      "train accuracy mean:  0.4976390770652279\n",
      "validation accuracy:  0.4855443457310646\n",
      "epoch:  32\n",
      "train accuracy mean:  0.49733828993603435\n",
      "validation accuracy:  0.48441499774130403\n",
      "epoch:  33\n",
      "train accuracy mean:  0.49749361443717527\n",
      "validation accuracy:  0.48528083120012044\n",
      "epoch:  34\n",
      "train accuracy mean:  0.49784209889486336\n",
      "validation accuracy:  0.48674898358680924\n",
      "Storing best model to char_best_model. Current acc: 0.48674898358680924, last best metric: 0.4859584399939768\n",
      "epoch:  35\n",
      "train accuracy mean:  0.4985077753283245\n",
      "validation accuracy:  0.4857325703960247\n",
      "epoch:  36\n",
      "train accuracy mean:  0.49857405849456227\n",
      "validation accuracy:  0.4868619183857853\n",
      "Storing best model to char_best_model. Current acc: 0.4868619183857853, last best metric: 0.48674898358680924\n",
      "epoch:  37\n",
      "train accuracy mean:  0.49920066673848135\n",
      "validation accuracy:  0.48721954524920946\n",
      "Storing best model to char_best_model. Current acc: 0.48721954524920946, last best metric: 0.4868619183857853\n",
      "epoch:  38\n",
      "train accuracy mean:  0.4987258744447007\n",
      "validation accuracy:  0.4872571901822015\n",
      "Storing best model to char_best_model. Current acc: 0.4872571901822015, last best metric: 0.48721954524920946\n",
      "epoch:  39\n",
      "train accuracy mean:  0.499797025583216\n",
      "validation accuracy:  0.48857476283692214\n",
      "Storing best model to char_best_model. Current acc: 0.48857476283692214, last best metric: 0.4872571901822015\n",
      "epoch:  40\n",
      "train accuracy mean:  0.4997052817157838\n",
      "validation accuracy:  0.4861090197259449\n",
      "epoch:  41\n",
      "train accuracy mean:  0.500002086165461\n",
      "validation accuracy:  0.48641017918988105\n",
      "epoch:  42\n",
      "train accuracy mean:  0.5004758353765111\n",
      "validation accuracy:  0.4878783315765698\n",
      "epoch:  43\n",
      "train accuracy mean:  0.5001560356939118\n",
      "validation accuracy:  0.486297244390905\n",
      "epoch:  44\n",
      "train accuracy mean:  0.5017714863663397\n",
      "validation accuracy:  0.48618430959192893\n",
      "epoch:  45\n",
      "train accuracy mean:  0.5008981890566587\n",
      "validation accuracy:  0.48895121216684234\n",
      "Storing best model to char_best_model. Current acc: 0.48895121216684234, last best metric: 0.48857476283692214\n",
      "epoch:  46\n",
      "train accuracy mean:  0.5006409269250377\n",
      "validation accuracy:  0.48985469055865083\n",
      "Storing best model to char_best_model. Current acc: 0.48985469055865083, last best metric: 0.48895121216684234\n",
      "epoch:  47\n",
      "train accuracy mean:  0.5014212002202991\n",
      "validation accuracy:  0.4865042915223611\n",
      "epoch:  48\n",
      "train accuracy mean:  0.5014192562933922\n",
      "validation accuracy:  0.4869560307182653\n",
      "epoch:  49\n",
      "train accuracy mean:  0.5011885453585474\n",
      "validation accuracy:  0.48876298750188224\n",
      "epoch:  50\n",
      "train accuracy mean:  0.5012841770797931\n",
      "validation accuracy:  0.48627842192440895\n",
      "epoch:  51\n",
      "train accuracy mean:  0.5015760980057775\n",
      "validation accuracy:  0.4875395271796416\n",
      "epoch:  52\n",
      "train accuracy mean:  0.5020418344449434\n",
      "validation accuracy:  0.4894970636952266\n",
      "epoch:  53\n",
      "train accuracy mean:  0.5023923102423897\n",
      "validation accuracy:  0.4904946544195151\n",
      "Storing best model to char_best_model. Current acc: 0.4904946544195151, last best metric: 0.48985469055865083\n",
      "epoch:  54\n",
      "train accuracy mean:  0.5023883749757246\n",
      "validation accuracy:  0.4884618280379461\n",
      "epoch:  55\n",
      "train accuracy mean:  0.5027713759892217\n",
      "validation accuracy:  0.48979822315916277\n",
      "epoch:  56\n",
      "train accuracy mean:  0.5026259607740204\n",
      "validation accuracy:  0.48985469055865083\n",
      "epoch:  57\n",
      "train accuracy mean:  0.5032141171954654\n",
      "validation accuracy:  0.4908334588164433\n",
      "Storing best model to char_best_model. Current acc: 0.4908334588164433, last best metric: 0.4904946544195151\n",
      "epoch:  58\n",
      "train accuracy mean:  0.5032149232139389\n",
      "validation accuracy:  0.48898885709983436\n",
      "epoch:  59\n",
      "train accuracy mean:  0.5035509380917124\n",
      "validation accuracy:  0.49028760728805904\n",
      "epoch:  60\n",
      "train accuracy mean:  0.5033812949094528\n",
      "validation accuracy:  0.4924145460021081\n",
      "Storing best model to char_best_model. Current acc: 0.4924145460021081, last best metric: 0.4908334588164433\n",
      "epoch:  61\n",
      "train accuracy mean:  0.5035200249126086\n",
      "validation accuracy:  0.49128519801234755\n",
      "epoch:  62\n",
      "train accuracy mean:  0.5032069104420547\n",
      "validation accuracy:  0.48970411082668275\n",
      "epoch:  63\n",
      "train accuracy mean:  0.5037021850876341\n",
      "validation accuracy:  0.49128519801234755\n",
      "epoch:  64\n",
      "train accuracy mean:  0.5044261319154488\n",
      "validation accuracy:  0.48674898358680924\n",
      "epoch:  65\n",
      "train accuracy mean:  0.504592171721003\n",
      "validation accuracy:  0.490231139888571\n",
      "epoch:  66\n",
      "train accuracy mean:  0.5044486056070059\n",
      "validation accuracy:  0.49169929227525977\n",
      "epoch:  67\n",
      "train accuracy mean:  0.5044411617893383\n",
      "validation accuracy:  0.4916051799427797\n",
      "epoch:  68\n",
      "train accuracy mean:  0.5038003296900034\n",
      "validation accuracy:  0.49130402047884353\n",
      "epoch:  69\n",
      "train accuracy mean:  0.5045483148334708\n",
      "validation accuracy:  0.49009938262309893\n",
      "epoch:  70\n",
      "train accuracy mean:  0.5045014235234501\n",
      "validation accuracy:  0.48994880289113085\n",
      "epoch:  71\n",
      "train accuracy mean:  0.5044804670431374\n",
      "validation accuracy:  0.49168046980876373\n",
      "epoch:  72\n",
      "train accuracy mean:  0.5044638725451522\n",
      "validation accuracy:  0.4920192742056919\n",
      "epoch:  73\n",
      "train accuracy mean:  0.5046327097089383\n",
      "validation accuracy:  0.49175575967474777\n",
      "epoch:  74\n",
      "train accuracy mean:  0.5050815671730106\n",
      "validation accuracy:  0.4900805601566029\n",
      "epoch:  75\n",
      "train accuracy mean:  0.5051988191544885\n",
      "validation accuracy:  0.4910216834814034\n",
      "epoch:  76\n",
      "train accuracy mean:  0.5056349225615381\n",
      "validation accuracy:  0.48904532449932236\n",
      "epoch:  77\n",
      "train accuracy mean:  0.5058686679188718\n",
      "validation accuracy:  0.4917369372082518\n",
      "epoch:  78\n",
      "train accuracy mean:  0.5051375143376463\n",
      "validation accuracy:  0.4913981328113236\n",
      "epoch:  79\n",
      "train accuracy mean:  0.5052282625351993\n",
      "validation accuracy:  0.4929227525975004\n",
      "Storing best model to char_best_model. Current acc: 0.4929227525975004, last best metric: 0.4924145460021081\n",
      "epoch:  80\n",
      "train accuracy mean:  0.505687835303685\n",
      "validation accuracy:  0.4914357777443156\n",
      "epoch:  81\n",
      "train accuracy mean:  0.5058604654955818\n",
      "validation accuracy:  0.4904946544195151\n",
      "epoch:  82\n",
      "train accuracy mean:  0.506114171663349\n",
      "validation accuracy:  0.4925651257340762\n",
      "epoch:  83\n",
      "train accuracy mean:  0.5057667777012429\n",
      "validation accuracy:  0.492132209004668\n",
      "epoch:  84\n",
      "train accuracy mean:  0.5065763521386609\n",
      "validation accuracy:  0.49132284294533957\n",
      "epoch:  85\n",
      "train accuracy mean:  0.5067559520197116\n",
      "validation accuracy:  0.4929227525975004\n",
      "epoch:  86\n",
      "train accuracy mean:  0.5065044268431081\n",
      "validation accuracy:  0.49072052401746724\n",
      "epoch:  87\n",
      "train accuracy mean:  0.5072356752500364\n",
      "validation accuracy:  0.4904193645535311\n",
      "epoch:  88\n",
      "train accuracy mean:  0.5068205283232995\n",
      "validation accuracy:  0.4932050895949405\n",
      "Storing best model to char_best_model. Current acc: 0.4932050895949405, last best metric: 0.4929227525975004\n",
      "epoch:  89\n",
      "train accuracy mean:  0.5069926369738311\n",
      "validation accuracy:  0.4938826983887969\n",
      "Storing best model to char_best_model. Current acc: 0.4938826983887969, last best metric: 0.4932050895949405\n",
      "epoch:  90\n",
      "train accuracy mean:  0.5064495701740545\n",
      "validation accuracy:  0.492207498870652\n",
      "epoch:  91\n",
      "train accuracy mean:  0.5062293374793659\n",
      "validation accuracy:  0.4931862671284445\n",
      "epoch:  92\n",
      "train accuracy mean:  0.506584080433437\n",
      "validation accuracy:  0.4933744917934046\n",
      "epoch:  93\n",
      "train accuracy mean:  0.506698297992426\n",
      "validation accuracy:  0.4923580786026201\n",
      "epoch:  94\n",
      "train accuracy mean:  0.5070713897199834\n",
      "validation accuracy:  0.49066405661797924\n",
      "epoch:  95\n",
      "train accuracy mean:  0.5064148639668399\n",
      "validation accuracy:  0.49233925613612406\n",
      "epoch:  96\n",
      "train accuracy mean:  0.507358047819464\n",
      "validation accuracy:  0.49168046980876373\n",
      "epoch:  97\n",
      "train accuracy mean:  0.5072904845062388\n",
      "validation accuracy:  0.4910216834814034\n",
      "epoch:  98\n",
      "train accuracy mean:  0.5076096678217702\n",
      "validation accuracy:  0.49367565125734075\n",
      "epoch:  99\n",
      "train accuracy mean:  0.5075911768097295\n",
      "validation accuracy:  0.49216985393766\n",
      "epoch:  100\n",
      "train accuracy mean:  0.5072616100797446\n",
      "validation accuracy:  0.4938826983887969\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "            \n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # close for each step\n",
    "    \n",
    "    # get metric for training set\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "    \n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    \n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'char_best_model', val_acc, best_metric)\n",
    "    \n",
    "    # patience and last_metric and best_metric update\n",
    "    counter = counter + 1 if last_metric > best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "    last_metric = val_acc\n",
    "    \n",
    "    # check if patience run out\n",
    "    if counter > patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioModel(\n",
       "  (embeddings): Embedding(352, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=352, bias=False)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Last Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing best model to char_last_model. Current acc: 0, last best metric: 0.4938826983887969\n"
     ]
    }
   ],
   "source": [
    "# store model if necessary\n",
    "state = {\n",
    "            'epoch' : 100,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model': model.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_metric': best_metric\n",
    "        }\n",
    "checkpoint(state, 'char_last_model', 0, best_metric, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5069454901370276"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(val_loader, model, use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NGram Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioModel(\n",
       "  (embeddings): Embedding(352, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=352, bias=False)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGramModel = NGramNeuralModel(char_ngram_builder, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>oye el dÃ­leco fea por mierda que chingas a âœŠ zya agarrasos @usuario la digar y vene un veo no que diga quet pasado nato dwegt a justo ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>me voy a chingad y en it paraguador verga en el c_x mamen llagon ensables la kiros patos digo a meses su mise mucho\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu, max_length=300)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>el robo son por atuger maÃ±ana de puto en no dejones puto asÃ­ solo madre tonen no de verga que te warÃ¡ el arme este el tiemposin no lo tembici el poctando maricÃ³nsiso me de que marica de verga de pÃ¡ginas del my pagudite y ella a sentalya jajajajajajaja\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu, max_length=300)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Probability Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.2720284"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete a la verga', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.524015"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('a la vete verga', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17.122591"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('esos hijos de la chingada', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-21.678864"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('esos chingada de los hijos', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.1125507"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('estuvieron', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.756233"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('estuveiron', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.3073807"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete alv', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.073044"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete avl', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.13043"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('veet avl', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.349459063337847"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.perplexity(val_documents, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self, filename):\n",
    "        self.embeddings = {}\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                values = line.split()\n",
    "                word, rep = values[0], np.array(list(map(float, values[1:])))\n",
    "                self.embeddings[word] = rep\n",
    "                \n",
    "        self.d_model = len(list(self.embeddings.values())[0])\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.embeddings[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings('data/word2vec_col.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "            \n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # close for each step\n",
    "    \n",
    "    # get metric for training set\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "    \n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    \n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'char_best_model', val_acc, best_metric)\n",
    "    \n",
    "    # patience and last_metric and best_metric update\n",
    "    counter = counter + 1 if last_metric > best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "    last_metric = val_acc\n",
    "    \n",
    "    # check if patience run out\n",
    "    if counter > patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distancia Coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_distance(data):\n",
    "    N = len(data)\n",
    "    distances = np.zeros((N, N))\n",
    "    magnitudes = np.linalg.norm(data, axis=1)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(i+1):\n",
    "            distances[i, j] = np.dot(data[i], data[j])/(magnitudes[i] * magnitudes[j])\n",
    "            if i != j:\n",
    "                distances[j, i] = distances[i, j]\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(dist_matrix, n):\n",
    "    N = len(dist_matrix)\n",
    "    \n",
    "    # get indexes of elements to be compared. dist_matrix should be symmetric, so we dont need to consider each pair of distances twice\n",
    "    indexes = [(i,j) for i in range(N) for j in range(i+1) if i!=j]\n",
    "\n",
    "    # get x and y indexes\n",
    "    x_indexes = tuple([ind[0] for ind in indexes])\n",
    "    y_indexes = tuple([ind[1] for ind in indexes])\n",
    "    \n",
    "    # get values of matrix\n",
    "    row_max = dist_matrix[x_indexes, y_indexes]\n",
    "    \n",
    "    # desc sort elements retrieved and get their positions\n",
    "    max_elements = np.flip(np.argsort(row_max))[:n]\n",
    "    \n",
    "    # return indexes in positions retrieved in previous step\n",
    "    return [indexes[max_index] for max_index in max_elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Most Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = cos_distance(model.embeddings.weight.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = get_most_similar(dist_matrix, 10)\n",
    "similar = [list(pair) for pair in similar]\n",
    "ngram_builder.inverse(similar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
