{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 - Juan Luis Baldelomar Cabrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    documents.pop(-1)\n",
    "    labels.pop(-1)\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_documents(documents, tokenizer=None):\n",
    "    # default tokenizer\n",
    "    tokenizer = TweetTokenizer().tokenize if tokenizer == None else tokenizer \n",
    "    documents_tokenized = []\n",
    "    # tokenize each document\n",
    "    for doc in documents:\n",
    "        documents_tokenized.append(tokenizer(doc.lower()))\n",
    "    return documents_tokenized\n",
    "\n",
    "def get_vocabulary(tokenized_docs, n):\n",
    "    tokens = [token for doc in tokenized_docs for token in doc]\n",
    "    unique_tokens = FreqDist(tokens).most_common(n)\n",
    "    return [token for token, _ in unique_tokens]\n",
    "\n",
    "def word2ids(vocabulary):\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    \n",
    "    # build both dictionaries\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        word2id[word] = i\n",
    "        id2word[i] = word\n",
    "    \n",
    "    # add special tokens\n",
    "    n = len(word2id)\n",
    "    word2id['<s>']   = n \n",
    "    word2id['</s>']  = n + 1\n",
    "    word2id['<unk>'] = n + 2\n",
    "    id2word[n]       = '<s>'\n",
    "    id2word[n + 1]   = '</s>'\n",
    "    id2word[n + 2]   = '<unk>'\n",
    "    \n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramBuilder:\n",
    "    def __init__(self, tokenizer=None, embeddings=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embeddings = embeddings\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "            \n",
    "    def fit(self, documents, N, t=10000):\n",
    "        self.N = N\n",
    "        # tokenize, get vocabulary and word2id and ids2word dicts\n",
    "        tokenized_docs = tokenize_documents(documents, self.tokenizer)\n",
    "        vocabulary = get_vocabulary(tokenized_docs, t)\n",
    "        self.word2id, self.id2word = word2ids(vocabulary)\n",
    "        return self._transform(tokenized_docs)\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        return set(self.word2id.keys())\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        # tokenize, get vocabulary and word2id and ids2word dicts\n",
    "        tokenized_docs = tokenize_documents(documents, self.tokenizer)\n",
    "        return self._transform(tokenized_docs)\n",
    "    \n",
    "    def inverse(self, docs_as_ids):\n",
    "        return [list(map(self.id2word.get, doc)) for doc in docs_as_ids]\n",
    "    \n",
    "    def _transform(self, tokenized_docs):\n",
    "        N = self.N\n",
    "        # docs and labels lists\n",
    "        ngram_docs, ngram_targs = [], []\n",
    "        # traverse each doc\n",
    "        for doc in tokenized_docs:\n",
    "            # add padding\n",
    "            doc = ['<s>']*(N - 1)  + doc + ['</s>']\n",
    "            # empty ngram and targets\n",
    "            ngram_doc, ngram_tar = [], []\n",
    "            # build list of ids from word2id dict      # DEFAULT VALUE of dict.get\n",
    "            ids = list(map(self.word2id.get, doc,  [self.word2id['<unk>']] * len(doc)))\n",
    "            \n",
    "            # traverse each word as center and build ngrams\n",
    "            for i in range(N-1, len(doc)):    \n",
    "                ngram_doc.append(ids[i-(N-1): i])\n",
    "                ngram_tar.append(ids[i])    \n",
    "            # append document and labels\n",
    "            ngram_docs.append(ngram_doc)\n",
    "            ngram_targs.append(ngram_tar)\n",
    "        \n",
    "        return ngram_docs, ngram_targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder()\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lo',\n",
       " 'peor',\n",
       " 'de',\n",
       " 'todo',\n",
       " 'es',\n",
       " 'que',\n",
       " 'no',\n",
       " 'me',\n",
       " 'dan',\n",
       " 'por',\n",
       " 'un',\n",
       " 'tiempo',\n",
       " 'y',\n",
       " 'luego',\n",
       " 'vuelven',\n",
       " 'estoy',\n",
       " 'hasta',\n",
       " 'la',\n",
       " 'verga',\n",
       " 'de',\n",
       " 'estl',\n",
       " '</s>']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder.inverse(ngram_labels)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
