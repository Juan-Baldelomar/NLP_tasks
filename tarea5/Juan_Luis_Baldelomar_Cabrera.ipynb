{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 - Juan Luis Baldelomar Cabrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os\n",
    "import random\n",
    "\n",
    "# NLP and numpy\n",
    "import nltk \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score as accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    documents.pop(-1)\n",
    "    labels.pop(-1)\n",
    "    file.close()\n",
    "    labels_file.close()\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_doc(doc:list, end=' ', stop=-1):\n",
    "    stop = len(doc) if stop is None else stop\n",
    "    for token in doc[:stop]:\n",
    "        print(token, end=end)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokenized_docs, n):\n",
    "    tokens = [token for doc in tokenized_docs for token in doc]\n",
    "    unique_tokens = FreqDist(tokens).most_common(n)\n",
    "    return [token for token, _ in unique_tokens]\n",
    "\n",
    "def word2ids(vocabulary):\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    \n",
    "    # build both dictionaries\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        word2id[word] = i\n",
    "        id2word[i] = word\n",
    "    \n",
    "    # add special tokens\n",
    "    n = len(word2id)\n",
    "    word2id['<s>']   = n \n",
    "    word2id['</s>']  = n + 1\n",
    "    word2id['<unk>'] = n + 2\n",
    "    id2word[n]       = '<s>'\n",
    "    id2word[n + 1]   = '</s>'\n",
    "    id2word[n + 2]   = '<unk>'\n",
    "    \n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram Builder Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation to Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['.', '...', ',', '!', '¡', '¿', '?', ';', ';', '\"', '|', '[', ']', '°', '(', ')', '*', '+', '/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramBuilder:\n",
    "    def __init__(self, tokenizer=None, embeddings=None, d_model=256, sos='<s>', eos='</s>', unk='<unk>', punctuation=punctuation, postprocess=None):\n",
    "        self.tokenizer = self.default_tokenizer() if tokenizer == None else tokenizer\n",
    "        self.embeddings = embeddings\n",
    "        self.d_model = d_model if embeddings is None else embeddings.d_model\n",
    "        # special symbols\n",
    "        self.SOS = sos\n",
    "        self.EOS = eos\n",
    "        self.UNK = unk\n",
    "        # vocabulary 2 id and viceversa\n",
    "        self.word2id  = None\n",
    "        self.id2word  = None\n",
    "        self.voc_size = 0\n",
    "        # post tokenization functions\n",
    "        self.punctuation = set(punctuation) if punctuation != None else None\n",
    "        self.postprocess = postprocess if postprocess is not None else lambda x : x\n",
    "        \n",
    "    def default_tokenizer(doc):\n",
    "        return TweetTokenizer().tokenize\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        return set(self.word2id.keys())\n",
    "    \n",
    "    def remove_punct(self, tokenized_documents):\n",
    "        if self.punctuation == None:\n",
    "            return tokenized_documents\n",
    "        else:\n",
    "            return [[token for token in doc if token not in self.punctuation] for doc in tokenized_documents]\n",
    "        \n",
    "    def get_ids(self, words:list):\n",
    "        # transform list of words to list of ids\n",
    "        unk_id = self.word2id.get(self.UNK, 0)\n",
    "        ids = [self.word2id.get(word, unk_id) for word in words]\n",
    "        return ids\n",
    "    \n",
    "    def _transform(self, tokenized_docs):\n",
    "        N = self.N\n",
    "        # docs and labels lists\n",
    "        ngram_docs, ngram_targs = [], []\n",
    "        # traverse each doc\n",
    "        for doc in tokenized_docs:\n",
    "            # add padding\n",
    "            doc = [self.SOS]*(N - 1)  + doc + [self.EOS]\n",
    "            ids = self.get_ids(doc)\n",
    "            # traverse each word as center and build ngrams\n",
    "            for i in range(N-1, len(doc)):    \n",
    "                ngram_docs.append(ids[i-(N-1): i])\n",
    "                ngram_targs.append(ids[i])\n",
    "                \n",
    "        return np.array(ngram_docs), np.array(ngram_targs)\n",
    "    \n",
    "    def _tokenize(self, documents):\n",
    "        tokenized_docs = [self.tokenizer(doc.lower()) for doc in documents]\n",
    "        tokenized_docs = self.remove_punct(tokenized_docs)\n",
    "        tokenized_docs = self.postprocess(tokenized_docs)\n",
    "        return tokenized_docs\n",
    "    \n",
    "    def build_emb_matrix(self):\n",
    "        dim_v = len(self.word2id)\n",
    "        if self.embeddings is None:\n",
    "            self.emb_matrix = np.random.rand(dim_v, self.d_model)\n",
    "        else:\n",
    "            self.emb_matrix = np.random.rand(dim_v, self.d_model)\n",
    "            for word in self.word2id.keys():\n",
    "                if word in self.embeddings:\n",
    "                    self.emb_matrix = self.embeddings[word]\n",
    "                \n",
    "    def fit(self, documents, N, t=10000):\n",
    "        self.N = N\n",
    "        # tokenize documents\n",
    "        tokenized_docs = self._tokenize(documents)\n",
    "        \n",
    "        # get vocabulary and word2id and ids2word dicts\n",
    "        vocabulary = get_vocabulary(tokenized_docs, t-3)\n",
    "        self.word2id, self.id2word = word2ids(vocabulary)\n",
    "        self.voc_size = len(self.word2id)\n",
    "        self.build_emb_matrix()\n",
    "        \n",
    "        return self._transform(tokenized_docs)\n",
    "    \n",
    "    def transform(self, documents: list[list or str]):\n",
    "        # list of documents as strings\n",
    "        if type(documents[0]) is str:\n",
    "            # tokenize documents\n",
    "            tokenized_docs = self._tokenize(documents)\n",
    "            return self._transform(tokenized_docs)\n",
    "        \n",
    "        # list of documents as list of tokens\n",
    "        elif type(documents[0]) is list:\n",
    "            return self._transform(documents)\n",
    "        \n",
    "        print('[ERR]: documents should be list of strings or list of lists of tokens')\n",
    "        return None\n",
    "    \n",
    "    def inverse(self, docs_as_ids):\n",
    "        # empty list\n",
    "        if len(docs_as_ids) == 0:\n",
    "            return None\n",
    "        \n",
    "        # multiple docs\n",
    "        if type(docs_as_ids[0]) in (list, np.ndarray):\n",
    "            return [[self.id2word.get(tok_id) for tok_id in doc] \n",
    "                    for doc in docs_as_ids ]\n",
    "        # single doc\n",
    "        return [self.id2word.get(tok_id) for tok_id in docs_as_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 256)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder = NGramBuilder()\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)\n",
    "val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)\n",
    "ngram_builder.emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl </s> a la vga no seas mamón 45 \n"
     ]
    }
   ],
   "source": [
    "doc = ngram_builder.inverse(ngram_labels)\n",
    "print_doc(doc[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9997, 9997, 9997],\n",
       "       [9997, 9997,   28],\n",
       "       [9997,   28,  282],\n",
       "       [  28,  282,    1],\n",
       "       [ 282,    1,   59],\n",
       "       [   1,   59,   17],\n",
       "       [  59,   17,    0],\n",
       "       [  17,    0,    6],\n",
       "       [   0,    6,    7],\n",
       "       [   6,    7,  315]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9997, 9997, 9997],\n",
       "        [9997, 9997,  670],\n",
       "        [9997,  670,   30],\n",
       "        [ 670,   30,  215]]),\n",
       " array([ 670,   30,  215, 9998]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder.transform(['hola como estas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9997, 9997, 9997],\n",
       "        [9997, 9997,  670],\n",
       "        [9997,  670,   30],\n",
       "        [ 670,   30,  215]]),\n",
       " array([ 670,   30,  215, 9998]))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder.transform([['hola', 'como', 'estas']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call after normal tokenization to get each word as a document, i.e <s> word1 </s>, <s> word2 </s>, ...\n",
    "def char_postprocess(documents):\n",
    "    return [[c for c in word] for doc in documents for word in doc]\n",
    "\n",
    "# tokenize documents char by char so you can add <s> and </s> at end of each doc\n",
    "def char_tokenizer(doc):\n",
    "    return [char for char in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'o', 'l', 'a', ' ', 'm', 'u', 'n', 'd', 'o']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tokenizer('hola mundo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder(postprocess=char_postprocess, punctuation=punctuation)\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)\n",
    "val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', '<s>', '<s>'],\n",
       " ['<s>', '<s>', 'l'],\n",
       " ['<s>', 'l', 'o'],\n",
       " ['<s>', '<s>', '<s>'],\n",
       " ['<s>', '<s>', 'p'],\n",
       " ['<s>', 'p', 'e'],\n",
       " ['p', 'e', 'o'],\n",
       " ['e', 'o', 'r'],\n",
       " ['<s>', '<s>', '<s>'],\n",
       " ['<s>', '<s>', 'd']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder.inverse(ngram_docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder(tokenizer=char_tokenizer, punctuation=punctuation)\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)\n",
    "val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490412, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[349, 349, 349],\n",
       "       [349, 349,  10],\n",
       "       [349,  10,   3],\n",
       "       [ 10,   3,   0],\n",
       "       [  3,   0,  14],\n",
       "       [  0,  14,   2],\n",
       "       [ 14,   2,   3],\n",
       "       [  2,   3,   5],\n",
       "       [  3,   5,   0],\n",
       "       [  5,   0,  11]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['l', 'o', ' ', 'p', 'e', 'o', 'r', ' ', 'd', 'e']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder.inverse(ngram_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n"
     ]
    }
   ],
   "source": [
    "if type(ngram_docs[:10][0]) in (list, np.ndarray):\n",
    "    print('TRUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo</s>peor</s>de</s>todo</s>es</s>que</s>no</s>me</s>dan</s>por</s>un</s>tiempo</s>y</s>luego</s>vuelven</s>estoy</s>hasta</s>la</s>verga</s>de</s>estl</s>a</s>la</s>vg\n"
     ]
    }
   ],
   "source": [
    "word = ngram_builder.inverse(ngram_labels)\n",
    "print_doc(word[:100], end='', stop=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[364, 364, 364],\n",
       "       [364, 364,   9],\n",
       "       [364,   9,   2],\n",
       "       [364, 364, 364],\n",
       "       [364, 364,  13],\n",
       "       [364,  13,   1],\n",
       "       [ 13,   1,   2],\n",
       "       [  1,   2,   4],\n",
       "       [364, 364, 364],\n",
       "       [364, 364,  10]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(ngram_builder, N, train_docs, val_docs, batch_size=64, num_workers=2):\n",
    "    ngram_docs, ngram_labels = ngram_builder.fit(documents, N=N)\n",
    "    val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)\n",
    "    \n",
    "    train_ds = TensorDataset(torch.tensor(ngram_docs, dtype=torch.int64), torch.tensor(ngram_labels, dtype=torch.int64))\n",
    "    train_loader = DataLoader(train_ds, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    val_ds = TensorDataset(torch.tensor(val_ngram_docs, dtype=torch.int64), torch.tensor(val_ngram_labels, dtype=torch.int64))\n",
    "    val_loader = DataLoader(val_ds, shuffle=False, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    return train_ds, train_loader, val_ds, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioModel(nn.Module):\n",
    "    def __init__(self, N, voc_size, d_model, hidden_size=128, emb_mat=None, dropout=0.1):\n",
    "        \n",
    "        super(BengioModel, self).__init__()\n",
    "        # parameters\n",
    "        self.N           = N\n",
    "        self.d_model     = d_model\n",
    "        self.voc_size    = voc_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Matriz entrenable de embeddings, tamaño vocab_size x Ngram.d_model\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(emb_mat), freeze=False)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(d_model * (N-1), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, voc_size, bias=False)\n",
    "        \n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        # Calcula el embedding para cada palabra.\n",
    "        x = self.embeddings(input_seq)\n",
    "        x = x.view(-1, (self.N-1) * self.d_model)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logit):\n",
    "    probs = F.softmax(raw_logit.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(raw_logit):\n",
    "    probs = F.softmax(raw_logit.detach(), dim=1)\n",
    "    return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = ngram_builder.voc_size\n",
    "N = ngram_builder.N\n",
    "d_model = ngram_builder.d_model\n",
    "\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, emb_mat=ngram_builder.emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targs = list(train_loader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7311, 1509, 8174, 9097, 5687, 7311, 3779, 1184, 6567,  844,  692,\n",
       "       2254,  778, 9837, 3779, 5687, 4098, 9837, 3779, 3779, 9837, 3823,\n",
       "        778, 7311, 1184, 1623, 6604,  692,  692,  692, 5158, 3885, 3885,\n",
       "       7311, 4908, 1961, 3885,  692,  692, 6122, 4098, 4098,  775, 7311,\n",
       "        692,  310, 6604,  778,  692, 6604, 9544, 3779, 3779,  692, 7311,\n",
       "       7311, 3779, 3779, 5687, 9837,  692,  692, 7788,  692])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_preds(model(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(ngram_builder.transform(['hola como estas'])[0])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(probs):\n",
    "    acc = np.cumsum(probs)       # build cumulative probability\n",
    "    val = np.random.uniform()    # get random number between [0, 1]\n",
    "    pos = np.argmax((val < acc)) # get the index of the word to sample\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramNeuralModel:\n",
    "    def __init__(self, NGram: NGramBuilder, neuralModel:nn.Module):\n",
    "        self.model = neuralModel\n",
    "        self.NGram = NGram\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict(self, context:list):\n",
    "        context = self.NGram.get_ids(context)\n",
    "        context = torch.tensor([context])\n",
    "        logits = self.model(context)\n",
    "        cond_probs = get_probs(logits)\n",
    "        index = sample(cond_probs)\n",
    "        return self.NGram.inverse([index])[0]\n",
    "    \n",
    "    def estimate_prob(self, sequence:str):\n",
    "        # feed model and get probs\n",
    "        ngrams, targets = self.NGram.transform([sequence])\n",
    "        ngrams = torch.tensor(ngrams)\n",
    "        logits = self.model(ngrams)\n",
    "        probs  = get_probs(logits)\n",
    "        \n",
    "        # get prob for each context and target\n",
    "        num_target = [i for i in range(len(targets))]\n",
    "        cond_probs = probs[num_target, targets]\n",
    "        log_prob = np.sum(np.log(cond_probs))\n",
    "        return np.exp(log_prob)\n",
    "        \n",
    "            \n",
    "    def generate_sequence(self):\n",
    "        sequence = ['<s>']*(self.NGram.N - 1)\n",
    "        context = [token for token in sequence]\n",
    "        while sequence[-1] != '</s>':\n",
    "            word = self.predict(context)\n",
    "            context.pop(0)\n",
    "            context.append(word)\n",
    "            sequence.append(word)\n",
    "            \n",
    "        return sequence\n",
    "    \n",
    "    def perplexity(self, test_set):\n",
    "        ngrams, targets = self.NGram.transform(test_set)\n",
    "        ngrams = torch.tensor(ngrams)\n",
    "        logits = self.model(ngrams)\n",
    "        probs = get_probs(logits)\n",
    "        \n",
    "        # get cond probs and perplexity\n",
    "        num_target = [i for i in range(len(targets))]\n",
    "        cond_probs = probs[num_target, targets]\n",
    "        log_perp = np.sum(-np.log(cond_probs))     # log(1/cond_probs) = log(1) - log(cond_probs) = -log(cond_probs)\n",
    "        perp = np.exp(1/len(targets) * log_perp)   # 1/N = 1/len(targets)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data, model, gpu=False):\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data:\n",
    "            if gpu:\n",
    "                # move inputs to gpu\n",
    "                inputs = inputs.cuda()\n",
    "            \n",
    "            # compute output predictions    \n",
    "            output = model(inputs)\n",
    "            batch_preds = get_preds(output)\n",
    "            # append preds and targets\n",
    "            preds.append(batch_preds)\n",
    "            targets.append(labels.numpy())\n",
    "    \n",
    "    # remove batch dimension\n",
    "    preds = [p for batch_pred in preds for p in batch_pred]\n",
    "    targets = [t for batch_tar in targets for t in batch_tar]\n",
    "    return accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(state, path, is_best):\n",
    "    if is_best:\n",
    "        torch.save(state, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_state = torch.load('best_model')\n",
    "model.load_state_dict(load_state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioModel(\n",
       "  (embeddings): Embedding(10000, 256)\n",
       "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10000, bias=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2.3e-1 \n",
    "epochs = 50\n",
    "patience = epochs//5\n",
    "\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, emb_mat=ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            'min',\n",
    "            patience = lr_patience,\n",
    "            verbose=True,\n",
    "            factor = lr_factor\n",
    "        )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', epoch)\n",
    "    epoch_metrics = []\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "            \n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # close for each step\n",
    "    \n",
    "    # get metric for training set\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "    \n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    \n",
    "    # store model if necessary\n",
    "    state = {\n",
    "            'epoch' : epoch + 1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model': model.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_metric': best_metric\n",
    "    }\n",
    "    checkpoint(state, 'best_model', val_acc>best_metric)\n",
    "    \n",
    "    # patience and last_metric and best_metric update\n",
    "    counter = counter + 1 if last_metric > best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "    last_metric = val_acc\n",
    "    \n",
    "    # check if patience run out\n",
    "    if counter > patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1217008797653959"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(val_loader, model, use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NGram Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGramModel = NGramNeuralModel(ngram_builder, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4464763e-05"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete a la verga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = NGramModel.generate_sequence()\n",
    "print_doc(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381.66231916274586"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.perplexity(val_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self, filename):\n",
    "        self.embeddings = {}\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                values = line.split()\n",
    "                word, rep = values[0], np.array(list(map(float, values[1:])))\n",
    "                self.embeddings[word] = rep\n",
    "                \n",
    "        self.d_model = len(list(self.embeddings.values())[0])\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.embeddings[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings('data/word2vec_col.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distancia Coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_distance(data):\n",
    "    N = len(data)\n",
    "    distances = np.zeros((N, N))\n",
    "    magnitudes = np.linalg.norm(data, axis=1)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(i+1):\n",
    "            distances[i, j] = np.dot(data[i], data[j])/(magnitudes[i] * magnitudes[j])\n",
    "            if i != j:\n",
    "                distances[j, i] = distances[i, j]\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(dist_matrix, n):\n",
    "    N = len(dist_matrix)\n",
    "    \n",
    "    # get indexes of elements to be compared. dist_matrix should be symmetric, so we dont need to consider each pair of distances twice\n",
    "    indexes = [(i,j) for i in range(N) for j in range(i+1) if i!=j]\n",
    "\n",
    "    # get x and y indexes\n",
    "    x_indexes = tuple([ind[0] for ind in indexes])\n",
    "    y_indexes = tuple([ind[1] for ind in indexes])\n",
    "    \n",
    "    # get values of matrix\n",
    "    row_max = dist_matrix[x_indexes, y_indexes]\n",
    "    \n",
    "    # desc sort elements retrieved and get their positions\n",
    "    max_elements = np.flip(np.argsort(row_max))[:n]\n",
    "    \n",
    "    # return indexes in positions retrieved in previous step\n",
    "    return [indexes[max_index] for max_index in max_elements]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
