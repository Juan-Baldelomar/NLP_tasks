{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 - Juan Luis Baldelomar Cabrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os\n",
    "import random\n",
    "\n",
    "# NLP and numpy\n",
    "import nltk \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# NGrams File\n",
    "from NGrams import NGramBuilder\n",
    "from NGrams import NGramNeuralModel\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score as accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    documents.pop(-1)\n",
    "    labels.pop(-1)\n",
    "    file.close()\n",
    "    labels_file.close()\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String and Doc Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_string(string):\n",
    "    return '\\033[1m' + string + '\\033[0m '\n",
    "\n",
    "def print_doc(doc:list, end=' ', stop=-1):\n",
    "    stop = len(doc) if stop is None else stop\n",
    "    for token in doc[:stop]:\n",
    "        print(token, end=end)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram Builder Class\n",
    "\n",
    "La clase para construir n-gramas se encuentra en el archivo NGrams.py. En esta también se encuentra la clase NGramNeuralModel que se encarga de mezclar las funcionalidades de un objeto NgramBuilder y el modelo neuronal entrenado. Entre estas funcionalidades se encuentra la estimación de probabilidades, generación de secuencias, muestreo de palabras, etc. A contitnuación tenemos unos ejemplos de como construir los ngramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 256)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder = NGramBuilder()\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)\n",
    "val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)\n",
    "ngram_builder.emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl </s> a la vga no seas mamón 45 \n"
     ]
    }
   ],
   "source": [
    "doc = ngram_builder.inverse(ngram_labels)\n",
    "print_doc(doc[:30])\n",
    "del(ngram_builder);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(ngram_builder, N, train_docs, val_docs, batch_size=64, num_workers=2):\n",
    "    ngram_docs, ngram_labels = ngram_builder.fit(documents, N=N)\n",
    "    val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)\n",
    "    \n",
    "    train_ds = TensorDataset(torch.tensor(ngram_docs, dtype=torch.int64), torch.tensor(ngram_labels, dtype=torch.int64))\n",
    "    train_loader = DataLoader(train_ds, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    val_ds = TensorDataset(torch.tensor(val_ngram_docs, dtype=torch.int64), torch.tensor(val_ngram_labels, dtype=torch.int64))\n",
    "    val_loader = DataLoader(val_ds, shuffle=False, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    return train_ds, train_loader, val_ds, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Syntactical and Morphological Structures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def get_perms(tokens):\n",
    "    perms = set(permutations(tokens))\n",
    "    return list(perms)\n",
    "\n",
    "def test_structures(tokens, ngram_model):\n",
    "    perms = get_perms(tokens)\n",
    "    likelihoods = [(ngram_model.estimate_prob(' '.join(perm), use_gpu=use_gpu), ' '.join(perm)) for perm in perms]\n",
    "    likelihoods = sorted(likelihoods, reverse=True)\n",
    "    for l, sentence in likelihoods:\n",
    "        print(sentence)\n",
    "        print('likelihood: ', l, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call after normal tokenization to get each word as a document, i.e <s> word1 </s>, <s> word2 </s>, ...\n",
    "def char_postprocess(documents):\n",
    "    return [[c for c in word] for doc in documents for word in doc]\n",
    "\n",
    "# tokenize documents char by char so you can add <s> and </s> at end of each doc\n",
    "def char_tokenizer(doc):\n",
    "    return [char for char in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ngram_builder = NGramBuilder(tokenizer=char_tokenizer, d_model=100)\n",
    "ngram_docs, ngram_labels = char_ngram_builder.fit(documents, N=6)\n",
    "val_ngram_docs, val_ngram_labels = char_ngram_builder.transform(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl</s>a la vg\n"
     ]
    }
   ],
   "source": [
    "words = char_ngram_builder.inverse(ngram_labels)\n",
    "print_doc(words[:100], end='', stop=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioModel(nn.Module):\n",
    "    def __init__(self, N, voc_size, d_model, hidden_size=128, emb_mat=None, dropout=0.1):\n",
    "        \n",
    "        super(BengioModel, self).__init__()\n",
    "        # parameters\n",
    "        self.N           = N\n",
    "        self.d_model     = d_model\n",
    "        self.voc_size    = voc_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Matriz entrenable de embeddings, tamaño vocab_size x Ngram.d_model\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(emb_mat), freeze=False)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(d_model * (N-1), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, voc_size, bias=False)\n",
    "        \n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        # Calcula el embedding para cada palabra.\n",
    "        x = self.embeddings(input_seq)\n",
    "        x = x.view(-1, (self.N-1) * self.d_model)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logit):\n",
    "    probs = F.softmax(raw_logit.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(raw_logit):\n",
    "    probs = F.softmax(raw_logit.detach(), dim=1)\n",
    "    return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data, model, gpu=False):\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data:\n",
    "            if gpu:\n",
    "                # move inputs to gpu\n",
    "                inputs = inputs.cuda()\n",
    "            \n",
    "            # compute output predictions    \n",
    "            output = model(inputs)\n",
    "            batch_preds = get_preds(output)\n",
    "            # append preds and targets\n",
    "            preds.append(batch_preds)\n",
    "            targets.append(labels.numpy())\n",
    "    \n",
    "    # remove batch dimension\n",
    "    preds = [p for batch_pred in preds for p in batch_pred]\n",
    "    targets = [t for batch_tar in targets for t in batch_tar]\n",
    "    return accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(state, path, val_acc, best_metric, override=False):\n",
    "    if val_acc > best_metric or override: \n",
    "        print('Storing best model to {0}. Current acc: {1}, last best metric: {2}'.format(path, val_acc, best_metric))\n",
    "        torch.save(state, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "voc_size = char_ngram_builder.voc_size\n",
    "N = char_ngram_builder.N\n",
    "d_model = char_ngram_builder.d_model\n",
    "\n",
    "# optimizer hyperparameters\n",
    "lr = 2.3e-1 \n",
    "epochs = 100\n",
    "patience = epochs//5\n",
    "\n",
    "# scheduler hyperparameters\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, hidden_size=200, emb_mat=char_ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'min',\n",
    "                patience = lr_patience,\n",
    "                verbose=True,\n",
    "                factor = lr_factor\n",
    "            )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_loader, val_ds, val_loader = get_datasets(char_ngram_builder, 6, documents, val_documents, batch_size=256, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "train accuracy mean:  0.3443141102940012\n",
      "validation accuracy:  0.4061286064491797\n",
      "mean loss:  2.297941785050635\n",
      "Storing best model to char_best_model. Current acc: 0.4061286064491797, last best metric: 0\n",
      "epoch:  2\n",
      "train accuracy mean:  0.4265518076082308\n",
      "validation accuracy:  0.4336413350933434\n",
      "mean loss:  1.960996209173008\n",
      "Storing best model to char_best_model. Current acc: 0.4336413350933434, last best metric: 0.4061286064491797\n",
      "epoch:  3\n",
      "train accuracy mean:  0.44810113517559286\n",
      "validation accuracy:  0.44148595134829344\n",
      "mean loss:  1.875410953305868\n",
      "Storing best model to char_best_model. Current acc: 0.44148595134829344, last best metric: 0.4336413350933434\n",
      "epoch:  4\n",
      "train accuracy mean:  0.46165679848009633\n",
      "validation accuracy:  0.4551386007920045\n",
      "mean loss:  1.8260459936045255\n",
      "Storing best model to char_best_model. Current acc: 0.4551386007920045, last best metric: 0.44148595134829344\n",
      "epoch:  5\n",
      "train accuracy mean:  0.47037113489838267\n",
      "validation accuracy:  0.466716952668301\n",
      "mean loss:  1.7927101451068166\n",
      "Storing best model to char_best_model. Current acc: 0.466716952668301, last best metric: 0.4551386007920045\n",
      "epoch:  6\n",
      "train accuracy mean:  0.47703169056614236\n",
      "validation accuracy:  0.46997925702432586\n",
      "mean loss:  1.7689015102585883\n",
      "Storing best model to char_best_model. Current acc: 0.46997925702432586, last best metric: 0.466716952668301\n",
      "epoch:  7\n",
      "train accuracy mean:  0.4825919199179458\n",
      "validation accuracy:  0.473882707901188\n",
      "mean loss:  1.7484703326935676\n",
      "Storing best model to char_best_model. Current acc: 0.473882707901188, last best metric: 0.46997925702432586\n",
      "epoch:  8\n",
      "train accuracy mean:  0.4865440828501957\n",
      "validation accuracy:  0.47261927211012633\n",
      "mean loss:  1.7327688713691898\n",
      "epoch:  9\n",
      "train accuracy mean:  0.4902978177620428\n",
      "validation accuracy:  0.47846501980011313\n",
      "mean loss:  1.7192346105635383\n",
      "Storing best model to char_best_model. Current acc: 0.47846501980011313, last best metric: 0.473882707901188\n",
      "epoch:  10\n",
      "train accuracy mean:  0.4925260057778517\n",
      "validation accuracy:  0.48498962851216293\n",
      "mean loss:  1.7085490039799516\n",
      "Storing best model to char_best_model. Current acc: 0.48498962851216293, last best metric: 0.47846501980011313\n",
      "epoch:  11\n",
      "train accuracy mean:  0.4956702619833991\n",
      "validation accuracy:  0.48510277201584007\n",
      "mean loss:  1.699093424083672\n",
      "Storing best model to char_best_model. Current acc: 0.48510277201584007, last best metric: 0.48498962851216293\n",
      "epoch:  12\n",
      "train accuracy mean:  0.49734922737569104\n",
      "validation accuracy:  0.48568734678483877\n",
      "mean loss:  1.6894064341548594\n",
      "Storing best model to char_best_model. Current acc: 0.48568734678483877, last best metric: 0.48510277201584007\n",
      "epoch:  13\n",
      "train accuracy mean:  0.500012499207971\n",
      "validation accuracy:  0.4881199321138978\n",
      "mean loss:  1.6831527984522427\n",
      "Storing best model to char_best_model. Current acc: 0.4881199321138978, last best metric: 0.48568734678483877\n",
      "epoch:  14\n",
      "train accuracy mean:  0.5017037286746187\n",
      "validation accuracy:  0.4881576466151235\n",
      "mean loss:  1.6759149239939417\n",
      "Storing best model to char_best_model. Current acc: 0.4881576466151235, last best metric: 0.4881199321138978\n",
      "epoch:  15\n",
      "train accuracy mean:  0.5034864869948835\n",
      "validation accuracy:  0.49094851970582687\n",
      "mean loss:  1.6695977317308544\n",
      "Storing best model to char_best_model. Current acc: 0.49094851970582687, last best metric: 0.4881576466151235\n",
      "epoch:  16\n",
      "train accuracy mean:  0.5044371940787911\n",
      "validation accuracy:  0.4907599471996983\n",
      "mean loss:  1.6644392325749773\n",
      "epoch:  17\n",
      "train accuracy mean:  0.5059225331266137\n",
      "validation accuracy:  0.4927588157646615\n",
      "mean loss:  1.658790879578598\n",
      "Storing best model to char_best_model. Current acc: 0.4927588157646615, last best metric: 0.49094851970582687\n",
      "epoch:  18\n",
      "train accuracy mean:  0.5071751641480143\n",
      "validation accuracy:  0.49404110880633606\n",
      "mean loss:  1.6539353736083793\n",
      "Storing best model to char_best_model. Current acc: 0.49404110880633606, last best metric: 0.4927588157646615\n",
      "epoch:  19\n",
      "train accuracy mean:  0.5081616362923538\n",
      "validation accuracy:  0.4941165378087875\n",
      "mean loss:  1.6504198213677972\n",
      "Storing best model to char_best_model. Current acc: 0.4941165378087875, last best metric: 0.49404110880633606\n",
      "epoch:  20\n",
      "train accuracy mean:  0.5094748946601404\n",
      "validation accuracy:  0.4967754101452008\n",
      "mean loss:  1.6464718117242825\n",
      "Storing best model to char_best_model. Current acc: 0.4967754101452008, last best metric: 0.4941165378087875\n",
      "epoch:  21\n",
      "train accuracy mean:  0.5101044463519143\n",
      "validation accuracy:  0.49632283613049216\n",
      "mean loss:  1.6421437247286774\n",
      "epoch:  22\n",
      "train accuracy mean:  0.5109298148434158\n",
      "validation accuracy:  0.49330567603243447\n",
      "mean loss:  1.6390370847788769\n",
      "epoch:  23\n",
      "train accuracy mean:  0.5108102555877647\n",
      "validation accuracy:  0.49966056948896853\n",
      "mean loss:  1.6360489616703127\n",
      "Storing best model to char_best_model. Current acc: 0.49966056948896853, last best metric: 0.4967754101452008\n",
      "epoch:  24\n",
      "train accuracy mean:  0.5120762644743303\n",
      "validation accuracy:  0.49570054686026777\n",
      "mean loss:  1.6325397177385648\n",
      "epoch:  25\n",
      "train accuracy mean:  0.5131289947963693\n",
      "validation accuracy:  0.4990948519705827\n",
      "mean loss:  1.6302246361955324\n",
      "epoch:  26\n",
      "train accuracy mean:  0.5141196869703306\n",
      "validation accuracy:  0.4979257024325853\n",
      "mean loss:  1.627466788169793\n",
      "epoch:  27\n",
      "train accuracy mean:  0.5140787860967225\n",
      "validation accuracy:  0.49918913822364697\n",
      "mean loss:  1.6250078032543942\n",
      "epoch:  28\n",
      "train accuracy mean:  0.5145649062831662\n",
      "validation accuracy:  0.5016217235527061\n",
      "mean loss:  1.6230604136980957\n",
      "Storing best model to char_best_model. Current acc: 0.5016217235527061, last best metric: 0.49966056948896853\n",
      "epoch:  29\n",
      "train accuracy mean:  0.5148441707654169\n",
      "validation accuracy:  0.4986045634546483\n",
      "mean loss:  1.6210605523627402\n",
      "epoch:  30\n",
      "train accuracy mean:  0.5158603563734575\n",
      "validation accuracy:  0.5014142937959646\n",
      "mean loss:  1.6186537424806395\n",
      "epoch:  31\n",
      "train accuracy mean:  0.5162123390201017\n",
      "validation accuracy:  0.5043560248915708\n",
      "mean loss:  1.6157058463692353\n",
      "Storing best model to char_best_model. Current acc: 0.5043560248915708, last best metric: 0.5016217235527061\n",
      "epoch:  32\n",
      "train accuracy mean:  0.5169406839566602\n",
      "validation accuracy:  0.501753724306996\n",
      "mean loss:  1.6136835936906757\n",
      "epoch:  33\n",
      "train accuracy mean:  0.5167457829404869\n",
      "validation accuracy:  0.5007354327739015\n",
      "mean loss:  1.6129126741944715\n",
      "epoch:  34\n",
      "train accuracy mean:  0.5170539069801517\n",
      "validation accuracy:  0.5002451442579672\n",
      "mean loss:  1.6116405131613716\n",
      "epoch:  35\n",
      "train accuracy mean:  0.5177207087867699\n",
      "validation accuracy:  0.5033000188572506\n",
      "mean loss:  1.6092382688766615\n",
      "epoch:  36\n",
      "train accuracy mean:  0.5183201262494258\n",
      "validation accuracy:  0.5046200264001509\n",
      "mean loss:  1.6076963283102113\n",
      "Storing best model to char_best_model. Current acc: 0.5046200264001509, last best metric: 0.5043560248915708\n",
      "epoch:  37\n",
      "train accuracy mean:  0.5178817381868871\n",
      "validation accuracy:  0.5025457288327362\n",
      "mean loss:  1.6063774044577264\n",
      "epoch:  38\n",
      "train accuracy mean:  0.5193778562546532\n",
      "validation accuracy:  0.5043560248915708\n",
      "mean loss:  1.6042748232256725\n",
      "epoch:  39\n",
      "train accuracy mean:  0.5192564654318934\n",
      "validation accuracy:  0.5036017348670564\n",
      "mean loss:  1.6031006069253142\n",
      "epoch:  40\n",
      "train accuracy mean:  0.5195418433881417\n",
      "validation accuracy:  0.5033377333584763\n",
      "mean loss:  1.6016458929114734\n",
      "epoch:  41\n",
      "train accuracy mean:  0.5197410386866574\n",
      "validation accuracy:  0.501772581557609\n",
      "mean loss:  1.6009723705829155\n",
      "epoch:  42\n",
      "train accuracy mean:  0.5199019319567869\n",
      "validation accuracy:  0.5040543088817651\n",
      "mean loss:  1.59988578476335\n",
      "epoch:  43\n",
      "train accuracy mean:  0.519960863866052\n",
      "validation accuracy:  0.5024514425796719\n",
      "mean loss:  1.5981696331245514\n",
      "epoch:  44\n",
      "train accuracy mean:  0.5204029150628079\n",
      "validation accuracy:  0.5035263058646049\n",
      "mean loss:  1.5969577131007144\n",
      "epoch:  45\n",
      "train accuracy mean:  0.5204700890240619\n",
      "validation accuracy:  0.5053177446728268\n",
      "mean loss:  1.5967476953714852\n",
      "Storing best model to char_best_model. Current acc: 0.5053177446728268, last best metric: 0.5046200264001509\n",
      "epoch:  46\n",
      "train accuracy mean:  0.5208453870249806\n",
      "validation accuracy:  0.504733169903828\n",
      "mean loss:  1.5955332660326083\n",
      "epoch:  47\n",
      "train accuracy mean:  0.521607034306737\n",
      "validation accuracy:  0.5027343013388648\n",
      "mean loss:  1.5942400401011225\n",
      "epoch:  48\n",
      "train accuracy mean:  0.5214265630692708\n",
      "validation accuracy:  0.5028663020931549\n",
      "mean loss:  1.5939784180850056\n",
      "epoch:  49\n",
      "train accuracy mean:  0.5223921206973023\n",
      "validation accuracy:  0.503243447105412\n",
      "mean loss:  1.59203168551958\n",
      "epoch:  50\n",
      "train accuracy mean:  0.522054221316669\n",
      "validation accuracy:  0.5054686026777296\n",
      "mean loss:  1.5910703004870936\n",
      "Storing best model to char_best_model. Current acc: 0.5054686026777296, last best metric: 0.5053177446728268\n",
      "epoch:  51\n",
      "train accuracy mean:  0.5219724690712667\n",
      "validation accuracy:  0.5054874599283424\n",
      "mean loss:  1.5910620407726648\n",
      "Storing best model to char_best_model. Current acc: 0.5054874599283424, last best metric: 0.5054686026777296\n",
      "epoch:  52\n",
      "train accuracy mean:  0.5222776353775602\n",
      "validation accuracy:  0.506090891947954\n",
      "mean loss:  1.5897803386343254\n",
      "Storing best model to char_best_model. Current acc: 0.506090891947954, last best metric: 0.5054874599283424\n",
      "epoch:  53\n",
      "train accuracy mean:  0.5222885505274913\n",
      "validation accuracy:  0.503658306618895\n",
      "mean loss:  1.589494605620245\n",
      "epoch:  54\n",
      "train accuracy mean:  0.5224554953943512\n",
      "validation accuracy:  0.5061474636997926\n",
      "mean loss:  1.5885921026858993\n",
      "Storing best model to char_best_model. Current acc: 0.5061474636997926, last best metric: 0.506090891947954\n",
      "epoch:  55\n",
      "train accuracy mean:  0.5227246862575045\n",
      "validation accuracy:  0.5046954554026023\n",
      "mean loss:  1.5869951332294685\n",
      "epoch:  56\n",
      "train accuracy mean:  0.5232093956422563\n",
      "validation accuracy:  0.5049405996605695\n",
      "mean loss:  1.5867155435754563\n",
      "epoch:  57\n",
      "train accuracy mean:  0.5228217098124475\n",
      "validation accuracy:  0.5060343201961154\n",
      "mean loss:  1.5849484950537713\n",
      "epoch:  58\n",
      "train accuracy mean:  0.5234755545193176\n",
      "validation accuracy:  0.506090891947954\n",
      "mean loss:  1.5858079163864902\n",
      "epoch:  59\n",
      "train accuracy mean:  0.5236401728009314\n",
      "validation accuracy:  0.5053743164246652\n",
      "mean loss:  1.5832882435983597\n",
      "epoch:  60\n",
      "train accuracy mean:  0.5236358166413217\n",
      "validation accuracy:  0.5069206109749198\n",
      "mean loss:  1.5835082382664778\n",
      "Storing best model to char_best_model. Current acc: 0.5069206109749198, last best metric: 0.5061474636997926\n",
      "epoch:  61\n",
      "train accuracy mean:  0.5241351043102219\n",
      "validation accuracy:  0.5071091834810485\n",
      "mean loss:  1.5826267333661332\n",
      "Storing best model to char_best_model. Current acc: 0.5071091834810485, last best metric: 0.5069206109749198\n",
      "epoch:  62\n",
      "train accuracy mean:  0.5239628751247445\n",
      "validation accuracy:  0.5072600414859514\n",
      "mean loss:  1.5825126408533594\n",
      "Storing best model to char_best_model. Current acc: 0.5072600414859514, last best metric: 0.5071091834810485\n",
      "epoch:  63\n",
      "train accuracy mean:  0.5238804298539499\n",
      "validation accuracy:  0.50744861399208\n",
      "mean loss:  1.5823450338385583\n",
      "Storing best model to char_best_model. Current acc: 0.50744861399208, last best metric: 0.5072600414859514\n",
      "epoch:  64\n",
      "train accuracy mean:  0.5246094678159008\n",
      "validation accuracy:  0.5066000377145012\n",
      "mean loss:  1.5805970864712227\n",
      "epoch:  65\n",
      "train accuracy mean:  0.5243085339146827\n",
      "validation accuracy:  0.506486894210824\n",
      "mean loss:  1.5808533461257168\n",
      "epoch:  66\n",
      "train accuracy mean:  0.5243166274612302\n",
      "validation accuracy:  0.5070148972279842\n",
      "mean loss:  1.579520926448106\n",
      "epoch:  67\n",
      "train accuracy mean:  0.52438450682333\n",
      "validation accuracy:  0.5063926079577598\n",
      "mean loss:  1.5798212310374251\n",
      "epoch:  68\n",
      "train accuracy mean:  0.5245403261575504\n",
      "validation accuracy:  0.5077691872524985\n",
      "mean loss:  1.578016116926948\n",
      "Storing best model to char_best_model. Current acc: 0.5077691872524985, last best metric: 0.50744861399208\n",
      "epoch:  69\n",
      "train accuracy mean:  0.5243357104104295\n",
      "validation accuracy:  0.5066000377145012\n",
      "mean loss:  1.5784297979008883\n",
      "epoch:  70\n",
      "train accuracy mean:  0.5252840166563703\n",
      "validation accuracy:  0.5082029040165944\n",
      "mean loss:  1.578421960996198\n",
      "Storing best model to char_best_model. Current acc: 0.5082029040165944, last best metric: 0.5077691872524985\n",
      "epoch:  71\n",
      "train accuracy mean:  0.5250039848960066\n",
      "validation accuracy:  0.5073543277390157\n",
      "mean loss:  1.5770860774568158\n",
      "epoch:  72\n",
      "train accuracy mean:  0.5251243980579449\n",
      "validation accuracy:  0.5079200452574014\n",
      "mean loss:  1.5763145157384797\n",
      "epoch:  73\n",
      "train accuracy mean:  0.5253230240856025\n",
      "validation accuracy:  0.5070714689798227\n",
      "mean loss:  1.575807356734707\n",
      "epoch:  74\n",
      "train accuracy mean:  0.5247290270715519\n",
      "validation accuracy:  0.5066377522157269\n",
      "mean loss:  1.5757553450299755\n",
      "epoch:  75\n",
      "train accuracy mean:  0.5256578419783301\n",
      "validation accuracy:  0.504733169903828\n",
      "mean loss:  1.5754738321992283\n",
      "epoch:  76\n",
      "train accuracy mean:  0.5256926912552076\n",
      "validation accuracy:  0.5068640392230813\n",
      "mean loss:  1.5743819762447775\n",
      "epoch:  77\n",
      "train accuracy mean:  0.5261279730789337\n",
      "validation accuracy:  0.5063548934565341\n",
      "mean loss:  1.573529511861507\n",
      "epoch:  78\n",
      "train accuracy mean:  0.5257067745212185\n",
      "validation accuracy:  0.5086177635300773\n",
      "mean loss:  1.5740207205003178\n",
      "Storing best model to char_best_model. Current acc: 0.5086177635300773, last best metric: 0.5082029040165944\n",
      "epoch:  79\n",
      "train accuracy mean:  0.5266412821365775\n",
      "validation accuracy:  0.5086743352819159\n",
      "mean loss:  1.5726940904807745\n",
      "Storing best model to char_best_model. Current acc: 0.5086743352819159, last best metric: 0.5086177635300773\n",
      "epoch:  80\n",
      "train accuracy mean:  0.5260865029344675\n",
      "validation accuracy:  0.5081086177635301\n",
      "mean loss:  1.5716662529682652\n",
      "epoch:  81\n",
      "train accuracy mean:  0.5255463020165059\n",
      "validation accuracy:  0.506901753724307\n",
      "mean loss:  1.5730111726715\n",
      "epoch:  82\n",
      "train accuracy mean:  0.5259694806467708\n",
      "validation accuracy:  0.5051291721666981\n",
      "mean loss:  1.5721573394689645\n",
      "epoch:  83\n",
      "train accuracy mean:  0.5261556445928179\n",
      "validation accuracy:  0.506222892702244\n",
      "mean loss:  1.5717371602489634\n",
      "epoch:  84\n",
      "train accuracy mean:  0.5266540288536172\n",
      "validation accuracy:  0.50771261550066\n",
      "mean loss:  1.570691915687565\n",
      "epoch:  85\n",
      "train accuracy mean:  0.5268205529550603\n",
      "validation accuracy:  0.5076183292475956\n",
      "mean loss:  1.5711279255024002\n",
      "epoch:  86\n",
      "train accuracy mean:  0.5266802895658097\n",
      "validation accuracy:  0.508806336036206\n",
      "mean loss:  1.5700427812241287\n",
      "Storing best model to char_best_model. Current acc: 0.508806336036206, last best metric: 0.5086743352819159\n",
      "epoch:  87\n",
      "train accuracy mean:  0.5262414807378543\n",
      "validation accuracy:  0.5072411842353385\n",
      "mean loss:  1.5694908162280021\n",
      "epoch:  88\n",
      "train accuracy mean:  0.5268292157724659\n",
      "validation accuracy:  0.5070903262304356\n",
      "mean loss:  1.5695634589915617\n",
      "epoch:  89\n",
      "train accuracy mean:  0.5268084373861458\n",
      "validation accuracy:  0.5073920422402414\n",
      "mean loss:  1.5677971388741556\n",
      "epoch:  90\n",
      "train accuracy mean:  0.5270891745473554\n",
      "validation accuracy:  0.5073731849896285\n",
      "mean loss:  1.568617004782382\n",
      "epoch:  91\n",
      "train accuracy mean:  0.5271943040242995\n",
      "validation accuracy:  0.5069771827267584\n",
      "mean loss:  1.5679313494656888\n",
      "epoch:  92\n",
      "train accuracy mean:  0.5270798063291039\n",
      "validation accuracy:  0.5090703375447859\n",
      "mean loss:  1.5668720608286002\n",
      "Storing best model to char_best_model. Current acc: 0.5090703375447859, last best metric: 0.508806336036206\n",
      "epoch:  93\n",
      "train accuracy mean:  0.5266705005821413\n",
      "validation accuracy:  0.5091269092966245\n",
      "mean loss:  1.5678902841399494\n",
      "Storing best model to char_best_model. Current acc: 0.5091269092966245, last best metric: 0.5090703375447859\n",
      "epoch:  94\n",
      "train accuracy mean:  0.5272706976983637\n",
      "validation accuracy:  0.5086743352819159\n",
      "mean loss:  1.5665444517659068\n",
      "epoch:  95\n",
      "train accuracy mean:  0.5277114742234156\n",
      "validation accuracy:  0.5060531774467283\n",
      "mean loss:  1.5662152385811374\n",
      "epoch:  96\n",
      "train accuracy mean:  0.5275096800796781\n",
      "validation accuracy:  0.5069206109749198\n",
      "mean loss:  1.5660156308825703\n",
      "epoch:  97\n",
      "train accuracy mean:  0.5277725223352183\n",
      "validation accuracy:  0.5072977559871771\n",
      "mean loss:  1.5658066702711602\n",
      "epoch:  98\n",
      "train accuracy mean:  0.5274395483850528\n",
      "validation accuracy:  0.5094851970582689\n",
      "mean loss:  1.5661837626550439\n",
      "Storing best model to char_best_model. Current acc: 0.5094851970582689, last best metric: 0.5091269092966245\n",
      "epoch:  99\n",
      "train accuracy mean:  0.527280969324716\n",
      "validation accuracy:  0.5093531963039789\n",
      "mean loss:  1.5651899813608667\n",
      "epoch:  100\n",
      "train accuracy mean:  0.5278289915292496\n",
      "validation accuracy:  0.5088440505374316\n",
      "mean loss:  1.5647787510632098\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "\n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # close for each step\n",
    "\n",
    "    # get metric for training set\n",
    "    model.eval()\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "\n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    print('mean loss: ', np.mean(epoch_losses))\n",
    "\n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'char_best_model', val_acc, best_metric)\n",
    "\n",
    "    # patience and last_metric and best_metric update\n",
    "    last_metric = val_acc\n",
    "    counter = counter + 1 if last_metric <= best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "\n",
    "    # check if patience run out\n",
    "    if counter >= patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_state = torch.load('char_best_model')\n",
    "model.load_state_dict(load_state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5226664152366585"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_model(val_loader, model, use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NGram Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGramModel = NGramNeuralModel(char_ngram_builder, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>padre por 35 el que se valer tal al estoy loca jajajaja de lefandarse no tengo monzaba que con\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>copanaduerdos aún así juegas a togerdistaso ajenatacianta que se volvera putas cosalo idea\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu, max_length=300)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>el robo son por atuger mañana de puto en no dejones puto así solo madre tonen no de verga que te wará el arme este el tiemposin no lo tembici el poctando maricónsiso me de que marica de verga de páginas del my pagudite y ella a sentalya jajajajajajaja\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu, max_length=300)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Probability Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.710024"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete a la verga', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.1000185"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('a la vete verga', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.086407"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('esos hijos de la chingada', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-21.678864"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('esos chingada de los hijos', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.1125507"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('estuvieron', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.756233"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('estuveiron', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.3073807"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete alv', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.073044"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete avl', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.13043"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('veet avl', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r o   a m\n",
      "likelihood:  -8.757793\n",
      "\n",
      "  r o a m\n",
      "likelihood:  -9.794529\n",
      "\n",
      "o r   a m\n",
      "likelihood:  -10.665074\n",
      "\n",
      "m o   a r\n",
      "likelihood:  -11.240616\n",
      "\n",
      "r   o a m\n",
      "likelihood:  -11.738606\n",
      "\n",
      "  m o a r\n",
      "likelihood:  -11.784495\n",
      "\n",
      "r m o   a\n",
      "likelihood:  -12.62795\n",
      "\n",
      "r a   o m\n",
      "likelihood:  -12.659768\n",
      "\n",
      "r m o a  \n",
      "likelihood:  -12.839628\n",
      "\n",
      "o m   a r\n",
      "likelihood:  -12.941523\n",
      "\n",
      "a r   o m\n",
      "likelihood:  -12.956074\n",
      "\n",
      "m   o a r\n",
      "likelihood:  -13.132235\n",
      "\n",
      "r m   o a\n",
      "likelihood:  -13.270688\n",
      "\n",
      "m r o a  \n",
      "likelihood:  -13.477239\n",
      "\n",
      "a r o m  \n",
      "likelihood:  -13.480266\n",
      "\n",
      "m r o   a\n",
      "likelihood:  -13.740321\n",
      "\n",
      "r o m a  \n",
      "likelihood:  -13.768693\n",
      "\n",
      "r m a   o\n",
      "likelihood:  -13.921881\n",
      "\n",
      "o m a   r\n",
      "likelihood:  -13.964831\n",
      "\n",
      "  r o m a\n",
      "likelihood:  -14.001951\n",
      "\n",
      "r m   a o\n",
      "likelihood:  -14.005707\n",
      "\n",
      "m r   a o\n",
      "likelihood:  -14.123345\n",
      "\n",
      "m r   o a\n",
      "likelihood:  -14.132148\n",
      "\n",
      "r   a o m\n",
      "likelihood:  -14.150971\n",
      "\n",
      "a r o   m\n",
      "likelihood:  -14.210142\n",
      "\n",
      "a m o   r\n",
      "likelihood:  -14.219949\n",
      "\n",
      "r o a m  \n",
      "likelihood:  -14.234589\n",
      "\n",
      "r   o m a\n",
      "likelihood:  -14.533393\n",
      "\n",
      "o r a   m\n",
      "likelihood:  -14.698946\n",
      "\n",
      "o r a m  \n",
      "likelihood:  -14.706935\n",
      "\n",
      "a m   o r\n",
      "likelihood:  -14.878987\n",
      "\n",
      "  a o m r\n",
      "likelihood:  -14.896548\n",
      "\n",
      "a   o m r\n",
      "likelihood:  -14.983065\n",
      "\n",
      "  o m a r\n",
      "likelihood:  -14.995438\n",
      "\n",
      "r a o m  \n",
      "likelihood:  -15.080059\n",
      "\n",
      "  o a m r\n",
      "likelihood:  -15.081883\n",
      "\n",
      "m a   o r\n",
      "likelihood:  -15.101563\n",
      "\n",
      "m r a   o\n",
      "likelihood:  -15.160372\n",
      "\n",
      "  r a o m\n",
      "likelihood:  -15.177333\n",
      "\n",
      "o   m a r\n",
      "likelihood:  -15.279388\n",
      "\n",
      "a o   m r\n",
      "likelihood:  -15.37584\n",
      "\n",
      "o   a m r\n",
      "likelihood:  -15.427321\n",
      "\n",
      "a o m   r\n",
      "likelihood:  -15.441156\n",
      "\n",
      "r o m   a\n",
      "likelihood:  -15.501533\n",
      "\n",
      "  m a o r\n",
      "likelihood:  -16.014069\n",
      "\n",
      "  m r a o\n",
      "likelihood:  -16.178854\n",
      "\n",
      "r o   m a\n",
      "likelihood:  -16.306595\n",
      "\n",
      "r o a   m\n",
      "likelihood:  -16.34736\n",
      "\n",
      "r   a m o\n",
      "likelihood:  -16.458439\n",
      "\n",
      "o r m   a\n",
      "likelihood:  -16.475891\n",
      "\n",
      "o m a r  \n",
      "likelihood:  -16.51433\n",
      "\n",
      "m o a   r\n",
      "likelihood:  -16.538872\n",
      "\n",
      "o m   r a\n",
      "likelihood:  -16.604118\n",
      "\n",
      "o a   m r\n",
      "likelihood:  -16.679476\n",
      "\n",
      "m   a o r\n",
      "likelihood:  -16.840668\n",
      "\n",
      "o r m a  \n",
      "likelihood:  -16.963787\n",
      "\n",
      "  m a r o\n",
      "likelihood:  -16.966805\n",
      "\n",
      "m r a o  \n",
      "likelihood:  -17.128593\n",
      "\n",
      "o m r   a\n",
      "likelihood:  -17.136257\n",
      "\n",
      "o m r a  \n",
      "likelihood:  -17.362505\n",
      "\n",
      "a m r   o\n",
      "likelihood:  -17.416498\n",
      "\n",
      "r m a o  \n",
      "likelihood:  -17.529531\n",
      "\n",
      "r   m a o\n",
      "likelihood:  -17.56765\n",
      "\n",
      "  r a m o\n",
      "likelihood:  -17.77716\n",
      "\n",
      "m   r a o\n",
      "likelihood:  -17.836134\n",
      "\n",
      "m a o   r\n",
      "likelihood:  -17.862913\n",
      "\n",
      "a r m   o\n",
      "likelihood:  -17.963978\n",
      "\n",
      "m   r o a\n",
      "likelihood:  -18.113659\n",
      "\n",
      "a m o r  \n",
      "likelihood:  -18.142649\n",
      "\n",
      "o a m   r\n",
      "likelihood:  -18.151566\n",
      "\n",
      "r   m o a\n",
      "likelihood:  -18.276093\n",
      "\n",
      "r a o   m\n",
      "likelihood:  -18.313536\n",
      "\n",
      "m o r a  \n",
      "likelihood:  -18.370077\n",
      "\n",
      "o r   m a\n",
      "likelihood:  -18.54717\n",
      "\n",
      "  o a r m\n",
      "likelihood:  -18.629086\n",
      "\n",
      "m o a r  \n",
      "likelihood:  -18.629454\n",
      "\n",
      "a o m r  \n",
      "likelihood:  -18.682661\n",
      "\n",
      "  o r a m\n",
      "likelihood:  -18.71208\n",
      "\n",
      "a o   r m\n",
      "likelihood:  -18.791803\n",
      "\n",
      "  r m o a\n",
      "likelihood:  -18.936193\n",
      "\n",
      "  m r o a\n",
      "likelihood:  -19.076874\n",
      "\n",
      "o   r a m\n",
      "likelihood:  -19.16842\n",
      "\n",
      "  m o r a\n",
      "likelihood:  -19.169783\n",
      "\n",
      "o   a r m\n",
      "likelihood:  -19.171225\n",
      "\n",
      "m o   r a\n",
      "likelihood:  -19.247086\n",
      "\n",
      "a m   r o\n",
      "likelihood:  -19.255072\n",
      "\n",
      "r a m   o\n",
      "likelihood:  -19.296135\n",
      "\n",
      "m o r   a\n",
      "likelihood:  -19.364283\n",
      "\n",
      "o a   r m\n",
      "likelihood:  -19.389141\n",
      "\n",
      "m a r o  \n",
      "likelihood:  -19.421803\n",
      "\n",
      "r a m o  \n",
      "likelihood:  -19.504116\n",
      "\n",
      "r a   m o\n",
      "likelihood:  -19.643324\n",
      "\n",
      "  r m a o\n",
      "likelihood:  -19.674217\n",
      "\n",
      "o   m r a\n",
      "likelihood:  -19.840435\n",
      "\n",
      "a r m o  \n",
      "likelihood:  -20.08249\n",
      "\n",
      "a   m o r\n",
      "likelihood:  -20.209583\n",
      "\n",
      "m a r   o\n",
      "likelihood:  -20.244722\n",
      "\n",
      "a r   m o\n",
      "likelihood:  -20.297892\n",
      "\n",
      "  a r o m\n",
      "likelihood:  -20.62782\n",
      "\n",
      "o a m r  \n",
      "likelihood:  -20.659174\n",
      "\n",
      "m   a r o\n",
      "likelihood:  -20.99902\n",
      "\n",
      "  a m o r\n",
      "likelihood:  -21.015709\n",
      "\n",
      "m   o r a\n",
      "likelihood:  -21.455397\n",
      "\n",
      "a   r o m\n",
      "likelihood:  -21.473293\n",
      "\n",
      "o a r m  \n",
      "likelihood:  -21.706776\n",
      "\n",
      "a m r o  \n",
      "likelihood:  -21.726063\n",
      "\n",
      "  o m r a\n",
      "likelihood:  -21.753942\n",
      "\n",
      "o a r   m\n",
      "likelihood:  -21.771223\n",
      "\n",
      "m a o r  \n",
      "likelihood:  -22.066513\n",
      "\n",
      "a o r   m\n",
      "likelihood:  -22.226685\n",
      "\n",
      "m a   r o\n",
      "likelihood:  -22.346434\n",
      "\n",
      "a   o r m\n",
      "likelihood:  -22.35689\n",
      "\n",
      "  a o r m\n",
      "likelihood:  -23.179127\n",
      "\n",
      "a o r m  \n",
      "likelihood:  -23.382088\n",
      "\n",
      "a   m r o\n",
      "likelihood:  -23.936272\n",
      "\n",
      "o   r m a\n",
      "likelihood:  -24.32052\n",
      "\n",
      "a   r m o\n",
      "likelihood:  -25.294746\n",
      "\n",
      "  o r m a\n",
      "likelihood:  -25.624954\n",
      "\n",
      "  a r m o\n",
      "likelihood:  -26.28173\n",
      "\n",
      "  a m r o\n",
      "likelihood:  -27.342218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_structures(list('amor '), NGramModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.051324235262274"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.perplexity(val_documents, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Podemos ver como la perplejidad de este modelo es más baja que la de los ngramas de palabras, pero esto es porque en general las posibilidades para continuar una secuencia son mucho menores, debido a que el tamaño del vocabulario en este caso son los diferentes caracteres que son mucho menos que el tamaño del vocabulario utilizado con palabras. Por lo tanto es de esperar que la perplejidad sea menor. \n",
    "\n",
    "Podemos ver a través de la generación de secuencias como logra construir algunas palabras correctamente, sin embargo no existe una estructura sintáctica de fondo, y no se esperaba que con este nivel de granularidad se pudiera modela la semántica o sintaxis de una oración.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets to be used in the rest of Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder(d_model=100)\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_loader, val_ds, val_loader = get_datasets(ngram_builder, 4, documents, val_documents, batch_size=64, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "Para este ejercicio entrenaremos dos modelos. Un modelo de tetragramas sin una tabla de embeddings pre-entrenados, y otro modelo de tetragramas con un modelo de embeddings pre-entrenado. \n",
    "\n",
    "Cabe resaltar que para todos los siguientes ejercicios se utilizó un vocabulario con los 10000 tokens más frecuentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Similar Words\n",
    "\n",
    "Esta función nos ayudara a obtener las palabras más similares a una palabra en específico según el modelo de embeddings obtenido al final del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_to(word, ngram_builder, embeddings, N):\n",
    "    # get word id\n",
    "    word_id = ngram_builder.get_ids([word])[0]\n",
    "    word_rep = embeddings[word_id]\n",
    "    # get norms to normalize later\n",
    "    embeddings_norm = np.linalg.norm(embeddings, axis=1)\n",
    "    word_norm = embeddings_norm[word_id]\n",
    "    # sim distance\n",
    "    distances = np.dot(word_rep, embeddings.T)\n",
    "    # normalize distances (cos distance)\n",
    "    distances = np.squeeze(distances/(word_norm * embeddings_norm))\n",
    "    \n",
    "    # most similar word is surely the word itself, so ignore the most similar\n",
    "    return np.argsort(distances)[-(N+1):-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder(d_model=100)\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "voc_size = ngram_builder.voc_size\n",
    "N = ngram_builder.N\n",
    "d_model = ngram_builder.d_model\n",
    "\n",
    "# optimizer hyperparameters\n",
    "lr = 2.3e-1 \n",
    "epochs = 100\n",
    "patience = epochs//5\n",
    "\n",
    "# scheduler hyperparameters\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, hidden_size=200, emb_mat=ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "# optimizer and scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'min',\n",
    "                patience = lr_patience,\n",
    "                verbose=True,\n",
    "                factor = lr_factor\n",
    "            )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "train accuracy mean:  0.061187744140625\n",
      "validation accuracy:  0.06553990610328639\n",
      "mean loss:  6.567409899706642\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.06553990610328639, last best metric: 0\n",
      "epoch:  2\n",
      "train accuracy mean:  0.09185936337425595\n",
      "validation accuracy:  0.10375586854460093\n",
      "mean loss:  6.168204473952453\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.10375586854460093, last best metric: 0.06553990610328639\n",
      "epoch:  3\n",
      "train accuracy mean:  0.10659886920262897\n",
      "validation accuracy:  0.12647887323943663\n",
      "mean loss:  5.965309233404696\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.12647887323943663, last best metric: 0.10375586854460093\n",
      "epoch:  4\n",
      "train accuracy mean:  0.11321197994171628\n",
      "validation accuracy:  0.12150234741784037\n",
      "mean loss:  5.819347424432635\n",
      "epoch:  5\n",
      "train accuracy mean:  0.12097361731150795\n",
      "validation accuracy:  0.08666666666666667\n",
      "mean loss:  5.689494291010003\n",
      "epoch:  6\n",
      "train accuracy mean:  0.1262114994109623\n",
      "validation accuracy:  0.1304225352112676\n",
      "mean loss:  5.576464232988656\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.1304225352112676, last best metric: 0.12647887323943663\n",
      "epoch:  7\n",
      "train accuracy mean:  0.12969147212921628\n",
      "validation accuracy:  0.13192488262910798\n",
      "mean loss:  5.472426605100433\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.13192488262910798, last best metric: 0.1304225352112676\n",
      "epoch:  8\n",
      "train accuracy mean:  0.13355654761904762\n",
      "validation accuracy:  0.12253521126760564\n",
      "mean loss:  5.378628156147897\n",
      "epoch:  9\n",
      "train accuracy mean:  0.13541811988467262\n",
      "validation accuracy:  0.11061032863849765\n",
      "mean loss:  5.283970780049761\n",
      "epoch:  10\n",
      "train accuracy mean:  0.13897898840525794\n",
      "validation accuracy:  0.12572769953051643\n",
      "mean loss:  5.190086302968363\n",
      "epoch:  11\n",
      "train accuracy mean:  0.14205109126984128\n",
      "validation accuracy:  0.14347417840375587\n",
      "mean loss:  5.097106757884224\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.14347417840375587, last best metric: 0.13192488262910798\n",
      "epoch:  12\n",
      "train accuracy mean:  0.1442590138268849\n",
      "validation accuracy:  0.13380281690140844\n",
      "mean loss:  5.006488430313766\n",
      "epoch:  13\n",
      "train accuracy mean:  0.14677114335317462\n",
      "validation accuracy:  0.14037558685446008\n",
      "mean loss:  4.9174217423424125\n",
      "epoch:  14\n",
      "train accuracy mean:  0.14718724810887898\n",
      "validation accuracy:  0.12995305164319249\n",
      "mean loss:  4.826801246342559\n",
      "epoch:  15\n",
      "train accuracy mean:  0.15042211139012898\n",
      "validation accuracy:  0.13061032863849764\n",
      "mean loss:  4.730063894453148\n",
      "epoch:  16\n",
      "train accuracy mean:  0.15300544859871032\n",
      "validation accuracy:  0.11981220657276995\n",
      "mean loss:  4.638235499151051\n",
      "epoch:  17\n",
      "train accuracy mean:  0.15463644360739087\n",
      "validation accuracy:  0.13737089201877933\n",
      "mean loss:  4.546363010536879\n",
      "epoch:  18\n",
      "train accuracy mean:  0.15821523515004962\n",
      "validation accuracy:  0.13539906103286384\n",
      "mean loss:  4.44970822862039\n",
      "epoch:  19\n",
      "train accuracy mean:  0.1614098927331349\n",
      "validation accuracy:  0.14732394366197182\n",
      "mean loss:  4.35583787985767\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.14732394366197182, last best metric: 0.14347417840375587\n",
      "epoch:  20\n",
      "train accuracy mean:  0.16547841874379962\n",
      "validation accuracy:  0.1107981220657277\n",
      "mean loss:  4.26090651905785\n",
      "epoch:  21\n",
      "train accuracy mean:  0.16988312251984128\n",
      "validation accuracy:  0.12253521126760564\n",
      "mean loss:  4.175421953822176\n",
      "epoch:  22\n",
      "train accuracy mean:  0.1752953907800099\n",
      "validation accuracy:  0.10338028169014085\n",
      "mean loss:  4.0786777477090554\n",
      "epoch:  23\n",
      "train accuracy mean:  0.1820092579675099\n",
      "validation accuracy:  0.12037558685446009\n",
      "mean loss:  3.9907711104800305\n",
      "epoch:  24\n",
      "train accuracy mean:  0.1905541798425099\n",
      "validation accuracy:  0.12112676056338029\n",
      "mean loss:  3.8980591230404875\n",
      "epoch:  25\n",
      "train accuracy mean:  0.1979394337487599\n",
      "validation accuracy:  0.1292018779342723\n",
      "mean loss:  3.813984111882746\n",
      "epoch:  26\n",
      "train accuracy mean:  0.21043226453993058\n",
      "validation accuracy:  0.1584037558685446\n",
      "mean loss:  3.72543667225788\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.1584037558685446, last best metric: 0.14732394366197182\n",
      "epoch:  27\n",
      "train accuracy mean:  0.2206445118737599\n",
      "validation accuracy:  0.14666666666666667\n",
      "mean loss:  3.6364563486228385\n",
      "epoch:  28\n",
      "train accuracy mean:  0.23406158931671628\n",
      "validation accuracy:  0.1256338028169014\n",
      "mean loss:  3.551667389770349\n",
      "epoch:  29\n",
      "train accuracy mean:  0.24787442646329363\n",
      "validation accuracy:  0.1171830985915493\n",
      "mean loss:  3.4724151304302118\n",
      "epoch:  30\n",
      "train accuracy mean:  0.26073492140997023\n",
      "validation accuracy:  0.12394366197183099\n",
      "mean loss:  3.3961738208308816\n",
      "epoch:  31\n",
      "train accuracy mean:  0.27310422867063494\n",
      "validation accuracy:  0.14225352112676057\n",
      "mean loss:  3.3243387560360134\n",
      "epoch:  32\n",
      "train accuracy mean:  0.2862970261346726\n",
      "validation accuracy:  0.1215962441314554\n",
      "mean loss:  3.2506092988575497\n",
      "epoch:  33\n",
      "train accuracy mean:  0.29709976438492064\n",
      "validation accuracy:  0.13615023474178403\n",
      "mean loss:  3.1846336824819446\n",
      "epoch:  34\n",
      "train accuracy mean:  0.30996946304563494\n",
      "validation accuracy:  0.12272300469483569\n",
      "mean loss:  3.1122709109137454\n",
      "epoch:  35\n",
      "train accuracy mean:  0.3229098849826389\n",
      "validation accuracy:  0.12741784037558684\n",
      "mean loss:  3.0466814477307103\n",
      "epoch:  36\n",
      "train accuracy mean:  0.3300916883680556\n",
      "validation accuracy:  0.12591549295774648\n",
      "mean loss:  2.987145040029039\n",
      "epoch:  37\n",
      "train accuracy mean:  0.3434298076326885\n",
      "validation accuracy:  0.11089201877934272\n",
      "mean loss:  2.9306451161392033\n",
      "epoch:  38\n",
      "train accuracy mean:  0.35555400545634924\n",
      "validation accuracy:  0.14713615023474177\n",
      "mean loss:  2.8674558588924506\n",
      "epoch:  39\n",
      "train accuracy mean:  0.36383395724826384\n",
      "validation accuracy:  0.1267605633802817\n",
      "mean loss:  2.813828083143259\n",
      "epoch:  40\n",
      "train accuracy mean:  0.37450493706597215\n",
      "validation accuracy:  0.13211267605633803\n",
      "mean loss:  2.7611564619001\n",
      "epoch:  41\n",
      "train accuracy mean:  0.38347710503472215\n",
      "validation accuracy:  0.11511737089201877\n",
      "mean loss:  2.7073762896470726\n",
      "epoch:  42\n",
      "train accuracy mean:  0.39303879510788686\n",
      "validation accuracy:  0.12582159624413145\n",
      "mean loss:  2.6609093429675945\n",
      "epoch:  43\n",
      "train accuracy mean:  0.4018172006758432\n",
      "validation accuracy:  0.13211267605633803\n",
      "mean loss:  2.610931623261422\n",
      "epoch:  44\n",
      "train accuracy mean:  0.40987723214285715\n",
      "validation accuracy:  0.12084507042253521\n",
      "mean loss:  2.563995443517342\n",
      "epoch:  45\n",
      "train accuracy mean:  0.41742234002976186\n",
      "validation accuracy:  0.1148356807511737\n",
      "mean loss:  2.5205164031746485\n",
      "epoch:  46\n",
      "train accuracy mean:  0.42686389741443453\n",
      "validation accuracy:  0.15568075117370891\n",
      "mean loss:  2.476567859062925\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "\n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # close for each step\n",
    "\n",
    "    # get metric for training set\n",
    "    model.eval()\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "\n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    print('mean loss: ', np.mean(epoch_losses))\n",
    "\n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'no_embeddings_best_model', val_acc, best_metric)\n",
    "\n",
    "    # patience and last_metric and best_metric update\n",
    "    last_metric = val_acc\n",
    "    counter = counter + 1 if last_metric <= best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "\n",
    "    # check if patience run out\n",
    "    if counter >= patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy in val:  0.1584037558685446\n"
     ]
    }
   ],
   "source": [
    "load_state = torch.load('no_embeddings_best_model')\n",
    "model.load_state_dict(load_state['model'])\n",
    "model.eval()\n",
    "NGramModel = NGramNeuralModel(ngram_builder, model)\n",
    "print('accuracy in val: ', eval_model(val_loader, model, use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mchinga\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aterrador',\n",
       " 'quééé',\n",
       " 'escoja',\n",
       " 'enseña',\n",
       " '#politicos',\n",
       " '⭐',\n",
       " 'imagen',\n",
       " 'verán',\n",
       " '👎🏼',\n",
       " 'pongas']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'chinga'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mamor\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['puntome',\n",
       " 'narcotrafico',\n",
       " 'entender',\n",
       " 'cool',\n",
       " 'sobredosis',\n",
       " '#milesheizer',\n",
       " 'clientes',\n",
       " 'ves',\n",
       " 'drastico',\n",
       " 'reparaciones']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'amor'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mverga\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['clases',\n",
       " '#btw',\n",
       " 'hablen',\n",
       " 'abdomen',\n",
       " 'blog',\n",
       " 'tardas',\n",
       " 'rayo',\n",
       " 'cabroncito',\n",
       " 'inundo',\n",
       " 'cerebro']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'verga'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos resaltar como la tabla de embeddings entrenados desde 0 no parece resaltar este concepto de agrupar términos similares a través de los embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352.9230342943041"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.perplexity(val_documents, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self, filename):\n",
    "        self.embeddings = {}\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                values = line.split()\n",
    "                word, rep = values[0], np.array(list(map(float, values[1:])))\n",
    "                self.embeddings[word] = rep\n",
    "                self.d_model = len(rep)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.embeddings.get(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.64168 ,  1.447671, -2.283216, -1.965226, -0.222943,  5.105217,\n",
       "       -0.120701, -0.126822, -3.177338, -3.454396, -0.943083, -0.094476,\n",
       "       -1.18936 , -0.812092, -2.572975, -0.613877, -2.311841,  1.05097 ,\n",
       "        5.634725, -5.827006,  1.237639,  1.071621,  3.822072,  2.395414,\n",
       "        0.169883,  3.256835,  2.897348,  3.274827, -2.936382,  0.272003,\n",
       "       -1.029505, -2.617288, -1.807143,  1.737624,  0.33913 ,  3.93293 ,\n",
       "        1.571361, -4.100074,  4.156816,  1.162366, -0.552316, -0.585887,\n",
       "       -4.767187,  0.253338, -1.124162, -0.115079, -5.606624,  2.976579,\n",
       "        4.426022,  1.019932,  3.76072 , -2.298347,  4.416567, -1.383988,\n",
       "       -1.862506,  0.399053, -1.09689 , -2.28599 ,  2.992802,  0.044008,\n",
       "        3.762375, -6.523126,  0.621278,  2.641829, -1.924327, -1.141184,\n",
       "       -3.831767,  0.549591,  2.260839, -1.318358, -1.134662, -3.788221,\n",
       "       -0.775024,  3.956695, -3.579425, -4.423733,  4.505686,  0.719133,\n",
       "       -1.399557,  3.097209,  0.107541,  2.829867, -0.760249, -0.277246,\n",
       "       -1.225739,  2.157902,  0.518391,  3.437977, -1.087452, -1.529914,\n",
       "        0.399476,  0.887398, -1.994891,  2.912205,  3.257981, -2.599953,\n",
       "       -1.995134, -0.176465,  3.946902, -2.792494])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = Embeddings('data/word2vec_col.txt')\n",
    "embeddings['de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder(embeddings=embeddings)\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "voc_size = ngram_builder.voc_size\n",
    "N = ngram_builder.N\n",
    "d_model = ngram_builder.d_model\n",
    "\n",
    "# optimizer hyperparameters\n",
    "lr = 2.3e-1 \n",
    "epochs = 100\n",
    "patience = epochs//5\n",
    "\n",
    "# scheduler hyperparameters\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, hidden_size=200, emb_mat=ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'min',\n",
    "                patience = lr_patience,\n",
    "                verbose=True,\n",
    "                factor = lr_factor\n",
    "            )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "train accuracy mean:  0.0976160443018353\n",
      "validation accuracy:  0.08610328638497652\n",
      "mean loss:  6.43358594737947\n",
      "Storing best model to embeddings_best_model. Current acc: 0.08610328638497652, last best metric: 0\n",
      "epoch:  2\n",
      "train accuracy mean:  0.11321149553571429\n",
      "validation accuracy:  0.14178403755868543\n",
      "mean loss:  5.907127746691306\n",
      "Storing best model to embeddings_best_model. Current acc: 0.14178403755868543, last best metric: 0.08610328638497652\n",
      "epoch:  3\n",
      "train accuracy mean:  0.11843000139508929\n",
      "validation accuracy:  0.1115492957746479\n",
      "mean loss:  5.643981348723173\n",
      "epoch:  4\n",
      "train accuracy mean:  0.12277415442088295\n",
      "validation accuracy:  0.10234741784037558\n",
      "mean loss:  5.4218880993624525\n",
      "epoch:  5\n",
      "train accuracy mean:  0.1268339611235119\n",
      "validation accuracy:  0.1123943661971831\n",
      "mean loss:  5.221812829375267\n",
      "epoch:  6\n",
      "train accuracy mean:  0.13099355546254962\n",
      "validation accuracy:  0.12065727699530517\n",
      "mean loss:  5.040732581013192\n",
      "epoch:  7\n",
      "train accuracy mean:  0.1360991947234623\n",
      "validation accuracy:  0.1127699530516432\n",
      "mean loss:  4.876494385612507\n",
      "epoch:  8\n",
      "train accuracy mean:  0.14216250465029762\n",
      "validation accuracy:  0.10384976525821596\n",
      "mean loss:  4.734184318066885\n",
      "epoch:  9\n",
      "train accuracy mean:  0.14979141477554564\n",
      "validation accuracy:  0.10873239436619718\n",
      "mean loss:  4.602845244109631\n",
      "epoch:  10\n",
      "train accuracy mean:  0.15801033141121032\n",
      "validation accuracy:  0.10657276995305165\n",
      "mean loss:  4.4867781686286135\n",
      "epoch:  11\n",
      "train accuracy mean:  0.1679498581659226\n",
      "validation accuracy:  0.09230046948356807\n",
      "mean loss:  4.373723892960697\n",
      "epoch:  12\n",
      "train accuracy mean:  0.1743885827442956\n",
      "validation accuracy:  0.12281690140845071\n",
      "mean loss:  4.29044725249211\n",
      "epoch:  13\n",
      "train accuracy mean:  0.18617999364459326\n",
      "validation accuracy:  0.11765258215962442\n",
      "mean loss:  4.202369302200775\n",
      "epoch:  14\n",
      "train accuracy mean:  0.19275096106150794\n",
      "validation accuracy:  0.09708920187793427\n",
      "mean loss:  4.136940429142366\n",
      "epoch:  15\n",
      "train accuracy mean:  0.20238531203497023\n",
      "validation accuracy:  0.10704225352112676\n",
      "mean loss:  4.0678097936324775\n",
      "epoch:  16\n",
      "train accuracy mean:  0.20724632626488093\n",
      "validation accuracy:  0.10544600938967136\n",
      "mean loss:  4.015904730496307\n",
      "epoch:  17\n",
      "train accuracy mean:  0.2129240490141369\n",
      "validation accuracy:  0.11474178403755869\n",
      "mean loss:  3.9652265462403498\n",
      "epoch:  18\n",
      "train accuracy mean:  0.2178155808221726\n",
      "validation accuracy:  0.11605633802816902\n",
      "mean loss:  3.9161685903867087\n",
      "epoch:  19\n",
      "train accuracy mean:  0.2248041062127976\n",
      "validation accuracy:  0.10582159624413146\n",
      "mean loss:  3.87322713729615\n",
      "epoch:  20\n",
      "train accuracy mean:  0.22998240637400794\n",
      "validation accuracy:  0.10037558685446009\n",
      "mean loss:  3.8269075898764036\n",
      "epoch:  21\n",
      "train accuracy mean:  0.2366749596974206\n",
      "validation accuracy:  0.1\n",
      "mean loss:  3.7865899816776314\n",
      "epoch:  22\n",
      "train accuracy mean:  0.24053131587921628\n",
      "validation accuracy:  0.12328638497652582\n",
      "mean loss:  3.7537602457838752\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "\n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # close for each step\n",
    "\n",
    "    # get metric for training set\n",
    "    model.eval()\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "\n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    print('mean loss: ', np.mean(epoch_losses))\n",
    "\n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'embeddings_best_model', val_acc, best_metric)\n",
    "\n",
    "    # patience and last_metric and best_metric update\n",
    "    last_metric = val_acc\n",
    "    counter = counter + 1 if last_metric <= best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "\n",
    "    # check if patience run out\n",
    "    if counter >= patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy in val:  0.14178403755868543\n"
     ]
    }
   ],
   "source": [
    "load_state = torch.load('embeddings_best_model')\n",
    "model.load_state_dict(load_state['model'])\n",
    "model.eval()\n",
    "print('accuracy in val: ', eval_model(val_loader, model, use_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGramModel = NGramNeuralModel(ngram_builder, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vete', 'a', 'la', 'verga', '🤔', '</s>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence('vete a la'.split(), use_gpu=use_gpu)\n",
    "print_doc(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinga', 'a', 'tu', 'madre', 'mátenme', '</s>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence('chinga a tu '.split(), use_gpu=use_gpu)\n",
    "print_doc(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estoy a punto vergazos este capítulo de como 💩 haz … <unk> a unos putos putos \n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence('estoy a punto'.split(), use_gpu=use_gpu)\n",
    "print_doc(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.998171"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('voy a estar en mi casa', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19.667858"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('voy a en estar mi casa', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.023245202"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('chinga a tu madre', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.934837"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('chinga a tu padre', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.591314"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('a madre tu chinga', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a vas chingar a tu madre\n",
      "likelihood:  -4.3084855\n",
      "\n",
      "vas a chingar a tu madre\n",
      "likelihood:  -5.482989\n",
      "\n",
      "vas madre a chingar a tu\n",
      "likelihood:  -7.2738085\n",
      "\n",
      "chingar a tu madre vas a\n",
      "likelihood:  -7.515975\n",
      "\n",
      "vas a chingar tu madre a\n",
      "likelihood:  -7.5483837\n",
      "\n",
      "tu vas madre a chingar a\n",
      "likelihood:  -7.9070125\n",
      "\n",
      "tu madre vas a chingar a\n",
      "likelihood:  -7.93376\n",
      "\n",
      "a chingar tu madre vas a\n",
      "likelihood:  -7.964194\n",
      "\n",
      "a vas a chingar tu madre\n",
      "likelihood:  -8.242043\n",
      "\n",
      "a chingar vas a tu madre\n",
      "likelihood:  -8.520243\n",
      "\n",
      "a vas chingar tu madre a\n",
      "likelihood:  -8.681716\n",
      "\n",
      "a madre vas a chingar tu\n",
      "likelihood:  -8.682941\n",
      "\n",
      "madre a vas a chingar tu\n",
      "likelihood:  -8.961501\n",
      "\n",
      "vas chingar a tu madre a\n",
      "likelihood:  -9.287172\n",
      "\n",
      "madre tu vas a chingar a\n",
      "likelihood:  -9.424377\n",
      "\n",
      "a chingar vas tu madre a\n",
      "likelihood:  -9.499789\n",
      "\n",
      "a vas tu madre chingar a\n",
      "likelihood:  -9.680814\n",
      "\n",
      "vas tu madre a chingar a\n",
      "likelihood:  -9.72591\n",
      "\n",
      "a vas tu madre a chingar\n",
      "likelihood:  -9.936181\n",
      "\n",
      "madre tu chingar a vas a\n",
      "likelihood:  -10.059231\n",
      "\n",
      "chingar a vas a tu madre\n",
      "likelihood:  -10.470872\n",
      "\n",
      "a tu chingar madre vas a\n",
      "likelihood:  -10.482743\n",
      "\n",
      "a chingar a tu madre vas\n",
      "likelihood:  -10.483976\n",
      "\n",
      "tu a vas a chingar madre\n",
      "likelihood:  -10.752337\n",
      "\n",
      "vas a tu madre a chingar\n",
      "likelihood:  -10.883236\n",
      "\n",
      "madre vas a chingar a tu\n",
      "likelihood:  -10.901761\n",
      "\n",
      "vas a tu madre chingar a\n",
      "likelihood:  -11.017798\n",
      "\n",
      "vas a a chingar tu madre\n",
      "likelihood:  -11.089592\n",
      "\n",
      "tu vas chingar a madre a\n",
      "likelihood:  -11.14246\n",
      "\n",
      "tu madre chingar a vas a\n",
      "likelihood:  -11.295541\n",
      "\n",
      "madre vas tu a chingar a\n",
      "likelihood:  -11.833676\n",
      "\n",
      "tu vas a chingar a madre\n",
      "likelihood:  -11.838669\n",
      "\n",
      "vas tu a chingar a madre\n",
      "likelihood:  -11.856934\n",
      "\n",
      "chingar tu a madre vas a\n",
      "likelihood:  -11.993791\n",
      "\n",
      "chingar a vas tu madre a\n",
      "likelihood:  -12.057165\n",
      "\n",
      "chingar madre vas a a tu\n",
      "likelihood:  -12.165815\n",
      "\n",
      "vas chingar tu madre a a\n",
      "likelihood:  -12.198259\n",
      "\n",
      "a tu vas a chingar madre\n",
      "likelihood:  -12.625823\n",
      "\n",
      "a a chingar vas tu madre\n",
      "likelihood:  -12.709546\n",
      "\n",
      "tu madre vas a a chingar\n",
      "likelihood:  -12.741095\n",
      "\n",
      "a chingar vas madre a tu\n",
      "likelihood:  -12.774832\n",
      "\n",
      "a chingar tu madre a vas\n",
      "likelihood:  -12.785389\n",
      "\n",
      "tu chingar vas a madre a\n",
      "likelihood:  -12.886546\n",
      "\n",
      "chingar a vas madre a tu\n",
      "likelihood:  -12.899754\n",
      "\n",
      "madre chingar vas a a tu\n",
      "likelihood:  -12.930603\n",
      "\n",
      "a tu madre vas a chingar\n",
      "likelihood:  -12.934748\n",
      "\n",
      "vas a madre chingar a tu\n",
      "likelihood:  -12.949212\n",
      "\n",
      "chingar vas tu madre a a\n",
      "likelihood:  -12.960356\n",
      "\n",
      "madre vas chingar a a tu\n",
      "likelihood:  -12.9946575\n",
      "\n",
      "a a vas chingar tu madre\n",
      "likelihood:  -13.038003\n",
      "\n",
      "chingar a a tu madre vas\n",
      "likelihood:  -13.054129\n",
      "\n",
      "chingar vas a tu madre a\n",
      "likelihood:  -13.103525\n",
      "\n",
      "tu madre a chingar a vas\n",
      "likelihood:  -13.162097\n",
      "\n",
      "madre tu vas a a chingar\n",
      "likelihood:  -13.210866\n",
      "\n",
      "a a chingar tu madre vas\n",
      "likelihood:  -13.426107\n",
      "\n",
      "chingar tu madre a vas a\n",
      "likelihood:  -13.504848\n",
      "\n",
      "chingar vas a a tu madre\n",
      "likelihood:  -13.523647\n",
      "\n",
      "chingar madre vas a tu a\n",
      "likelihood:  -13.536311\n",
      "\n",
      "tu a madre vas a chingar\n",
      "likelihood:  -13.603271\n",
      "\n",
      "tu chingar a madre vas a\n",
      "likelihood:  -13.647656\n",
      "\n",
      "vas chingar a a tu madre\n",
      "likelihood:  -13.65996\n",
      "\n",
      "a tu chingar vas a madre\n",
      "likelihood:  -13.673817\n",
      "\n",
      "madre chingar vas a tu a\n",
      "likelihood:  -13.685355\n",
      "\n",
      "vas madre chingar a a tu\n",
      "likelihood:  -13.822964\n",
      "\n",
      "chingar tu vas madre a a\n",
      "likelihood:  -13.914587\n",
      "\n",
      "tu chingar madre a vas a\n",
      "likelihood:  -13.953055\n",
      "\n",
      "tu a chingar a madre vas\n",
      "likelihood:  -13.954996\n",
      "\n",
      "vas a chingar a madre tu\n",
      "likelihood:  -13.996675\n",
      "\n",
      "a vas chingar a madre tu\n",
      "likelihood:  -14.019035\n",
      "\n",
      "chingar a a vas tu madre\n",
      "likelihood:  -14.053483\n",
      "\n",
      "tu vas chingar a a madre\n",
      "likelihood:  -14.073879\n",
      "\n",
      "chingar vas madre a tu a\n",
      "likelihood:  -14.127687\n",
      "\n",
      "tu madre a vas a chingar\n",
      "likelihood:  -14.198506\n",
      "\n",
      "a vas madre a chingar tu\n",
      "likelihood:  -14.216595\n",
      "\n",
      "madre vas chingar a tu a\n",
      "likelihood:  -14.277628\n",
      "\n",
      "madre tu a vas a chingar\n",
      "likelihood:  -14.335317\n",
      "\n",
      "a tu vas madre a chingar\n",
      "likelihood:  -14.381735\n",
      "\n",
      "chingar a tu madre a vas\n",
      "likelihood:  -14.487398\n",
      "\n",
      "tu a chingar madre vas a\n",
      "likelihood:  -14.530333\n",
      "\n",
      "a chingar madre vas a tu\n",
      "likelihood:  -14.531504\n",
      "\n",
      "tu chingar madre vas a a\n",
      "likelihood:  -14.583139\n",
      "\n",
      "madre a chingar a tu vas\n",
      "likelihood:  -14.649239\n",
      "\n",
      "chingar a madre vas a tu\n",
      "likelihood:  -14.714661\n",
      "\n",
      "a madre tu vas a chingar\n",
      "likelihood:  -14.740127\n",
      "\n",
      "tu a chingar a vas madre\n",
      "likelihood:  -14.785224\n",
      "\n",
      "chingar madre a tu vas a\n",
      "likelihood:  -14.945162\n",
      "\n",
      "vas tu chingar a madre a\n",
      "likelihood:  -14.966887\n",
      "\n",
      "vas madre tu a chingar a\n",
      "likelihood:  -15.055538\n",
      "\n",
      "chingar tu madre vas a a\n",
      "likelihood:  -15.07761\n",
      "\n",
      "a chingar vas a madre tu\n",
      "likelihood:  -15.146157\n",
      "\n",
      "tu vas madre a a chingar\n",
      "likelihood:  -15.185913\n",
      "\n",
      "tu madre chingar a a vas\n",
      "likelihood:  -15.225409\n",
      "\n",
      "vas chingar a madre a tu\n",
      "likelihood:  -15.282843\n",
      "\n",
      "madre a chingar a vas tu\n",
      "likelihood:  -15.283082\n",
      "\n",
      "a tu chingar a vas madre\n",
      "likelihood:  -15.361239\n",
      "\n",
      "madre tu chingar a a vas\n",
      "likelihood:  -15.446615\n",
      "\n",
      "chingar madre tu vas a a\n",
      "likelihood:  -15.452356\n",
      "\n",
      "vas tu a madre chingar a\n",
      "likelihood:  -15.485003\n",
      "\n",
      "a chingar a vas tu madre\n",
      "likelihood:  -15.493352\n",
      "\n",
      "vas a a tu madre chingar\n",
      "likelihood:  -15.552862\n",
      "\n",
      "chingar madre tu a vas a\n",
      "likelihood:  -15.578053\n",
      "\n",
      "a a vas tu madre chingar\n",
      "likelihood:  -15.635621\n",
      "\n",
      "tu chingar vas a a madre\n",
      "likelihood:  -15.692476\n",
      "\n",
      "a chingar madre tu vas a\n",
      "likelihood:  -15.783613\n",
      "\n",
      "chingar tu vas a a madre\n",
      "likelihood:  -15.836306\n",
      "\n",
      "madre vas tu a a chingar\n",
      "likelihood:  -15.929836\n",
      "\n",
      "madre a vas chingar a tu\n",
      "likelihood:  -15.935328\n",
      "\n",
      "a tu vas chingar a madre\n",
      "likelihood:  -15.944546\n",
      "\n",
      "chingar vas madre a a tu\n",
      "likelihood:  -15.9851265\n",
      "\n",
      "a madre chingar a tu vas\n",
      "likelihood:  -16.024593\n",
      "\n",
      "tu chingar a vas madre a\n",
      "likelihood:  -16.058475\n",
      "\n",
      "vas madre chingar a tu a\n",
      "likelihood:  -16.116787\n",
      "\n",
      "a vas chingar madre a tu\n",
      "likelihood:  -16.129957\n",
      "\n",
      "a vas madre chingar a tu\n",
      "likelihood:  -16.149418\n",
      "\n",
      "a tu chingar madre a vas\n",
      "likelihood:  -16.246616\n",
      "\n",
      "madre a tu vas a chingar\n",
      "likelihood:  -16.270468\n",
      "\n",
      "vas tu a madre a chingar\n",
      "likelihood:  -16.36051\n",
      "\n",
      "a madre chingar a vas tu\n",
      "likelihood:  -16.403053\n",
      "\n",
      "vas tu chingar madre a a\n",
      "likelihood:  -16.45551\n",
      "\n",
      "vas a madre a chingar tu\n",
      "likelihood:  -16.49997\n",
      "\n",
      "madre tu a chingar vas a\n",
      "likelihood:  -16.638597\n",
      "\n",
      "vas a chingar madre a tu\n",
      "likelihood:  -16.668663\n",
      "\n",
      "madre chingar a tu vas a\n",
      "likelihood:  -16.669052\n",
      "\n",
      "tu madre a chingar vas a\n",
      "likelihood:  -16.748552\n",
      "\n",
      "vas tu madre chingar a a\n",
      "likelihood:  -16.807547\n",
      "\n",
      "madre tu a chingar a vas\n",
      "likelihood:  -16.809229\n",
      "\n",
      "chingar a madre tu vas a\n",
      "likelihood:  -16.89813\n",
      "\n",
      "vas tu chingar a a madre\n",
      "likelihood:  -16.946959\n",
      "\n",
      "vas madre a a chingar tu\n",
      "likelihood:  -17.046623\n",
      "\n",
      "chingar tu vas a madre a\n",
      "likelihood:  -17.067505\n",
      "\n",
      "tu a chingar vas a madre\n",
      "likelihood:  -17.072899\n",
      "\n",
      "tu a vas chingar a madre\n",
      "likelihood:  -17.114412\n",
      "\n",
      "a vas a chingar madre tu\n",
      "likelihood:  -17.147932\n",
      "\n",
      "madre a a chingar vas tu\n",
      "likelihood:  -17.202206\n",
      "\n",
      "vas chingar madre a tu a\n",
      "likelihood:  -17.276424\n",
      "\n",
      "chingar vas a madre a tu\n",
      "likelihood:  -17.2792\n",
      "\n",
      "a madre vas a tu chingar\n",
      "likelihood:  -17.288654\n",
      "\n",
      "a tu chingar a madre vas\n",
      "likelihood:  -17.381275\n",
      "\n",
      "a madre chingar vas a tu\n",
      "likelihood:  -17.415133\n",
      "\n",
      "vas tu madre a a chingar\n",
      "likelihood:  -17.453014\n",
      "\n",
      "chingar a vas tu a madre\n",
      "likelihood:  -17.464735\n",
      "\n",
      "chingar madre a vas a tu\n",
      "likelihood:  -17.503288\n",
      "\n",
      "tu a chingar vas madre a\n",
      "likelihood:  -17.517582\n",
      "\n",
      "tu a vas madre a chingar\n",
      "likelihood:  -17.573154\n",
      "\n",
      "a chingar vas tu a madre\n",
      "likelihood:  -17.616304\n",
      "\n",
      "a vas chingar madre tu a\n",
      "likelihood:  -17.6196\n",
      "\n",
      "a vas tu a chingar madre\n",
      "likelihood:  -17.665834\n",
      "\n",
      "madre tu chingar vas a a\n",
      "likelihood:  -17.786777\n",
      "\n",
      "tu vas a chingar madre a\n",
      "likelihood:  -17.993967\n",
      "\n",
      "tu vas a a chingar madre\n",
      "likelihood:  -18.09928\n",
      "\n",
      "vas madre a tu a chingar\n",
      "likelihood:  -18.135757\n",
      "\n",
      "madre vas a a chingar tu\n",
      "likelihood:  -18.203009\n",
      "\n",
      "a madre vas chingar a tu\n",
      "likelihood:  -18.230425\n",
      "\n",
      "tu vas a madre chingar a\n",
      "likelihood:  -18.348475\n",
      "\n",
      "tu a vas a madre chingar\n",
      "likelihood:  -18.384226\n",
      "\n",
      "vas a chingar madre tu a\n",
      "likelihood:  -18.390669\n",
      "\n",
      "vas tu a a chingar madre\n",
      "likelihood:  -18.427216\n",
      "\n",
      "vas a a chingar madre tu\n",
      "likelihood:  -18.430536\n",
      "\n",
      "chingar a vas a madre tu\n",
      "likelihood:  -18.435055\n",
      "\n",
      "madre a vas tu a chingar\n",
      "likelihood:  -18.44017\n",
      "\n",
      "a tu chingar vas madre a\n",
      "likelihood:  -18.483269\n",
      "\n",
      "tu chingar vas madre a a\n",
      "likelihood:  -18.56616\n",
      "\n",
      "chingar vas a tu a madre\n",
      "likelihood:  -18.616274\n",
      "\n",
      "vas a tu chingar madre a\n",
      "likelihood:  -18.637594\n",
      "\n",
      "chingar tu a vas a madre\n",
      "likelihood:  -18.64048\n",
      "\n",
      "tu a madre chingar a vas\n",
      "likelihood:  -18.657368\n",
      "\n",
      "a vas a tu madre chingar\n",
      "likelihood:  -18.761827\n",
      "\n",
      "tu chingar a vas a madre\n",
      "likelihood:  -18.777767\n",
      "\n",
      "a vas tu chingar madre a\n",
      "likelihood:  -18.957243\n",
      "\n",
      "madre a chingar vas a tu\n",
      "likelihood:  -18.968842\n",
      "\n",
      "vas chingar a madre tu a\n",
      "likelihood:  -18.985193\n",
      "\n",
      "a tu vas a madre chingar\n",
      "likelihood:  -18.992933\n",
      "\n",
      "madre chingar a vas a tu\n",
      "likelihood:  -19.03957\n",
      "\n",
      "chingar vas tu a madre a\n",
      "likelihood:  -19.111439\n",
      "\n",
      "tu madre chingar vas a a\n",
      "likelihood:  -19.1815\n",
      "\n",
      "chingar a madre a tu vas\n",
      "likelihood:  -19.238998\n",
      "\n",
      "vas chingar a tu a madre\n",
      "likelihood:  -19.280804\n",
      "\n",
      "vas a madre tu a chingar\n",
      "likelihood:  -19.28261\n",
      "\n",
      "a chingar a madre tu vas\n",
      "likelihood:  -19.312424\n",
      "\n",
      "madre chingar tu a vas a\n",
      "likelihood:  -19.39608\n",
      "\n",
      "chingar madre a tu a vas\n",
      "likelihood:  -19.413187\n",
      "\n",
      "chingar tu a madre a vas\n",
      "likelihood:  -19.432705\n",
      "\n",
      "vas tu a chingar madre a\n",
      "likelihood:  -19.444393\n",
      "\n",
      "chingar a vas madre tu a\n",
      "likelihood:  -19.500069\n",
      "\n",
      "vas a a tu chingar madre\n",
      "likelihood:  -19.528164\n",
      "\n",
      "a vas chingar tu a madre\n",
      "likelihood:  -19.530113\n",
      "\n",
      "vas chingar madre tu a a\n",
      "likelihood:  -19.5458\n",
      "\n",
      "tu a madre chingar vas a\n",
      "likelihood:  -19.570145\n",
      "\n",
      "a tu madre chingar a vas\n",
      "likelihood:  -19.586496\n",
      "\n",
      "tu a a chingar vas madre\n",
      "likelihood:  -19.614897\n",
      "\n",
      "vas madre tu chingar a a\n",
      "likelihood:  -19.63364\n",
      "\n",
      "chingar vas a madre tu a\n",
      "likelihood:  -19.71691\n",
      "\n",
      "a vas tu a madre chingar\n",
      "likelihood:  -19.71905\n",
      "\n",
      "a madre chingar tu vas a\n",
      "likelihood:  -19.798693\n",
      "\n",
      "a madre vas tu a chingar\n",
      "likelihood:  -19.818413\n",
      "\n",
      "chingar a a tu vas madre\n",
      "likelihood:  -19.8587\n",
      "\n",
      "tu a a chingar madre vas\n",
      "likelihood:  -19.87529\n",
      "\n",
      "madre a vas a tu chingar\n",
      "likelihood:  -19.894264\n",
      "\n",
      "madre a tu vas chingar a\n",
      "likelihood:  -19.900702\n",
      "\n",
      "a madre a chingar vas tu\n",
      "likelihood:  -19.91764\n",
      "\n",
      "tu vas a madre a chingar\n",
      "likelihood:  -19.946295\n",
      "\n",
      "vas a tu chingar a madre\n",
      "likelihood:  -19.955269\n",
      "\n",
      "a a vas chingar madre tu\n",
      "likelihood:  -20.039492\n",
      "\n",
      "vas madre tu a a chingar\n",
      "likelihood:  -20.220222\n",
      "\n",
      "a chingar vas madre tu a\n",
      "likelihood:  -20.300694\n",
      "\n",
      "a chingar madre a tu vas\n",
      "likelihood:  -20.313305\n",
      "\n",
      "chingar madre vas tu a a\n",
      "likelihood:  -20.331394\n",
      "\n",
      "a chingar tu vas madre a\n",
      "likelihood:  -20.40751\n",
      "\n",
      "vas a tu a chingar madre\n",
      "likelihood:  -20.422369\n",
      "\n",
      "madre vas a tu a chingar\n",
      "likelihood:  -20.42354\n",
      "\n",
      "vas a madre tu chingar a\n",
      "likelihood:  -20.523714\n",
      "\n",
      "a tu vas madre chingar a\n",
      "likelihood:  -20.595345\n",
      "\n",
      "madre chingar a vas tu a\n",
      "likelihood:  -20.6371\n",
      "\n",
      "tu madre a a chingar vas\n",
      "likelihood:  -20.662771\n",
      "\n",
      "a a chingar madre tu vas\n",
      "likelihood:  -20.672026\n",
      "\n",
      "madre chingar vas tu a a\n",
      "likelihood:  -20.680208\n",
      "\n",
      "a a vas tu chingar madre\n",
      "likelihood:  -20.718252\n",
      "\n",
      "madre chingar a a tu vas\n",
      "likelihood:  -20.72098\n",
      "\n",
      "chingar madre a vas tu a\n",
      "likelihood:  -20.721249\n",
      "\n",
      "a vas madre tu chingar a\n",
      "likelihood:  -20.768871\n",
      "\n",
      "vas a tu a madre chingar\n",
      "likelihood:  -20.771284\n",
      "\n",
      "a vas tu chingar a madre\n",
      "likelihood:  -20.803955\n",
      "\n",
      "tu chingar a madre a vas\n",
      "likelihood:  -20.824207\n",
      "\n",
      "a madre tu chingar a vas\n",
      "likelihood:  -20.866426\n",
      "\n",
      "vas a chingar tu a madre\n",
      "likelihood:  -20.90037\n",
      "\n",
      "madre a chingar tu vas a\n",
      "likelihood:  -20.984268\n",
      "\n",
      "vas chingar madre a a tu\n",
      "likelihood:  -21.001698\n",
      "\n",
      "madre tu a vas chingar a\n",
      "likelihood:  -21.008902\n",
      "\n",
      "madre vas a tu chingar a\n",
      "likelihood:  -21.022272\n",
      "\n",
      "vas chingar tu a madre a\n",
      "likelihood:  -21.081226\n",
      "\n",
      "chingar madre tu a a vas\n",
      "likelihood:  -21.08187\n",
      "\n",
      "a madre tu vas chingar a\n",
      "likelihood:  -21.096272\n",
      "\n",
      "a chingar a tu vas madre\n",
      "likelihood:  -21.123962\n",
      "\n",
      "tu madre a vas chingar a\n",
      "likelihood:  -21.135555\n",
      "\n",
      "madre chingar a a vas tu\n",
      "likelihood:  -21.180216\n",
      "\n",
      "a madre tu a chingar vas\n",
      "likelihood:  -21.20935\n",
      "\n",
      "a tu madre chingar vas a\n",
      "likelihood:  -21.210714\n",
      "\n",
      "a chingar a vas madre tu\n",
      "likelihood:  -21.253761\n",
      "\n",
      "madre chingar tu vas a a\n",
      "likelihood:  -21.259392\n",
      "\n",
      "a tu madre a chingar vas\n",
      "likelihood:  -21.331833\n",
      "\n",
      "chingar tu madre a a vas\n",
      "likelihood:  -21.376827\n",
      "\n",
      "madre tu vas chingar a a\n",
      "likelihood:  -21.38734\n",
      "\n",
      "tu a madre a chingar vas\n",
      "likelihood:  -21.412775\n",
      "\n",
      "a tu a chingar madre vas\n",
      "likelihood:  -21.41756\n",
      "\n",
      "tu vas madre chingar a a\n",
      "likelihood:  -21.457626\n",
      "\n",
      "tu a vas madre chingar a\n",
      "likelihood:  -21.458914\n",
      "\n",
      "a vas madre tu a chingar\n",
      "likelihood:  -21.524075\n",
      "\n",
      "a tu a chingar vas madre\n",
      "likelihood:  -21.525652\n",
      "\n",
      "vas a a madre chingar tu\n",
      "likelihood:  -21.547321\n",
      "\n",
      "chingar vas tu a a madre\n",
      "likelihood:  -21.55745\n",
      "\n",
      "tu a a vas chingar madre\n",
      "likelihood:  -21.575562\n",
      "\n",
      "tu a chingar madre a vas\n",
      "likelihood:  -21.725471\n",
      "\n",
      "madre tu a a chingar vas\n",
      "likelihood:  -21.747482\n",
      "\n",
      "chingar a tu vas madre a\n",
      "likelihood:  -21.757164\n",
      "\n",
      "a chingar madre vas tu a\n",
      "likelihood:  -21.779469\n",
      "\n",
      "a a tu madre chingar vas\n",
      "likelihood:  -21.803759\n",
      "\n",
      "a a chingar vas madre tu\n",
      "likelihood:  -21.827953\n",
      "\n",
      "chingar a a madre tu vas\n",
      "likelihood:  -21.83685\n",
      "\n",
      "madre a tu chingar vas a\n",
      "likelihood:  -21.848541\n",
      "\n",
      "tu chingar a a madre vas\n",
      "likelihood:  -21.90497\n",
      "\n",
      "vas madre chingar tu a a\n",
      "likelihood:  -21.911991\n",
      "\n",
      "chingar a madre vas tu a\n",
      "likelihood:  -21.916473\n",
      "\n",
      "a chingar a madre vas tu\n",
      "likelihood:  -21.939732\n",
      "\n",
      "a a tu chingar madre vas\n",
      "likelihood:  -21.945488\n",
      "\n",
      "a chingar madre tu a vas\n",
      "likelihood:  -21.973648\n",
      "\n",
      "chingar a tu vas a madre\n",
      "likelihood:  -22.068314\n",
      "\n",
      "vas chingar a a madre tu\n",
      "likelihood:  -22.127918\n",
      "\n",
      "chingar tu a vas madre a\n",
      "likelihood:  -22.129457\n",
      "\n",
      "a vas a tu chingar madre\n",
      "likelihood:  -22.187828\n",
      "\n",
      "madre a a vas chingar tu\n",
      "likelihood:  -22.216572\n",
      "\n",
      "a a chingar tu vas madre\n",
      "likelihood:  -22.26822\n",
      "\n",
      "a madre tu chingar vas a\n",
      "likelihood:  -22.332233\n",
      "\n",
      "chingar vas madre tu a a\n",
      "likelihood:  -22.433279\n",
      "\n",
      "a chingar madre a vas tu\n",
      "likelihood:  -22.499805\n",
      "\n",
      "madre vas a chingar tu a\n",
      "likelihood:  -22.50985\n",
      "\n",
      "madre a vas tu chingar a\n",
      "likelihood:  -22.644123\n",
      "\n",
      "chingar a madre a vas tu\n",
      "likelihood:  -22.664375\n",
      "\n",
      "madre a chingar vas tu a\n",
      "likelihood:  -22.66857\n",
      "\n",
      "tu chingar a a vas madre\n",
      "likelihood:  -22.674337\n",
      "\n",
      "tu a vas chingar madre a\n",
      "likelihood:  -22.708645\n",
      "\n",
      "vas madre a tu chingar a\n",
      "likelihood:  -22.835548\n",
      "\n",
      "madre a tu a chingar vas\n",
      "likelihood:  -22.878506\n",
      "\n",
      "chingar a madre tu a vas\n",
      "likelihood:  -22.879683\n",
      "\n",
      "madre chingar a tu a vas\n",
      "likelihood:  -22.899569\n",
      "\n",
      "a vas a madre chingar tu\n",
      "likelihood:  -22.987865\n",
      "\n",
      "chingar tu a a vas madre\n",
      "likelihood:  -22.997057\n",
      "\n",
      "tu vas a a madre chingar\n",
      "likelihood:  -23.008022\n",
      "\n",
      "tu chingar madre a a vas\n",
      "likelihood:  -23.066235\n",
      "\n",
      "a a vas madre chingar tu\n",
      "likelihood:  -23.094858\n",
      "\n",
      "vas tu a a madre chingar\n",
      "likelihood:  -23.099445\n",
      "\n",
      "tu a madre vas chingar a\n",
      "likelihood:  -23.113976\n",
      "\n",
      "chingar madre a a tu vas\n",
      "likelihood:  -23.25369\n",
      "\n",
      "madre tu a a vas chingar\n",
      "likelihood:  -23.50176\n",
      "\n",
      "a vas madre a tu chingar\n",
      "likelihood:  -23.532785\n",
      "\n",
      "madre a a chingar tu vas\n",
      "likelihood:  -23.539547\n",
      "\n",
      "vas madre a chingar tu a\n",
      "likelihood:  -23.612648\n",
      "\n",
      "madre vas a a tu chingar\n",
      "likelihood:  -23.62856\n",
      "\n",
      "vas a madre chingar tu a\n",
      "likelihood:  -23.743465\n",
      "\n",
      "chingar a a vas madre tu\n",
      "likelihood:  -23.809568\n",
      "\n",
      "a madre a chingar tu vas\n",
      "likelihood:  -23.863056\n",
      "\n",
      "chingar vas a a madre tu\n",
      "likelihood:  -23.998783\n",
      "\n",
      "a madre chingar tu a vas\n",
      "likelihood:  -23.999722\n",
      "\n",
      "a madre a tu vas chingar\n",
      "likelihood:  -24.041084\n",
      "\n",
      "chingar tu a a madre vas\n",
      "likelihood:  -24.103565\n",
      "\n",
      "chingar a tu a madre vas\n",
      "likelihood:  -24.164352\n",
      "\n",
      "a a chingar madre vas tu\n",
      "likelihood:  -24.276848\n",
      "\n",
      "a chingar tu vas a madre\n",
      "likelihood:  -24.430593\n",
      "\n",
      "madre a a tu chingar vas\n",
      "likelihood:  -24.48582\n",
      "\n",
      "a tu a madre chingar vas\n",
      "likelihood:  -24.501062\n",
      "\n",
      "chingar a a madre vas tu\n",
      "likelihood:  -24.5564\n",
      "\n",
      "vas madre a a tu chingar\n",
      "likelihood:  -24.568714\n",
      "\n",
      "madre a a tu vas chingar\n",
      "likelihood:  -24.720444\n",
      "\n",
      "tu vas chingar madre a a\n",
      "likelihood:  -24.754044\n",
      "\n",
      "chingar madre a a vas tu\n",
      "likelihood:  -24.827244\n",
      "\n",
      "a tu a vas chingar madre\n",
      "likelihood:  -24.845406\n",
      "\n",
      "a madre chingar vas tu a\n",
      "likelihood:  -24.86287\n",
      "\n",
      "madre vas chingar tu a a\n",
      "likelihood:  -25.164886\n",
      "\n",
      "madre chingar tu a a vas\n",
      "likelihood:  -25.24347\n",
      "\n",
      "a vas madre chingar tu a\n",
      "likelihood:  -25.34127\n",
      "\n",
      "vas a madre a tu chingar\n",
      "likelihood:  -25.453926\n",
      "\n",
      "a madre a vas chingar tu\n",
      "likelihood:  -25.699142\n",
      "\n",
      "tu madre a a vas chingar\n",
      "likelihood:  -25.733116\n",
      "\n",
      "a chingar tu a madre vas\n",
      "likelihood:  -25.820698\n",
      "\n",
      "a madre tu a vas chingar\n",
      "likelihood:  -25.97506\n",
      "\n",
      "madre a tu a vas chingar\n",
      "likelihood:  -25.991873\n",
      "\n",
      "madre a tu chingar a vas\n",
      "likelihood:  -25.99427\n",
      "\n",
      "a tu madre a vas chingar\n",
      "likelihood:  -26.11424\n",
      "\n",
      "a tu vas chingar madre a\n",
      "likelihood:  -26.220797\n",
      "\n",
      "a madre a tu chingar vas\n",
      "likelihood:  -26.241196\n",
      "\n",
      "a a madre chingar vas tu\n",
      "likelihood:  -26.404863\n",
      "\n",
      "tu a a vas madre chingar\n",
      "likelihood:  -26.426338\n",
      "\n",
      "vas chingar tu a a madre\n",
      "likelihood:  -26.651392\n",
      "\n",
      "a a madre chingar tu vas\n",
      "likelihood:  -26.97041\n",
      "\n",
      "a a tu chingar vas madre\n",
      "likelihood:  -27.066853\n",
      "\n",
      "tu a madre a vas chingar\n",
      "likelihood:  -27.250254\n",
      "\n",
      "tu a a madre chingar vas\n",
      "likelihood:  -27.282106\n",
      "\n",
      "madre vas tu chingar a a\n",
      "likelihood:  -27.28361\n",
      "\n",
      "a tu madre vas chingar a\n",
      "likelihood:  -27.289202\n",
      "\n",
      "madre a a vas tu chingar\n",
      "likelihood:  -27.32898\n",
      "\n",
      "madre a vas chingar tu a\n",
      "likelihood:  -27.347664\n",
      "\n",
      "a a tu vas madre chingar\n",
      "likelihood:  -27.49336\n",
      "\n",
      "tu madre vas chingar a a\n",
      "likelihood:  -27.895035\n",
      "\n",
      "a a tu madre vas chingar\n",
      "likelihood:  -27.954504\n",
      "\n",
      "a a vas madre tu chingar\n",
      "likelihood:  -27.996593\n",
      "\n",
      "a tu a madre vas chingar\n",
      "likelihood:  -28.117153\n",
      "\n",
      "madre a chingar tu a vas\n",
      "likelihood:  -28.168951\n",
      "\n",
      "chingar a tu a vas madre\n",
      "likelihood:  -28.209064\n",
      "\n",
      "a tu a vas madre chingar\n",
      "likelihood:  -28.241697\n",
      "\n",
      "a a tu vas chingar madre\n",
      "likelihood:  -28.47279\n",
      "\n",
      "a madre a vas tu chingar\n",
      "likelihood:  -28.501442\n",
      "\n",
      "vas a a madre tu chingar\n",
      "likelihood:  -28.544237\n",
      "\n",
      "a a madre tu vas chingar\n",
      "likelihood:  -29.150307\n",
      "\n",
      "a vas a madre tu chingar\n",
      "likelihood:  -29.24225\n",
      "\n",
      "a madre vas tu chingar a\n",
      "likelihood:  -29.93504\n",
      "\n",
      "a a madre vas chingar tu\n",
      "likelihood:  -29.966505\n",
      "\n",
      "a a madre tu chingar vas\n",
      "likelihood:  -30.638538\n",
      "\n",
      "tu a a madre vas chingar\n",
      "likelihood:  -31.45301\n",
      "\n",
      "a chingar tu a vas madre\n",
      "likelihood:  -31.52551\n",
      "\n",
      "a madre vas chingar tu a\n",
      "likelihood:  -32.51433\n",
      "\n",
      "a a madre vas tu chingar\n",
      "likelihood:  -34.89305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_structures('vas a chingar a tu madre'.split(), NGramModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Similar Words to Target Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mchinga\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mentarte',\n",
       " 'reputisima',\n",
       " 'concha',\n",
       " 'chingue',\n",
       " 'put',\n",
       " 'reputa',\n",
       " 'putisima',\n",
       " 'chingar',\n",
       " 'chiga',\n",
       " 'chingas']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'chinga'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mamor\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['llanto',\n",
       " 'hombre',\n",
       " 'pensamiento',\n",
       " 'desprecio',\n",
       " 'orgullo',\n",
       " 'sufrimiento',\n",
       " 'alma',\n",
       " 'corazon',\n",
       " 'corazón',\n",
       " 'cariño']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'amor'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mverga\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['berga',\n",
       " 'caca',\n",
       " 'fregada',\n",
       " 'vergaaaaaa',\n",
       " 'ñonga',\n",
       " 'mierda',\n",
       " 'chingada',\n",
       " 'pija',\n",
       " 'verg',\n",
       " 'vrg']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'verga'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cos Distance Among all Data\n",
    "\n",
    "En esta sección obtendremos los 10 pares de tokens más similares entre sí entre todos los tokens luego de haber cargado los embeddings y haber realizado el entranamiento de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_distance(data):\n",
    "    N = len(data)\n",
    "    distances = np.zeros((N, N))\n",
    "    magnitudes = np.linalg.norm(data, axis=1)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(i+1):\n",
    "            distances[i, j] = np.dot(data[i], data[j])/(magnitudes[i] * magnitudes[j])\n",
    "            if i != j:\n",
    "                distances[j, i] = distances[i, j]\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(dist_matrix, n):\n",
    "    N = len(dist_matrix)\n",
    "    \n",
    "    # get indexes of elements to be compared. dist_matrix should be symmetric, so we dont need to consider each pair of distances twice\n",
    "    indexes = [(i,j) for i in range(N) for j in range(i+1) if i!=j]\n",
    "\n",
    "    # get x and y indexes\n",
    "    x_indexes = tuple([ind[0] for ind in indexes])\n",
    "    y_indexes = tuple([ind[1] for ind in indexes])\n",
    "    \n",
    "    # get values of matrix\n",
    "    row_max = dist_matrix[x_indexes, y_indexes]\n",
    "    \n",
    "    # desc sort elements retrieved and get their positions\n",
    "    max_elements = np.flip(np.argsort(row_max))[:n]\n",
    "    \n",
    "    # return indexes in positions retrieved in previous step\n",
    "    return [indexes[max_index] for max_index in max_elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Most Similar among all words\n",
    "\n",
    "En los siguientes pares podemos ver cuales son los más parecidos de todas las palabras del vocabulario. Podríamos pensar que de hecho estamos obteniendo a la palabra consigo misma como par, pero de hecho en la implementación nos encargamos de evitar que eso sea posible, y si nos fijamos cuidadosamente las palabras a pesar de ser muy parecidas difieren en la cantidad de caracteres que tienen. Podemos ver entonces de esta forma y con las palabras más parecidas a una palabra en específico en la sección anterior que a pesar del entrenamiento, la red aún conserva la similitud a través de los embeddings en varias palabras. Pero, incluso con esto no obtenemos un mejor valor para la perplejidad ni la precisión en el conjunto de datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = cos_distance(model.embeddings.weight.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['goooooool', 'gooooool'],\n",
       " ['jajajajajajajajajaja', 'jajajajajajajajaja'],\n",
       " ['goooool', 'gooooool'],\n",
       " ['jajajajajajajajaja', 'jajajajajajajaja'],\n",
       " ['goooool', 'gooool'],\n",
       " ['goooooool', 'goooool'],\n",
       " ['jajajajajajajaja', 'jajajajajajaja'],\n",
       " ['jajajajajajajajajajaja', 'jajajajajajajajajaja'],\n",
       " ['jajajajajajaja', 'jajajajajaja'],\n",
       " ['yaaaaaa', 'yaaaaa']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar = get_most_similar(dist_matrix, 10)\n",
    "similar = [list(pair) for pair in similar]\n",
    "ngram_builder.inverse(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266.03431212853246"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.perplexity(val_documents, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "A través de este ejercicio podemos resaltar varias cosas interesantes. Primero podemos comparar como con ambos modelos uno termina con un modelo de embeddings en donde las palabras más cercanas no parecen tener relación entre sí, y el otro (el modelo con embeddings pre-entrenados) definitivamente agrupa palabras similares entre sí al comparar a través de la distancia coseno. Esto es de esperar pues el modelo pre-entrenado ya tenía palabras que guardaban una similitud entre sí cercanas inicialmente. \n",
    "\n",
    "Sin embargo, esto no parece ayudar al modelo, pues el modelo sin embeddings pre-entrenados obtiene una mejor precisión en el conjunto de validación durante el entrenamiento, pero como dato curioso, la perplejidad del modelo es mayor con un valor de 353 en comparación con 266 para el modelo con embeddings pre-entrenados. Esto puede parecer contra intuitivo, pero una posible explicación podría ser el modelo con embeddings preentrenado en efecto asigna una mayor probabilidad de manera significativa al dataset de validación, aunque en cuanto a la cantidad el modelo sin embeddings preentrenados logra predecir de manera correcta más tokens, aunque la frontera de decisión sea mucho más parecida a otros tokens. La intuición de que este modelo pretende agrupar términos similares a través de sus embeddings y generalizar de esta manera palabras que tiene una relación semántica, podría tener lugar debido a que puede que la red al tener estos embeddings preentrenados asigne una mayor probabilidad a tokens con una semántica similar en función de los patrones que observo durante entrenamiento. Es decir, tal vez durante entrenamiento observó la secuencuia \"veré a tu padre\" y a través de los embeddings predice que la frase en validación \"veré a tu madre\" tiener mayor probabilidad que otras opciones y asigna una mayor probabilidad de manera significativa al token \"madre\" en comparación con el modelo sin embeddings preentrenados. \n",
    "\n",
    "Sin embargo, vale la pena mencionar que este resultado no se obtuvo de manera consistente todas las veces que se entrenó y experimentó con el modelo, había escenarios en los cuales la perplejidad del modelo sin embeddings pre-entrenados obtenía un menor valor que el de embeddings preentrenados. Esto podría sugerir la necesidad de muchos más datos para que las redes obtengan un comportamiento estable. \n",
    "\n",
    "Otro dato que también vale la pena mencionar, es que de manera consistente el modelo con embeddings preentrenados siempre requirió de menos iteraciones para terminar de ser entrenado, lo cual sugiere que los embeddings preentrenados si ayudan al modelo a converger más rápido a una solución probablemente local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioModel(nn.Module):\n",
    "    def __init__(self, N, voc_size, d_model, hidden_size=128, emb_mat=None, dropout=0.1):\n",
    "        \n",
    "        super(BengioModel, self).__init__()\n",
    "        # parameters\n",
    "        self.N           = N\n",
    "        self.d_model     = d_model\n",
    "        self.voc_size    = voc_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Matriz entrenable de embeddings, tamaño vocab_size x Ngram.d_model\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(emb_mat), freeze=False)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(d_model * (N-1), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, voc_size, bias=False)\n",
    "        # direct connection to output\n",
    "        self.W = nn.Linear(d_model * (N-1), voc_size, bias=False)\n",
    "        \n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        # Calcula el embedding para cada palabra.\n",
    "        x = self.embeddings(input_seq)\n",
    "        x = x.view(-1, (self.N-1) * self.d_model)\n",
    "        \n",
    "        # direct connextion to output\n",
    "        direct_link = self.W(x)\n",
    "        # rest of bengio model\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x + direct_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder(embeddings=embeddings)\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "voc_size = ngram_builder.voc_size\n",
    "N = ngram_builder.N\n",
    "d_model = ngram_builder.d_model\n",
    "\n",
    "# optimizer hyperparameters\n",
    "lr = 2.3e-1 \n",
    "epochs = 100\n",
    "patience = epochs//5\n",
    "\n",
    "# scheduler hyperparameters\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, hidden_size=200, emb_mat=ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'min',\n",
    "                patience = lr_patience,\n",
    "                verbose=True,\n",
    "                factor = lr_factor\n",
    "            )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "train accuracy mean:  0.07259211464533731\n",
      "validation accuracy:  0.05314553990610329\n",
      "mean loss:  9.010061198845506\n",
      "Storing best model to dirlink_best_model. Current acc: 0.05314553990610329, last best metric: 0\n",
      "epoch:  2\n",
      "train accuracy mean:  0.08799331907242064\n",
      "validation accuracy:  0.0787793427230047\n",
      "mean loss:  7.38438277070721\n",
      "Storing best model to dirlink_best_model. Current acc: 0.0787793427230047, last best metric: 0.05314553990610329\n",
      "epoch:  3\n",
      "train accuracy mean:  0.10860431005084326\n",
      "validation accuracy:  0.08488262910798122\n",
      "mean loss:  6.57156110368669\n",
      "Storing best model to dirlink_best_model. Current acc: 0.08488262910798122, last best metric: 0.0787793427230047\n",
      "epoch:  4\n",
      "train accuracy mean:  0.13793073381696427\n",
      "validation accuracy:  0.09539906103286384\n",
      "mean loss:  6.046455083725353\n",
      "Storing best model to dirlink_best_model. Current acc: 0.09539906103286384, last best metric: 0.08488262910798122\n",
      "epoch:  5\n",
      "train accuracy mean:  0.16436961340525794\n",
      "validation accuracy:  0.0815962441314554\n",
      "mean loss:  5.6620619762688875\n",
      "epoch:  6\n",
      "train accuracy mean:  0.1903202117435516\n",
      "validation accuracy:  0.07427230046948356\n",
      "mean loss:  5.354777363905062\n",
      "epoch:  7\n",
      "train accuracy mean:  0.2082848927331349\n",
      "validation accuracy:  0.09070422535211267\n",
      "mean loss:  5.104784130584449\n",
      "epoch:  8\n",
      "train accuracy mean:  0.22658332945808532\n",
      "validation accuracy:  0.07624413145539906\n",
      "mean loss:  4.911396710357319\n",
      "epoch:  9\n",
      "train accuracy mean:  0.24481394934275794\n",
      "validation accuracy:  0.07380281690140846\n",
      "mean loss:  4.738549492166688\n",
      "epoch:  10\n",
      "train accuracy mean:  0.25985039605034727\n",
      "validation accuracy:  0.06948356807511737\n",
      "mean loss:  4.57259591234227\n",
      "epoch:  11\n",
      "train accuracy mean:  0.27035619342137895\n",
      "validation accuracy:  0.09605633802816901\n",
      "mean loss:  4.456454519803326\n",
      "Storing best model to dirlink_best_model. Current acc: 0.09605633802816901, last best metric: 0.09539906103286384\n",
      "epoch:  12\n",
      "train accuracy mean:  0.28143698071676587\n",
      "validation accuracy:  0.09727699530516432\n",
      "mean loss:  4.348210205479215\n",
      "Storing best model to dirlink_best_model. Current acc: 0.09727699530516432, last best metric: 0.09605633802816901\n",
      "epoch:  13\n",
      "train accuracy mean:  0.2937040783110119\n",
      "validation accuracy:  0.10291079812206573\n",
      "mean loss:  4.214262213868399\n",
      "Storing best model to dirlink_best_model. Current acc: 0.10291079812206573, last best metric: 0.09727699530516432\n",
      "epoch:  14\n",
      "train accuracy mean:  0.3009561205667163\n",
      "validation accuracy:  0.08788732394366197\n",
      "mean loss:  4.144289195692788\n",
      "epoch:  15\n",
      "train accuracy mean:  0.3093818785652282\n",
      "validation accuracy:  0.09990610328638498\n",
      "mean loss:  4.058817867965748\n",
      "epoch:  16\n",
      "train accuracy mean:  0.32026454380580355\n",
      "validation accuracy:  0.09333333333333334\n",
      "mean loss:  3.9578197503772876\n",
      "epoch:  17\n",
      "train accuracy mean:  0.3295016818576389\n",
      "validation accuracy:  0.08779342723004695\n",
      "mean loss:  3.8890710074144104\n",
      "epoch:  18\n",
      "train accuracy mean:  0.33608330620659727\n",
      "validation accuracy:  0.09680751173708921\n",
      "mean loss:  3.821900711239626\n",
      "epoch:  19\n",
      "train accuracy mean:  0.3425317189050099\n",
      "validation accuracy:  0.09474178403755869\n",
      "mean loss:  3.765113071848949\n",
      "epoch:  20\n",
      "train accuracy mean:  0.34822978670634924\n",
      "validation accuracy:  0.09070422535211267\n",
      "mean loss:  3.69959954948475\n",
      "epoch:  21\n",
      "train accuracy mean:  0.3554324195498512\n",
      "validation accuracy:  0.09765258215962441\n",
      "mean loss:  3.6475182704937956\n",
      "epoch:  22\n",
      "train accuracy mean:  0.36194283621651785\n",
      "validation accuracy:  0.09352112676056339\n",
      "mean loss:  3.5996889754508934\n",
      "epoch:  23\n",
      "train accuracy mean:  0.3666934058779762\n",
      "validation accuracy:  0.09605633802816901\n",
      "mean loss:  3.5364962201565504\n",
      "epoch:  24\n",
      "train accuracy mean:  0.37346733940972215\n",
      "validation accuracy:  0.10544600938967136\n",
      "mean loss:  3.4961461931622275\n",
      "Storing best model to dirlink_best_model. Current acc: 0.10544600938967136, last best metric: 0.10291079812206573\n",
      "epoch:  25\n",
      "train accuracy mean:  0.3779127332899305\n",
      "validation accuracy:  0.1132394366197183\n",
      "mean loss:  3.449073371011764\n",
      "Storing best model to dirlink_best_model. Current acc: 0.1132394366197183, last best metric: 0.10544600938967136\n",
      "epoch:  26\n",
      "train accuracy mean:  0.3813016376798115\n",
      "validation accuracy:  0.10018779342723004\n",
      "mean loss:  3.4032655080469945\n",
      "epoch:  27\n",
      "train accuracy mean:  0.38628472222222215\n",
      "validation accuracy:  0.10535211267605633\n",
      "mean loss:  3.3643848733821264\n",
      "epoch:  28\n",
      "train accuracy mean:  0.39104594881572424\n",
      "validation accuracy:  0.1115492957746479\n",
      "mean loss:  3.32950816800197\n",
      "epoch:  29\n",
      "train accuracy mean:  0.39604114350818453\n",
      "validation accuracy:  0.11070422535211268\n",
      "mean loss:  3.2880465732887387\n",
      "epoch:  30\n",
      "train accuracy mean:  0.40103536938864087\n",
      "validation accuracy:  0.09201877934272301\n",
      "mean loss:  3.2549568579997867\n",
      "epoch:  31\n",
      "train accuracy mean:  0.40736316499255953\n",
      "validation accuracy:  0.09173708920187794\n",
      "mean loss:  3.2001777704960355\n",
      "epoch:  32\n",
      "train accuracy mean:  0.40826706659226186\n",
      "validation accuracy:  0.09971830985915493\n",
      "mean loss:  3.1766352542520813\n",
      "epoch:  33\n",
      "train accuracy mean:  0.4129260835193452\n",
      "validation accuracy:  0.11643192488262911\n",
      "mean loss:  3.143779134610668\n",
      "Storing best model to dirlink_best_model. Current acc: 0.11643192488262911, last best metric: 0.1132394366197183\n",
      "epoch:  34\n",
      "train accuracy mean:  0.41612170991443453\n",
      "validation accuracy:  0.10591549295774648\n",
      "mean loss:  3.123802365269512\n",
      "epoch:  35\n",
      "train accuracy mean:  0.42038399832589285\n",
      "validation accuracy:  0.09248826291079812\n",
      "mean loss:  3.0819507512884834\n",
      "epoch:  36\n",
      "train accuracy mean:  0.4224296448722718\n",
      "validation accuracy:  0.10272300469483568\n",
      "mean loss:  3.0565577098168433\n",
      "epoch:  37\n",
      "train accuracy mean:  0.4287356422061012\n",
      "validation accuracy:  0.10262910798122066\n",
      "mean loss:  3.0291038346476853\n",
      "epoch:  38\n",
      "train accuracy mean:  0.43003724113343256\n",
      "validation accuracy:  0.10131455399061033\n",
      "mean loss:  3.003875845189517\n",
      "epoch:  39\n",
      "train accuracy mean:  0.4351554749503968\n",
      "validation accuracy:  0.11061032863849765\n",
      "mean loss:  2.981362069879348\n",
      "epoch:  40\n",
      "train accuracy mean:  0.4373624286954365\n",
      "validation accuracy:  0.1111737089201878\n",
      "mean loss:  2.9406757086204984\n",
      "epoch:  41\n",
      "train accuracy mean:  0.4389798603360615\n",
      "validation accuracy:  0.1152112676056338\n",
      "mean loss:  2.9306510493624955\n",
      "epoch:  42\n",
      "train accuracy mean:  0.44281296502976186\n",
      "validation accuracy:  0.11145539906103287\n",
      "mean loss:  2.899794862838462\n",
      "epoch:  43\n",
      "train accuracy mean:  0.4441983661954365\n",
      "validation accuracy:  0.11615023474178404\n",
      "mean loss:  2.8884691979425647\n",
      "epoch:  44\n",
      "train accuracy mean:  0.4482063414558532\n",
      "validation accuracy:  0.09962441314553991\n",
      "mean loss:  2.866723204807689\n",
      "epoch:  45\n",
      "train accuracy mean:  0.4498649476066468\n",
      "validation accuracy:  0.10948356807511737\n",
      "mean loss:  2.847205543424934\n",
      "epoch:  46\n",
      "train accuracy mean:  0.45405360630580355\n",
      "validation accuracy:  0.10018779342723004\n",
      "mean loss:  2.814126191350321\n",
      "epoch:  47\n",
      "train accuracy mean:  0.45904976981026785\n",
      "validation accuracy:  0.10619718309859155\n",
      "mean loss:  2.791893313949307\n",
      "epoch:  48\n",
      "train accuracy mean:  0.46200949048239087\n",
      "validation accuracy:  0.10826291079812207\n",
      "mean loss:  2.7723979803267866\n",
      "epoch:  49\n",
      "train accuracy mean:  0.45996529715401785\n",
      "validation accuracy:  0.10751173708920188\n",
      "mean loss:  2.766968988114968\n",
      "epoch:  50\n",
      "train accuracy mean:  0.4647657606336805\n",
      "validation accuracy:  0.09943661971830986\n",
      "mean loss:  2.731318869162351\n",
      "epoch:  51\n",
      "train accuracy mean:  0.46682109530009924\n",
      "validation accuracy:  0.09962441314553991\n",
      "mean loss:  2.7123341283295304\n",
      "epoch:  52\n",
      "train accuracy mean:  0.4717964293464782\n",
      "validation accuracy:  0.10403755868544601\n",
      "mean loss:  2.6981509344962737\n",
      "epoch:  53\n",
      "train accuracy mean:  0.47424655490451384\n",
      "validation accuracy:  0.1095774647887324\n",
      "mean loss:  2.6592350490391254\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "\n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # close for each step\n",
    "\n",
    "    # get metric for training set\n",
    "    model.eval()\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "\n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    print('mean loss: ', np.mean(epoch_losses))\n",
    "\n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'dirlink_best_model', val_acc, best_metric)\n",
    "\n",
    "    # patience and last_metric and best_metric update\n",
    "    last_metric = val_acc\n",
    "    counter = counter + 1 if last_metric <= best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "\n",
    "    # check if patience run out\n",
    "    if counter >= patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11643192488262911"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_state = torch.load('dirlink_best_model')\n",
    "model.load_state_dict(load_state['model'])\n",
    "model.eval()\n",
    "eval_model(val_loader, model, use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGramModel = NGramNeuralModel(ngram_builder, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/Documents/CIMAT/nlp/tareas/tarea5/NGrams.py:228: RuntimeWarning: divide by zero encountered in log\n",
      "  log_perp = np.sum(-np.log(cond_probs))     # log(1/cond_probs) = log(1) - log(cond_probs) = -log(cond_probs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.perplexity(val_documents, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el modelo devuelve una perplejidad de infinito, lo cual significa que alguno de los tokens como objetivo tiene una probabilidad de 0. Esto no es de extrañar ya que en otras experimentaciones se obtenía valores muy altos de perplejidad para este modelo, lo que parece indicar que había tokens objetivo que tenían probabilidades sumamente bajas. Entonces, de todos los tokens que no tenían una probabilidad de 0 se calculo el que tenía la menor probabilidad, obtuvimos un token que tenía una probabilidad de 4e-45. Entonces, sumamos un valor sumamente pequeño de 1e-60 a las probabilidades que tenían un valor de 0 y calculamos nuevamente las perplejidades en el siguiente bloque, y obtuvimos el valor que se muestra abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4e-45"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams, targets = ngram_builder.transform(val_documents)\n",
    "ngrams = torch.tensor(ngrams)\n",
    "if use_gpu:\n",
    "    ngrams = ngrams.cuda()\n",
    "logits = model(ngrams)\n",
    "probs = get_probs(logits)\n",
    "\n",
    "# get cond probs and perplexity\n",
    "num_target = [i for i in range(len(targets))]\n",
    "cond_probs = probs[num_target, targets]\n",
    "np.min(cond_probs[cond_probs>0])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33692.316505321425"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_probs = cond_probs + (cond_probs==0) * (1e-60)\n",
    "log_perp = np.sum(-np.log(mod_probs))     # log(1/cond_probs) = log(1) - log(cond_probs) = -log(cond_probs)\n",
    "perp = np.exp(1/len(targets) * log_perp)\n",
    "perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> mira ud tiene que entender que me <unk> <unk> <unk> <unk> <unk> en tu <unk> por <unk> que estaba todo \n"
     ]
    }
   ],
   "source": [
    "print_doc(NGramModel.generate_sequence(use_gpu=use_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> 😄 por qué <unk> me <unk> fotos jajajaja \n"
     ]
    }
   ],
   "source": [
    "print_doc(NGramModel.generate_sequence(use_gpu=use_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> no <unk> a mi escuela le chingas a tu madre <unk> a mi que yo <unk> de <unk> y un letrero aqui está valiendo madre por gastar dinero <unk> las <unk> y <unk> ahi valen verga como <unk> <unk> <unk> \n"
     ]
    }
   ],
   "source": [
    "print_doc(NGramModel.generate_sequence(use_gpu=use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Podemos ver que inicialmente la pérdida es mayor que para los otros dos modelos, esto probablemente se debe a que hay una mayor cantidad de parámetros que deben ser ajustados. Para este modelo se debe propagar el error obtenido en la salida a esta nueva matriz de parámetros, y podemos ver que el modelo requiere en general de más epocas para entrenarse. \n",
    "\n",
    "El modelo de hecho no logra obtener un mejor valor para la perplejidad que los dos modelos anteriores, y es de hecho una perplejidad mucho mayor que los otros modelos, incluso cuando no se obtenía valores de infinito, la perplejidad era del orden de 10e+4. A través de la generación de secuencias podemos ver que el modelo genera muchas veces el token UNK. Si bien la capa extra para conectar directamente los embeddings a la salida tal vez podría ser de utilidad, en este escenario y considerando que hay pocos datos relativamente para realizar el entrenamiento, esto nos lleva a pensar que no se puede ver el beneficio de estas conexiones directas, o bien podría ser que de manera general no ayudan ni aportan a la red información adicional para poder predecir el siguiente token.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
