{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 - Juan Luis Baldelomar Cabrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os\n",
    "import random\n",
    "\n",
    "# NLP and numpy\n",
    "import nltk \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score as accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    documents.pop(-1)\n",
    "    labels.pop(-1)\n",
    "    file.close()\n",
    "    labels_file.close()\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_doc(doc:list, end=' ', stop=-1):\n",
    "    stop = len(doc) if stop is None else stop\n",
    "    for token in doc[:stop]:\n",
    "        print(token, end=end)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokenized_docs, n):\n",
    "    tokens = [token for doc in tokenized_docs for token in doc]\n",
    "    unique_tokens = FreqDist(tokens).most_common(n)\n",
    "    return [token for token, _ in unique_tokens]\n",
    "\n",
    "def word2ids(vocabulary):\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    \n",
    "    # build both dictionaries\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        word2id[word] = i\n",
    "        id2word[i] = word\n",
    "    \n",
    "    # add special tokens\n",
    "    n = len(word2id)\n",
    "    word2id['<s>']   = n \n",
    "    word2id['</s>']  = n + 1\n",
    "    word2id['<unk>'] = n + 2\n",
    "    id2word[n]       = '<s>'\n",
    "    id2word[n + 1]   = '</s>'\n",
    "    id2word[n + 2]   = '<unk>'\n",
    "    \n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramBuilder:\n",
    "    def __init__(self, tokenizer=None, embeddings=None, d_model=256, sos='<s>', eos='</s>', unk='<unk>', punctuation=None, postprocess=None):\n",
    "        self.tokenizer = self.default_tokenizer() if tokenizer == None else tokenizer\n",
    "        self.embeddings = embeddings\n",
    "        self.d_model = d_model\n",
    "        # special symbols\n",
    "        self.SOS = sos\n",
    "        self.EOS = eos\n",
    "        self.UNK = unk\n",
    "        # vocabulary 2 id and viceversa\n",
    "        self.word2id  = None\n",
    "        self.id2word  = None\n",
    "        self.voc_size = 0\n",
    "        # post tokenization functions\n",
    "        self.punctuation = set(punctuation) if punctuation != None else None\n",
    "        self.postprocess = postprocess if postprocess is not None else lambda x : x\n",
    "        \n",
    "    def default_tokenizer(doc):\n",
    "        return TweetTokenizer().tokenize\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        return set(self.word2id.keys())\n",
    "    \n",
    "    def remove_punct(self, tokenized_documents):\n",
    "        if self.punctuation == None:\n",
    "            return tokenized_documents\n",
    "        else:\n",
    "            return [[token for token in doc if token not in self.punctuation] for doc in tokenized_documents]\n",
    "        \n",
    "    def _transform(self, tokenized_docs):\n",
    "        N = self.N\n",
    "        # docs and labels lists\n",
    "        ngram_docs, ngram_targs = [], []\n",
    "        # traverse each doc\n",
    "        for doc in tokenized_docs:\n",
    "            # add padding\n",
    "            doc = [self.SOS]*(N - 1)  + doc + [self.EOS]\n",
    "            # build list of ids from word2id dict and replace with UNK\n",
    "            unk_id = self.word2id.get(self.UNK, 0)\n",
    "            ids = [self.word2id.get(word, unk_id) for word in doc]\n",
    "            \n",
    "            # traverse each word as center and build ngrams\n",
    "            for i in range(N-1, len(doc)):    \n",
    "                ngram_docs.append(ids[i-(N-1): i])\n",
    "                ngram_targs.append(ids[i])\n",
    "                \n",
    "        return np.array(ngram_docs), np.array(ngram_targs)\n",
    "    \n",
    "    def _tokenize(self, documents):\n",
    "        tokenized_docs = [self.tokenizer(doc.lower()) for doc in documents]\n",
    "        tokenized_docs = self.remove_punct(tokenized_docs)\n",
    "        tokenized_docs = self.postprocess(tokenized_docs)\n",
    "        return tokenized_docs\n",
    "    \n",
    "    def build_emb_matrix(self):\n",
    "        dim_v = len(self.word2id)\n",
    "        if self.embeddings is None:\n",
    "            self.emb_matrix = np.random.rand(dim_v, self.d_model)\n",
    "        else:\n",
    "            self.emb_matrix = np.zeros((dim_v, self.d_model))\n",
    "            for word in self.word2id.keys():\n",
    "                if word in self.embeddings:\n",
    "                    self.emb_matrix = self.embeddings[word]\n",
    "                \n",
    "    def fit(self, documents, N, t=10000):\n",
    "        self.N = N\n",
    "        # tokenize documents\n",
    "        tokenized_docs = self._tokenize(documents)\n",
    "        \n",
    "        # get vocabulary and word2id and ids2word dicts\n",
    "        vocabulary = get_vocabulary(tokenized_docs, t-3)\n",
    "        self.word2id, self.id2word = word2ids(vocabulary)\n",
    "        self.voc_size = len(self.word2id)\n",
    "        self.build_emb_matrix()\n",
    "        \n",
    "        return self._transform(tokenized_docs)\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        # tokenize, get vocabulary and word2id and ids2word dicts\n",
    "        tokenized_docs = self._tokenize(documents)\n",
    "        return self._transform(tokenized_docs)\n",
    "    \n",
    "    def inverse(self, docs_as_ids):\n",
    "        return [self.id2word.get(tok_id) for tok_id in docs_as_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder()\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)\n",
    "val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 256)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder.emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl </s> a la vga no seas mam√≥n 45 \n"
     ]
    }
   ],
   "source": [
    "doc = ngram_builder.inverse(ngram_labels)\n",
    "print_doc(doc[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10000, 10000, 10000],\n",
       "       [10000, 10000,    28],\n",
       "       [10000,    28,   282],\n",
       "       [   28,   282,     1],\n",
       "       [  282,     1,    59],\n",
       "       [    1,    59,    17],\n",
       "       [   59,    17,     0],\n",
       "       [   17,     0,     6],\n",
       "       [    0,     6,     7],\n",
       "       [    6,     7,   315]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_postprocess(documents):\n",
    "    return [[c for c in word] for doc in documents for word in doc]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder(postprocess=char_postprocess)\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)\n",
    "val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo</s>peor</s>de</s>todo</s>es</s>que</s>no</s>me</s>dan</s>por</s>un</s>tiempo</s>y</s>luego</s>vuelven</s>estoy</s>hasta</s>la</s>verga</s>de</s>estl</s>a</s>la</s>vg\n"
     ]
    }
   ],
   "source": [
    "word = ngram_builder.inverse(ngram_labels)\n",
    "print_doc(word[:100], end='', stop=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder.word2id['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[364, 364, 364],\n",
       "       [364, 364,   9],\n",
       "       [364,   9,   2],\n",
       "       [364, 364, 364],\n",
       "       [364, 364,  13],\n",
       "       [364,  13,   1],\n",
       "       [ 13,   1,   2],\n",
       "       [  1,   2,   4],\n",
       "       [364, 364, 364],\n",
       "       [364, 364,  10]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(torch.tensor(ngram_docs, dtype=torch.int64), torch.tensor(ngram_labels, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_ds, shuffle=True, batch_size=64, num_workers=2)\n",
    "\n",
    "val_ds = TensorDataset(torch.tensor(val_ngram_docs, dtype=torch.int64), torch.tensor(val_ngram_labels, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_ds, batch_size=64, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_loader)[0][0].cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioModel(nn.Module):\n",
    "    def __init__(self, N, voc_size, d_model, hidden_size=128, emb_mat=None, dropout=0.1):\n",
    "        \n",
    "        super(BengioModel, self).__init__()\n",
    "        # parameters\n",
    "        self.N           = N\n",
    "        self.d_model     = d_model\n",
    "        self.voc_size    = voc_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Matriz entrenable de embeddings, tama√±o vocab_size x Ngram.d_model\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(emb_mat), freeze=False)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(d_model * (N-1), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, voc_size)\n",
    "        \n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        # Calcula el embedding para cada palabra.\n",
    "        x = self.embeddings(input_seq)\n",
    "        x = x.view(-1, (self.N-1) * self.d_model)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logit):\n",
    "    probs = F.softmax(raw_logit.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = ngram_builder.voc_size\n",
    "N = ngram_builder.N\n",
    "d_model = ngram_builder.d_model\n",
    "\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, emb_mat=ngram_builder.emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targs = list(train_loader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7311, 1509, 8174, 9097, 5687, 7311, 3779, 1184, 6567,  844,  692,\n",
       "       2254,  778, 9837, 3779, 5687, 4098, 9837, 3779, 3779, 9837, 3823,\n",
       "        778, 7311, 1184, 1623, 6604,  692,  692,  692, 5158, 3885, 3885,\n",
       "       7311, 4908, 1961, 3885,  692,  692, 6122, 4098, 4098,  775, 7311,\n",
       "        692,  310, 6604,  778,  692, 6604, 9544, 3779, 3779,  692, 7311,\n",
       "       7311, 3779, 3779, 5687, 9837,  692,  692, 7788,  692])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_preds(model(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data, model, gpu=False):\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data:\n",
    "            if gpu:\n",
    "                # move inputs to gpu\n",
    "                inputs = inputs.cuda()\n",
    "            \n",
    "            # compute output predictions    \n",
    "            output = model(inputs)\n",
    "            batch_preds = get_preds(output)\n",
    "            # append preds and targets\n",
    "            preds.append(batch_preds)\n",
    "            targets.append(labels.numpy())\n",
    "    \n",
    "    # remove batch dimension\n",
    "    preds = [p for batch_pred in preds for p in batch_pred]\n",
    "    targets = [t for batch_tar in targets for t in batch_tar]\n",
    "    return accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(state, path, is_best):\n",
    "    if is_best:\n",
    "        torch.save(state, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2.3e-1 \n",
    "epochs = 50\n",
    "patience = epochs//5\n",
    "\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, emb_mat=ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            'min',\n",
    "            patience = lr_patience,\n",
    "            verbose=True,\n",
    "            factor = lr_factor\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "accuracy mean:  0.04045678827751196\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "epoch:  3\n",
      "epoch:  4\n",
      "epoch:  5\n",
      "accuracy mean:  0.0975927033492823\n",
      "epoch:  6\n",
      "epoch:  7\n",
      "epoch:  8\n",
      "epoch:  9\n",
      "epoch:  10\n",
      "accuracy mean:  0.12285249700956938\n",
      "epoch:  11\n",
      "epoch:  12\n",
      "epoch:  13\n",
      "epoch:  14\n",
      "epoch:  15\n",
      "accuracy mean:  0.1393596740430622\n",
      "epoch:  16\n",
      "epoch:  17\n",
      "epoch:  18\n",
      "epoch:  19\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "best_model = None\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', epoch)\n",
    "    epoch_metrics = []\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "            \n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # get metric for training set\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "    \n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    \n",
    "    # patience and last metric update\n",
    "    counter = counter + 1 last_metric > val_acc else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "    last_metric = val_acc\n",
    "    \n",
    "    state = {\n",
    "            'epoch' : epoch + 1\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            'model': model.state_dict()\n",
    "            'scheduler': scheduler.state_dict()\n",
    "            'best_metric': best_metric\n",
    "    }\n",
    "    \n",
    "    checkpoint(state, 'best_model', val_acc>best_metric)\n",
    "    \n",
    "    \n",
    "    if counter > patience:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1323098154217699"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(val_loader, model, use_gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
