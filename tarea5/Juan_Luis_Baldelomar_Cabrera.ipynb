{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 - Juan Luis Baldelomar Cabrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os\n",
    "import random\n",
    "\n",
    "# NLP and numpy\n",
    "import nltk \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# NGrams File\n",
    "from NGrams import NGramBuilder\n",
    "from NGrams import NGramNeuralModel\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score as accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_filename):\n",
    "    file = open(filename, 'r')\n",
    "    labels_file = open(labels_filename, 'r')\n",
    "    tweets = file.read()\n",
    "    labels = labels_file.read()\n",
    "    documents = tweets.split('\\n')\n",
    "    labels = labels.split('\\n')\n",
    "    documents.pop(-1)\n",
    "    labels.pop(-1)\n",
    "    file.close()\n",
    "    labels_file.close()\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, labels = load_data('data/mex_train.txt', 'data/mex_train_labels.txt')\n",
    "val_documents, val_labels = load_data('data/mex_val.txt', 'data/mex_val_labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_string(string):\n",
    "    return '\\033[1m' + string + '\\033[0m '\n",
    "\n",
    "def print_doc(doc:list, end=' ', stop=-1):\n",
    "    stop = len(doc) if stop is None else stop\n",
    "    for token in doc[:stop]:\n",
    "        print(token, end=end)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram Builder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 256)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_builder = NGramBuilder()\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)\n",
    "val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)\n",
    "ngram_builder.emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl </s> a la vga no seas mamón 45 \n"
     ]
    }
   ],
   "source": [
    "doc = ngram_builder.inverse(ngram_labels)\n",
    "print_doc(doc[:30])\n",
    "del(ngram_builder);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call after normal tokenization to get each word as a document, i.e <s> word1 </s>, <s> word2 </s>, ...\n",
    "def char_postprocess(documents):\n",
    "    return [[c for c in word] for doc in documents for word in doc]\n",
    "\n",
    "# tokenize documents char by char so you can add <s> and </s> at end of each doc\n",
    "def char_tokenizer(doc):\n",
    "    return [char for char in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ngram_builder = NGramBuilder(tokenizer=char_tokenizer, d_model=100)\n",
    "ngram_docs, ngram_labels = char_ngram_builder.fit(documents, N=6)\n",
    "val_ngram_docs, val_ngram_labels = char_ngram_builder.transform(val_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl</s>a la vg\n"
     ]
    }
   ],
   "source": [
    "words = char_ngram_builder.inverse(ngram_labels)\n",
    "print_doc(words[:100], end='', stop=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(ngram_builder, N, train_docs, val_docs, batch_size=64, num_workers=2):\n",
    "    ngram_docs, ngram_labels = ngram_builder.fit(documents, N=N)\n",
    "    val_ngram_docs, val_ngram_labels = ngram_builder.transform(val_documents)\n",
    "    \n",
    "    train_ds = TensorDataset(torch.tensor(ngram_docs, dtype=torch.int64), torch.tensor(ngram_labels, dtype=torch.int64))\n",
    "    train_loader = DataLoader(train_ds, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    val_ds = TensorDataset(torch.tensor(val_ngram_docs, dtype=torch.int64), torch.tensor(val_ngram_labels, dtype=torch.int64))\n",
    "    val_loader = DataLoader(val_ds, shuffle=False, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    return train_ds, train_loader, val_ds, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Syntactical and Morphological Structures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def get_perms(tokens):\n",
    "    perms = set(permutations(tokens))\n",
    "    return list(perms)\n",
    "\n",
    "def test_structures(tokens, ngram_model):\n",
    "    perms = get_perms(tokens)\n",
    "    likelihoods = [(ngram_model.estimate_prob(' '.join(perm), use_gpu=use_gpu), ' '.join(perm)) for perm in perms]\n",
    "    likelihoods = sorted(likelihoods, reverse=True)\n",
    "    for l, sentence in likelihoods:\n",
    "        print(sentence)\n",
    "        print('likelihood: ', l, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioModel(nn.Module):\n",
    "    def __init__(self, N, voc_size, d_model, hidden_size=128, emb_mat=None, dropout=0.1):\n",
    "        \n",
    "        super(BengioModel, self).__init__()\n",
    "        # parameters\n",
    "        self.N           = N\n",
    "        self.d_model     = d_model\n",
    "        self.voc_size    = voc_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Matriz entrenable de embeddings, tamaño vocab_size x Ngram.d_model\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(emb_mat), freeze=False)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(d_model * (N-1), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, voc_size, bias=False)\n",
    "        \n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        # Calcula el embedding para cada palabra.\n",
    "        x = self.embeddings(input_seq)\n",
    "        x = x.view(-1, (self.N-1) * self.d_model)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logit):\n",
    "    probs = F.softmax(raw_logit.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(raw_logit):\n",
    "    probs = F.softmax(raw_logit.detach(), dim=1)\n",
    "    return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data, model, gpu=False):\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data:\n",
    "            if gpu:\n",
    "                # move inputs to gpu\n",
    "                inputs = inputs.cuda()\n",
    "            \n",
    "            # compute output predictions    \n",
    "            output = model(inputs)\n",
    "            batch_preds = get_preds(output)\n",
    "            # append preds and targets\n",
    "            preds.append(batch_preds)\n",
    "            targets.append(labels.numpy())\n",
    "    \n",
    "    # remove batch dimension\n",
    "    preds = [p for batch_pred in preds for p in batch_pred]\n",
    "    targets = [t for batch_tar in targets for t in batch_tar]\n",
    "    return accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(state, path, val_acc, best_metric, override=False):\n",
    "    if val_acc > best_metric or override: \n",
    "        print('Storing best model to {0}. Current acc: {1}, last best metric: {2}'.format(path, val_acc, best_metric))\n",
    "        torch.save(state, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "voc_size = char_ngram_builder.voc_size\n",
    "N = char_ngram_builder.N\n",
    "d_model = char_ngram_builder.d_model\n",
    "\n",
    "# optimizer hyperparameters\n",
    "lr = 2.3e-1 \n",
    "epochs = 100\n",
    "patience = epochs//5\n",
    "\n",
    "# scheduler hyperparameters\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, hidden_size=200, emb_mat=char_ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'min',\n",
    "                patience = lr_patience,\n",
    "                verbose=True,\n",
    "                factor = lr_factor\n",
    "            )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.multiprocessing\n",
    "#torch.multiprocessing.set_sharing_strategy('file_descriptor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_loader, val_ds, val_loader = get_datasets(char_ngram_builder, 6, documents, val_documents, batch_size=256, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "train accuracy mean:  0.3443141102940012\n",
      "validation accuracy:  0.4061286064491797\n",
      "mean loss:  2.297941785050635\n",
      "Storing best model to char_best_model. Current acc: 0.4061286064491797, last best metric: 0\n",
      "epoch:  2\n",
      "train accuracy mean:  0.4265518076082308\n",
      "validation accuracy:  0.4336413350933434\n",
      "mean loss:  1.960996209173008\n",
      "Storing best model to char_best_model. Current acc: 0.4336413350933434, last best metric: 0.4061286064491797\n",
      "epoch:  3\n",
      "train accuracy mean:  0.44810113517559286\n",
      "validation accuracy:  0.44148595134829344\n",
      "mean loss:  1.875410953305868\n",
      "Storing best model to char_best_model. Current acc: 0.44148595134829344, last best metric: 0.4336413350933434\n",
      "epoch:  4\n",
      "train accuracy mean:  0.46165679848009633\n",
      "validation accuracy:  0.4551386007920045\n",
      "mean loss:  1.8260459936045255\n",
      "Storing best model to char_best_model. Current acc: 0.4551386007920045, last best metric: 0.44148595134829344\n",
      "epoch:  5\n",
      "train accuracy mean:  0.47037113489838267\n",
      "validation accuracy:  0.466716952668301\n",
      "mean loss:  1.7927101451068166\n",
      "Storing best model to char_best_model. Current acc: 0.466716952668301, last best metric: 0.4551386007920045\n",
      "epoch:  6\n",
      "train accuracy mean:  0.47703169056614236\n",
      "validation accuracy:  0.46997925702432586\n",
      "mean loss:  1.7689015102585883\n",
      "Storing best model to char_best_model. Current acc: 0.46997925702432586, last best metric: 0.466716952668301\n",
      "epoch:  7\n",
      "train accuracy mean:  0.4825919199179458\n",
      "validation accuracy:  0.473882707901188\n",
      "mean loss:  1.7484703326935676\n",
      "Storing best model to char_best_model. Current acc: 0.473882707901188, last best metric: 0.46997925702432586\n",
      "epoch:  8\n",
      "train accuracy mean:  0.4865440828501957\n",
      "validation accuracy:  0.47261927211012633\n",
      "mean loss:  1.7327688713691898\n",
      "epoch:  9\n",
      "train accuracy mean:  0.4902978177620428\n",
      "validation accuracy:  0.47846501980011313\n",
      "mean loss:  1.7192346105635383\n",
      "Storing best model to char_best_model. Current acc: 0.47846501980011313, last best metric: 0.473882707901188\n",
      "epoch:  10\n",
      "train accuracy mean:  0.4925260057778517\n",
      "validation accuracy:  0.48498962851216293\n",
      "mean loss:  1.7085490039799516\n",
      "Storing best model to char_best_model. Current acc: 0.48498962851216293, last best metric: 0.47846501980011313\n",
      "epoch:  11\n",
      "train accuracy mean:  0.4956702619833991\n",
      "validation accuracy:  0.48510277201584007\n",
      "mean loss:  1.699093424083672\n",
      "Storing best model to char_best_model. Current acc: 0.48510277201584007, last best metric: 0.48498962851216293\n",
      "epoch:  12\n",
      "train accuracy mean:  0.49734922737569104\n",
      "validation accuracy:  0.48568734678483877\n",
      "mean loss:  1.6894064341548594\n",
      "Storing best model to char_best_model. Current acc: 0.48568734678483877, last best metric: 0.48510277201584007\n",
      "epoch:  13\n",
      "train accuracy mean:  0.500012499207971\n",
      "validation accuracy:  0.4881199321138978\n",
      "mean loss:  1.6831527984522427\n",
      "Storing best model to char_best_model. Current acc: 0.4881199321138978, last best metric: 0.48568734678483877\n",
      "epoch:  14\n",
      "train accuracy mean:  0.5017037286746187\n",
      "validation accuracy:  0.4881576466151235\n",
      "mean loss:  1.6759149239939417\n",
      "Storing best model to char_best_model. Current acc: 0.4881576466151235, last best metric: 0.4881199321138978\n",
      "epoch:  15\n",
      "train accuracy mean:  0.5034864869948835\n",
      "validation accuracy:  0.49094851970582687\n",
      "mean loss:  1.6695977317308544\n",
      "Storing best model to char_best_model. Current acc: 0.49094851970582687, last best metric: 0.4881576466151235\n",
      "epoch:  16\n",
      "train accuracy mean:  0.5044371940787911\n",
      "validation accuracy:  0.4907599471996983\n",
      "mean loss:  1.6644392325749773\n",
      "epoch:  17\n",
      "train accuracy mean:  0.5059225331266137\n",
      "validation accuracy:  0.4927588157646615\n",
      "mean loss:  1.658790879578598\n",
      "Storing best model to char_best_model. Current acc: 0.4927588157646615, last best metric: 0.49094851970582687\n",
      "epoch:  18\n",
      "train accuracy mean:  0.5071751641480143\n",
      "validation accuracy:  0.49404110880633606\n",
      "mean loss:  1.6539353736083793\n",
      "Storing best model to char_best_model. Current acc: 0.49404110880633606, last best metric: 0.4927588157646615\n",
      "epoch:  19\n",
      "train accuracy mean:  0.5081616362923538\n",
      "validation accuracy:  0.4941165378087875\n",
      "mean loss:  1.6504198213677972\n",
      "Storing best model to char_best_model. Current acc: 0.4941165378087875, last best metric: 0.49404110880633606\n",
      "epoch:  20\n",
      "train accuracy mean:  0.5094748946601404\n",
      "validation accuracy:  0.4967754101452008\n",
      "mean loss:  1.6464718117242825\n",
      "Storing best model to char_best_model. Current acc: 0.4967754101452008, last best metric: 0.4941165378087875\n",
      "epoch:  21\n",
      "train accuracy mean:  0.5101044463519143\n",
      "validation accuracy:  0.49632283613049216\n",
      "mean loss:  1.6421437247286774\n",
      "epoch:  22\n",
      "train accuracy mean:  0.5109298148434158\n",
      "validation accuracy:  0.49330567603243447\n",
      "mean loss:  1.6390370847788769\n",
      "epoch:  23\n",
      "train accuracy mean:  0.5108102555877647\n",
      "validation accuracy:  0.49966056948896853\n",
      "mean loss:  1.6360489616703127\n",
      "Storing best model to char_best_model. Current acc: 0.49966056948896853, last best metric: 0.4967754101452008\n",
      "epoch:  24\n",
      "train accuracy mean:  0.5120762644743303\n",
      "validation accuracy:  0.49570054686026777\n",
      "mean loss:  1.6325397177385648\n",
      "epoch:  25\n",
      "train accuracy mean:  0.5131289947963693\n",
      "validation accuracy:  0.4990948519705827\n",
      "mean loss:  1.6302246361955324\n",
      "epoch:  26\n",
      "train accuracy mean:  0.5141196869703306\n",
      "validation accuracy:  0.4979257024325853\n",
      "mean loss:  1.627466788169793\n",
      "epoch:  27\n",
      "train accuracy mean:  0.5140787860967225\n",
      "validation accuracy:  0.49918913822364697\n",
      "mean loss:  1.6250078032543942\n",
      "epoch:  28\n",
      "train accuracy mean:  0.5145649062831662\n",
      "validation accuracy:  0.5016217235527061\n",
      "mean loss:  1.6230604136980957\n",
      "Storing best model to char_best_model. Current acc: 0.5016217235527061, last best metric: 0.49966056948896853\n",
      "epoch:  29\n",
      "train accuracy mean:  0.5148441707654169\n",
      "validation accuracy:  0.4986045634546483\n",
      "mean loss:  1.6210605523627402\n",
      "epoch:  30\n",
      "train accuracy mean:  0.5158603563734575\n",
      "validation accuracy:  0.5014142937959646\n",
      "mean loss:  1.6186537424806395\n",
      "epoch:  31\n",
      "train accuracy mean:  0.5162123390201017\n",
      "validation accuracy:  0.5043560248915708\n",
      "mean loss:  1.6157058463692353\n",
      "Storing best model to char_best_model. Current acc: 0.5043560248915708, last best metric: 0.5016217235527061\n",
      "epoch:  32\n",
      "train accuracy mean:  0.5169406839566602\n",
      "validation accuracy:  0.501753724306996\n",
      "mean loss:  1.6136835936906757\n",
      "epoch:  33\n",
      "train accuracy mean:  0.5167457829404869\n",
      "validation accuracy:  0.5007354327739015\n",
      "mean loss:  1.6129126741944715\n",
      "epoch:  34\n",
      "train accuracy mean:  0.5170539069801517\n",
      "validation accuracy:  0.5002451442579672\n",
      "mean loss:  1.6116405131613716\n",
      "epoch:  35\n",
      "train accuracy mean:  0.5177207087867699\n",
      "validation accuracy:  0.5033000188572506\n",
      "mean loss:  1.6092382688766615\n",
      "epoch:  36\n",
      "train accuracy mean:  0.5183201262494258\n",
      "validation accuracy:  0.5046200264001509\n",
      "mean loss:  1.6076963283102113\n",
      "Storing best model to char_best_model. Current acc: 0.5046200264001509, last best metric: 0.5043560248915708\n",
      "epoch:  37\n",
      "train accuracy mean:  0.5178817381868871\n",
      "validation accuracy:  0.5025457288327362\n",
      "mean loss:  1.6063774044577264\n",
      "epoch:  38\n",
      "train accuracy mean:  0.5193778562546532\n",
      "validation accuracy:  0.5043560248915708\n",
      "mean loss:  1.6042748232256725\n",
      "epoch:  39\n",
      "train accuracy mean:  0.5192564654318934\n",
      "validation accuracy:  0.5036017348670564\n",
      "mean loss:  1.6031006069253142\n",
      "epoch:  40\n",
      "train accuracy mean:  0.5195418433881417\n",
      "validation accuracy:  0.5033377333584763\n",
      "mean loss:  1.6016458929114734\n",
      "epoch:  41\n",
      "train accuracy mean:  0.5197410386866574\n",
      "validation accuracy:  0.501772581557609\n",
      "mean loss:  1.6009723705829155\n",
      "epoch:  42\n",
      "train accuracy mean:  0.5199019319567869\n",
      "validation accuracy:  0.5040543088817651\n",
      "mean loss:  1.59988578476335\n",
      "epoch:  43\n",
      "train accuracy mean:  0.519960863866052\n",
      "validation accuracy:  0.5024514425796719\n",
      "mean loss:  1.5981696331245514\n",
      "epoch:  44\n",
      "train accuracy mean:  0.5204029150628079\n",
      "validation accuracy:  0.5035263058646049\n",
      "mean loss:  1.5969577131007144\n",
      "epoch:  45\n",
      "train accuracy mean:  0.5204700890240619\n",
      "validation accuracy:  0.5053177446728268\n",
      "mean loss:  1.5967476953714852\n",
      "Storing best model to char_best_model. Current acc: 0.5053177446728268, last best metric: 0.5046200264001509\n",
      "epoch:  46\n",
      "train accuracy mean:  0.5208453870249806\n",
      "validation accuracy:  0.504733169903828\n",
      "mean loss:  1.5955332660326083\n",
      "epoch:  47\n",
      "train accuracy mean:  0.521607034306737\n",
      "validation accuracy:  0.5027343013388648\n",
      "mean loss:  1.5942400401011225\n",
      "epoch:  48\n",
      "train accuracy mean:  0.5214265630692708\n",
      "validation accuracy:  0.5028663020931549\n",
      "mean loss:  1.5939784180850056\n",
      "epoch:  49\n",
      "train accuracy mean:  0.5223921206973023\n",
      "validation accuracy:  0.503243447105412\n",
      "mean loss:  1.59203168551958\n",
      "epoch:  50\n",
      "train accuracy mean:  0.522054221316669\n",
      "validation accuracy:  0.5054686026777296\n",
      "mean loss:  1.5910703004870936\n",
      "Storing best model to char_best_model. Current acc: 0.5054686026777296, last best metric: 0.5053177446728268\n",
      "epoch:  51\n",
      "train accuracy mean:  0.5219724690712667\n",
      "validation accuracy:  0.5054874599283424\n",
      "mean loss:  1.5910620407726648\n",
      "Storing best model to char_best_model. Current acc: 0.5054874599283424, last best metric: 0.5054686026777296\n",
      "epoch:  52\n",
      "train accuracy mean:  0.5222776353775602\n",
      "validation accuracy:  0.506090891947954\n",
      "mean loss:  1.5897803386343254\n",
      "Storing best model to char_best_model. Current acc: 0.506090891947954, last best metric: 0.5054874599283424\n",
      "epoch:  53\n",
      "train accuracy mean:  0.5222885505274913\n",
      "validation accuracy:  0.503658306618895\n",
      "mean loss:  1.589494605620245\n",
      "epoch:  54\n",
      "train accuracy mean:  0.5224554953943512\n",
      "validation accuracy:  0.5061474636997926\n",
      "mean loss:  1.5885921026858993\n",
      "Storing best model to char_best_model. Current acc: 0.5061474636997926, last best metric: 0.506090891947954\n",
      "epoch:  55\n",
      "train accuracy mean:  0.5227246862575045\n",
      "validation accuracy:  0.5046954554026023\n",
      "mean loss:  1.5869951332294685\n",
      "epoch:  56\n",
      "train accuracy mean:  0.5232093956422563\n",
      "validation accuracy:  0.5049405996605695\n",
      "mean loss:  1.5867155435754563\n",
      "epoch:  57\n",
      "train accuracy mean:  0.5228217098124475\n",
      "validation accuracy:  0.5060343201961154\n",
      "mean loss:  1.5849484950537713\n",
      "epoch:  58\n",
      "train accuracy mean:  0.5234755545193176\n",
      "validation accuracy:  0.506090891947954\n",
      "mean loss:  1.5858079163864902\n",
      "epoch:  59\n",
      "train accuracy mean:  0.5236401728009314\n",
      "validation accuracy:  0.5053743164246652\n",
      "mean loss:  1.5832882435983597\n",
      "epoch:  60\n",
      "train accuracy mean:  0.5236358166413217\n",
      "validation accuracy:  0.5069206109749198\n",
      "mean loss:  1.5835082382664778\n",
      "Storing best model to char_best_model. Current acc: 0.5069206109749198, last best metric: 0.5061474636997926\n",
      "epoch:  61\n",
      "train accuracy mean:  0.5241351043102219\n",
      "validation accuracy:  0.5071091834810485\n",
      "mean loss:  1.5826267333661332\n",
      "Storing best model to char_best_model. Current acc: 0.5071091834810485, last best metric: 0.5069206109749198\n",
      "epoch:  62\n",
      "train accuracy mean:  0.5239628751247445\n",
      "validation accuracy:  0.5072600414859514\n",
      "mean loss:  1.5825126408533594\n",
      "Storing best model to char_best_model. Current acc: 0.5072600414859514, last best metric: 0.5071091834810485\n",
      "epoch:  63\n",
      "train accuracy mean:  0.5238804298539499\n",
      "validation accuracy:  0.50744861399208\n",
      "mean loss:  1.5823450338385583\n",
      "Storing best model to char_best_model. Current acc: 0.50744861399208, last best metric: 0.5072600414859514\n",
      "epoch:  64\n",
      "train accuracy mean:  0.5246094678159008\n",
      "validation accuracy:  0.5066000377145012\n",
      "mean loss:  1.5805970864712227\n",
      "epoch:  65\n",
      "train accuracy mean:  0.5243085339146827\n",
      "validation accuracy:  0.506486894210824\n",
      "mean loss:  1.5808533461257168\n",
      "epoch:  66\n",
      "train accuracy mean:  0.5243166274612302\n",
      "validation accuracy:  0.5070148972279842\n",
      "mean loss:  1.579520926448106\n",
      "epoch:  67\n",
      "train accuracy mean:  0.52438450682333\n",
      "validation accuracy:  0.5063926079577598\n",
      "mean loss:  1.5798212310374251\n",
      "epoch:  68\n",
      "train accuracy mean:  0.5245403261575504\n",
      "validation accuracy:  0.5077691872524985\n",
      "mean loss:  1.578016116926948\n",
      "Storing best model to char_best_model. Current acc: 0.5077691872524985, last best metric: 0.50744861399208\n",
      "epoch:  69\n",
      "train accuracy mean:  0.5243357104104295\n",
      "validation accuracy:  0.5066000377145012\n",
      "mean loss:  1.5784297979008883\n",
      "epoch:  70\n",
      "train accuracy mean:  0.5252840166563703\n",
      "validation accuracy:  0.5082029040165944\n",
      "mean loss:  1.578421960996198\n",
      "Storing best model to char_best_model. Current acc: 0.5082029040165944, last best metric: 0.5077691872524985\n",
      "epoch:  71\n",
      "train accuracy mean:  0.5250039848960066\n",
      "validation accuracy:  0.5073543277390157\n",
      "mean loss:  1.5770860774568158\n",
      "epoch:  72\n",
      "train accuracy mean:  0.5251243980579449\n",
      "validation accuracy:  0.5079200452574014\n",
      "mean loss:  1.5763145157384797\n",
      "epoch:  73\n",
      "train accuracy mean:  0.5253230240856025\n",
      "validation accuracy:  0.5070714689798227\n",
      "mean loss:  1.575807356734707\n",
      "epoch:  74\n",
      "train accuracy mean:  0.5247290270715519\n",
      "validation accuracy:  0.5066377522157269\n",
      "mean loss:  1.5757553450299755\n",
      "epoch:  75\n",
      "train accuracy mean:  0.5256578419783301\n",
      "validation accuracy:  0.504733169903828\n",
      "mean loss:  1.5754738321992283\n",
      "epoch:  76\n",
      "train accuracy mean:  0.5256926912552076\n",
      "validation accuracy:  0.5068640392230813\n",
      "mean loss:  1.5743819762447775\n",
      "epoch:  77\n",
      "train accuracy mean:  0.5261279730789337\n",
      "validation accuracy:  0.5063548934565341\n",
      "mean loss:  1.573529511861507\n",
      "epoch:  78\n",
      "train accuracy mean:  0.5257067745212185\n",
      "validation accuracy:  0.5086177635300773\n",
      "mean loss:  1.5740207205003178\n",
      "Storing best model to char_best_model. Current acc: 0.5086177635300773, last best metric: 0.5082029040165944\n",
      "epoch:  79\n",
      "train accuracy mean:  0.5266412821365775\n",
      "validation accuracy:  0.5086743352819159\n",
      "mean loss:  1.5726940904807745\n",
      "Storing best model to char_best_model. Current acc: 0.5086743352819159, last best metric: 0.5086177635300773\n",
      "epoch:  80\n",
      "train accuracy mean:  0.5260865029344675\n",
      "validation accuracy:  0.5081086177635301\n",
      "mean loss:  1.5716662529682652\n",
      "epoch:  81\n",
      "train accuracy mean:  0.5255463020165059\n",
      "validation accuracy:  0.506901753724307\n",
      "mean loss:  1.5730111726715\n",
      "epoch:  82\n",
      "train accuracy mean:  0.5259694806467708\n",
      "validation accuracy:  0.5051291721666981\n",
      "mean loss:  1.5721573394689645\n",
      "epoch:  83\n",
      "train accuracy mean:  0.5261556445928179\n",
      "validation accuracy:  0.506222892702244\n",
      "mean loss:  1.5717371602489634\n",
      "epoch:  84\n",
      "train accuracy mean:  0.5266540288536172\n",
      "validation accuracy:  0.50771261550066\n",
      "mean loss:  1.570691915687565\n",
      "epoch:  85\n",
      "train accuracy mean:  0.5268205529550603\n",
      "validation accuracy:  0.5076183292475956\n",
      "mean loss:  1.5711279255024002\n",
      "epoch:  86\n",
      "train accuracy mean:  0.5266802895658097\n",
      "validation accuracy:  0.508806336036206\n",
      "mean loss:  1.5700427812241287\n",
      "Storing best model to char_best_model. Current acc: 0.508806336036206, last best metric: 0.5086743352819159\n",
      "epoch:  87\n",
      "train accuracy mean:  0.5262414807378543\n",
      "validation accuracy:  0.5072411842353385\n",
      "mean loss:  1.5694908162280021\n",
      "epoch:  88\n",
      "train accuracy mean:  0.5268292157724659\n",
      "validation accuracy:  0.5070903262304356\n",
      "mean loss:  1.5695634589915617\n",
      "epoch:  89\n",
      "train accuracy mean:  0.5268084373861458\n",
      "validation accuracy:  0.5073920422402414\n",
      "mean loss:  1.5677971388741556\n",
      "epoch:  90\n",
      "train accuracy mean:  0.5270891745473554\n",
      "validation accuracy:  0.5073731849896285\n",
      "mean loss:  1.568617004782382\n",
      "epoch:  91\n",
      "train accuracy mean:  0.5271943040242995\n",
      "validation accuracy:  0.5069771827267584\n",
      "mean loss:  1.5679313494656888\n",
      "epoch:  92\n",
      "train accuracy mean:  0.5270798063291039\n",
      "validation accuracy:  0.5090703375447859\n",
      "mean loss:  1.5668720608286002\n",
      "Storing best model to char_best_model. Current acc: 0.5090703375447859, last best metric: 0.508806336036206\n",
      "epoch:  93\n",
      "train accuracy mean:  0.5266705005821413\n",
      "validation accuracy:  0.5091269092966245\n",
      "mean loss:  1.5678902841399494\n",
      "Storing best model to char_best_model. Current acc: 0.5091269092966245, last best metric: 0.5090703375447859\n",
      "epoch:  94\n",
      "train accuracy mean:  0.5272706976983637\n",
      "validation accuracy:  0.5086743352819159\n",
      "mean loss:  1.5665444517659068\n",
      "epoch:  95\n",
      "train accuracy mean:  0.5277114742234156\n",
      "validation accuracy:  0.5060531774467283\n",
      "mean loss:  1.5662152385811374\n",
      "epoch:  96\n",
      "train accuracy mean:  0.5275096800796781\n",
      "validation accuracy:  0.5069206109749198\n",
      "mean loss:  1.5660156308825703\n",
      "epoch:  97\n",
      "train accuracy mean:  0.5277725223352183\n",
      "validation accuracy:  0.5072977559871771\n",
      "mean loss:  1.5658066702711602\n",
      "epoch:  98\n",
      "train accuracy mean:  0.5274395483850528\n",
      "validation accuracy:  0.5094851970582689\n",
      "mean loss:  1.5661837626550439\n",
      "Storing best model to char_best_model. Current acc: 0.5094851970582689, last best metric: 0.5091269092966245\n",
      "epoch:  99\n",
      "train accuracy mean:  0.527280969324716\n",
      "validation accuracy:  0.5093531963039789\n",
      "mean loss:  1.5651899813608667\n",
      "epoch:  100\n",
      "train accuracy mean:  0.5278289915292496\n",
      "validation accuracy:  0.5088440505374316\n",
      "mean loss:  1.5647787510632098\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    epoch_losses = []\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "\n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # close for each step\n",
    "\n",
    "    # get metric for training set\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "\n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    print('mean loss: ', np.mean(epoch_losses))\n",
    "\n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'char_best_model', val_acc, best_metric)\n",
    "\n",
    "    # patience and last_metric and best_metric update\n",
    "    last_metric = val_acc\n",
    "    counter = counter + 1 if last_metric <= best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "\n",
    "    # check if patience run out\n",
    "    if counter >= patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_state = torch.load('char_best_model')\n",
    "model.load_state_dict(load_state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5226664152366585"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_model(val_loader, model, use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NGram Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGramModel = NGramNeuralModel(char_ngram_builder, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>padre por 35 el que se valer tal al estoy loca jajajaja de lefandarse no tengo monzaba que con\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>copanaduerdos aún así juegas a togerdistaso ajenatacianta que se volvera putas cosalo idea\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu, max_length=300)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s>el robo son por atuger mañana de puto en no dejones puto así solo madre tonen no de verga que te wará el arme este el tiemposin no lo tembici el poctando maricónsiso me de que marica de verga de páginas del my pagudite y ella a sentalya jajajajajajaja\n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence(use_gpu=use_gpu, max_length=300)\n",
    "print_doc(seq, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Probability Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.710024"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete a la verga', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.1000185"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('a la vete verga', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.086407"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('esos hijos de la chingada', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-21.678864"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('esos chingada de los hijos', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.1125507"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('estuvieron', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.756233"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('estuveiron', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.3073807"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete alv', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.073044"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('vete avl', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.13043"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('veet avl', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r o   a m\n",
      "likelihood:  -8.757793\n",
      "\n",
      "  r o a m\n",
      "likelihood:  -9.794529\n",
      "\n",
      "o r   a m\n",
      "likelihood:  -10.665074\n",
      "\n",
      "m o   a r\n",
      "likelihood:  -11.240616\n",
      "\n",
      "r   o a m\n",
      "likelihood:  -11.738606\n",
      "\n",
      "  m o a r\n",
      "likelihood:  -11.784495\n",
      "\n",
      "r m o   a\n",
      "likelihood:  -12.62795\n",
      "\n",
      "r a   o m\n",
      "likelihood:  -12.659768\n",
      "\n",
      "r m o a  \n",
      "likelihood:  -12.839628\n",
      "\n",
      "o m   a r\n",
      "likelihood:  -12.941523\n",
      "\n",
      "a r   o m\n",
      "likelihood:  -12.956074\n",
      "\n",
      "m   o a r\n",
      "likelihood:  -13.132235\n",
      "\n",
      "r m   o a\n",
      "likelihood:  -13.270688\n",
      "\n",
      "m r o a  \n",
      "likelihood:  -13.477239\n",
      "\n",
      "a r o m  \n",
      "likelihood:  -13.480266\n",
      "\n",
      "m r o   a\n",
      "likelihood:  -13.740321\n",
      "\n",
      "r o m a  \n",
      "likelihood:  -13.768693\n",
      "\n",
      "r m a   o\n",
      "likelihood:  -13.921881\n",
      "\n",
      "o m a   r\n",
      "likelihood:  -13.964831\n",
      "\n",
      "  r o m a\n",
      "likelihood:  -14.001951\n",
      "\n",
      "r m   a o\n",
      "likelihood:  -14.005707\n",
      "\n",
      "m r   a o\n",
      "likelihood:  -14.123345\n",
      "\n",
      "m r   o a\n",
      "likelihood:  -14.132148\n",
      "\n",
      "r   a o m\n",
      "likelihood:  -14.150971\n",
      "\n",
      "a r o   m\n",
      "likelihood:  -14.210142\n",
      "\n",
      "a m o   r\n",
      "likelihood:  -14.219949\n",
      "\n",
      "r o a m  \n",
      "likelihood:  -14.234589\n",
      "\n",
      "r   o m a\n",
      "likelihood:  -14.533393\n",
      "\n",
      "o r a   m\n",
      "likelihood:  -14.698946\n",
      "\n",
      "o r a m  \n",
      "likelihood:  -14.706935\n",
      "\n",
      "a m   o r\n",
      "likelihood:  -14.878987\n",
      "\n",
      "  a o m r\n",
      "likelihood:  -14.896548\n",
      "\n",
      "a   o m r\n",
      "likelihood:  -14.983065\n",
      "\n",
      "  o m a r\n",
      "likelihood:  -14.995438\n",
      "\n",
      "r a o m  \n",
      "likelihood:  -15.080059\n",
      "\n",
      "  o a m r\n",
      "likelihood:  -15.081883\n",
      "\n",
      "m a   o r\n",
      "likelihood:  -15.101563\n",
      "\n",
      "m r a   o\n",
      "likelihood:  -15.160372\n",
      "\n",
      "  r a o m\n",
      "likelihood:  -15.177333\n",
      "\n",
      "o   m a r\n",
      "likelihood:  -15.279388\n",
      "\n",
      "a o   m r\n",
      "likelihood:  -15.37584\n",
      "\n",
      "o   a m r\n",
      "likelihood:  -15.427321\n",
      "\n",
      "a o m   r\n",
      "likelihood:  -15.441156\n",
      "\n",
      "r o m   a\n",
      "likelihood:  -15.501533\n",
      "\n",
      "  m a o r\n",
      "likelihood:  -16.014069\n",
      "\n",
      "  m r a o\n",
      "likelihood:  -16.178854\n",
      "\n",
      "r o   m a\n",
      "likelihood:  -16.306595\n",
      "\n",
      "r o a   m\n",
      "likelihood:  -16.34736\n",
      "\n",
      "r   a m o\n",
      "likelihood:  -16.458439\n",
      "\n",
      "o r m   a\n",
      "likelihood:  -16.475891\n",
      "\n",
      "o m a r  \n",
      "likelihood:  -16.51433\n",
      "\n",
      "m o a   r\n",
      "likelihood:  -16.538872\n",
      "\n",
      "o m   r a\n",
      "likelihood:  -16.604118\n",
      "\n",
      "o a   m r\n",
      "likelihood:  -16.679476\n",
      "\n",
      "m   a o r\n",
      "likelihood:  -16.840668\n",
      "\n",
      "o r m a  \n",
      "likelihood:  -16.963787\n",
      "\n",
      "  m a r o\n",
      "likelihood:  -16.966805\n",
      "\n",
      "m r a o  \n",
      "likelihood:  -17.128593\n",
      "\n",
      "o m r   a\n",
      "likelihood:  -17.136257\n",
      "\n",
      "o m r a  \n",
      "likelihood:  -17.362505\n",
      "\n",
      "a m r   o\n",
      "likelihood:  -17.416498\n",
      "\n",
      "r m a o  \n",
      "likelihood:  -17.529531\n",
      "\n",
      "r   m a o\n",
      "likelihood:  -17.56765\n",
      "\n",
      "  r a m o\n",
      "likelihood:  -17.77716\n",
      "\n",
      "m   r a o\n",
      "likelihood:  -17.836134\n",
      "\n",
      "m a o   r\n",
      "likelihood:  -17.862913\n",
      "\n",
      "a r m   o\n",
      "likelihood:  -17.963978\n",
      "\n",
      "m   r o a\n",
      "likelihood:  -18.113659\n",
      "\n",
      "a m o r  \n",
      "likelihood:  -18.142649\n",
      "\n",
      "o a m   r\n",
      "likelihood:  -18.151566\n",
      "\n",
      "r   m o a\n",
      "likelihood:  -18.276093\n",
      "\n",
      "r a o   m\n",
      "likelihood:  -18.313536\n",
      "\n",
      "m o r a  \n",
      "likelihood:  -18.370077\n",
      "\n",
      "o r   m a\n",
      "likelihood:  -18.54717\n",
      "\n",
      "  o a r m\n",
      "likelihood:  -18.629086\n",
      "\n",
      "m o a r  \n",
      "likelihood:  -18.629454\n",
      "\n",
      "a o m r  \n",
      "likelihood:  -18.682661\n",
      "\n",
      "  o r a m\n",
      "likelihood:  -18.71208\n",
      "\n",
      "a o   r m\n",
      "likelihood:  -18.791803\n",
      "\n",
      "  r m o a\n",
      "likelihood:  -18.936193\n",
      "\n",
      "  m r o a\n",
      "likelihood:  -19.076874\n",
      "\n",
      "o   r a m\n",
      "likelihood:  -19.16842\n",
      "\n",
      "  m o r a\n",
      "likelihood:  -19.169783\n",
      "\n",
      "o   a r m\n",
      "likelihood:  -19.171225\n",
      "\n",
      "m o   r a\n",
      "likelihood:  -19.247086\n",
      "\n",
      "a m   r o\n",
      "likelihood:  -19.255072\n",
      "\n",
      "r a m   o\n",
      "likelihood:  -19.296135\n",
      "\n",
      "m o r   a\n",
      "likelihood:  -19.364283\n",
      "\n",
      "o a   r m\n",
      "likelihood:  -19.389141\n",
      "\n",
      "m a r o  \n",
      "likelihood:  -19.421803\n",
      "\n",
      "r a m o  \n",
      "likelihood:  -19.504116\n",
      "\n",
      "r a   m o\n",
      "likelihood:  -19.643324\n",
      "\n",
      "  r m a o\n",
      "likelihood:  -19.674217\n",
      "\n",
      "o   m r a\n",
      "likelihood:  -19.840435\n",
      "\n",
      "a r m o  \n",
      "likelihood:  -20.08249\n",
      "\n",
      "a   m o r\n",
      "likelihood:  -20.209583\n",
      "\n",
      "m a r   o\n",
      "likelihood:  -20.244722\n",
      "\n",
      "a r   m o\n",
      "likelihood:  -20.297892\n",
      "\n",
      "  a r o m\n",
      "likelihood:  -20.62782\n",
      "\n",
      "o a m r  \n",
      "likelihood:  -20.659174\n",
      "\n",
      "m   a r o\n",
      "likelihood:  -20.99902\n",
      "\n",
      "  a m o r\n",
      "likelihood:  -21.015709\n",
      "\n",
      "m   o r a\n",
      "likelihood:  -21.455397\n",
      "\n",
      "a   r o m\n",
      "likelihood:  -21.473293\n",
      "\n",
      "o a r m  \n",
      "likelihood:  -21.706776\n",
      "\n",
      "a m r o  \n",
      "likelihood:  -21.726063\n",
      "\n",
      "  o m r a\n",
      "likelihood:  -21.753942\n",
      "\n",
      "o a r   m\n",
      "likelihood:  -21.771223\n",
      "\n",
      "m a o r  \n",
      "likelihood:  -22.066513\n",
      "\n",
      "a o r   m\n",
      "likelihood:  -22.226685\n",
      "\n",
      "m a   r o\n",
      "likelihood:  -22.346434\n",
      "\n",
      "a   o r m\n",
      "likelihood:  -22.35689\n",
      "\n",
      "  a o r m\n",
      "likelihood:  -23.179127\n",
      "\n",
      "a o r m  \n",
      "likelihood:  -23.382088\n",
      "\n",
      "a   m r o\n",
      "likelihood:  -23.936272\n",
      "\n",
      "o   r m a\n",
      "likelihood:  -24.32052\n",
      "\n",
      "a   r m o\n",
      "likelihood:  -25.294746\n",
      "\n",
      "  o r m a\n",
      "likelihood:  -25.624954\n",
      "\n",
      "  a r m o\n",
      "likelihood:  -26.28173\n",
      "\n",
      "  a m r o\n",
      "likelihood:  -27.342218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_structures(list('amor '), NGramModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.051324235262274"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.perplexity(val_documents, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder(d_model=100)\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_loader, val_ds, val_loader = get_datasets(ngram_builder, 4, documents, val_documents, batch_size=64, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "voc_size = ngram_builder.voc_size\n",
    "N = ngram_builder.N\n",
    "d_model = ngram_builder.d_model\n",
    "\n",
    "# optimizer hyperparameters\n",
    "lr = 2.3e-1 \n",
    "epochs = 100\n",
    "patience = epochs//5\n",
    "\n",
    "# scheduler hyperparameters\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, hidden_size=200, emb_mat=ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "# optimizer and scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'min',\n",
    "                patience = lr_patience,\n",
    "                verbose=True,\n",
    "                factor = lr_factor\n",
    "            )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "train accuracy mean:  0.061523921906001984\n",
      "validation accuracy:  0.04769953051643192\n",
      "mean loss:  6.571348320382337\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.04769953051643192, last best metric: 0\n",
      "epoch:  2\n",
      "train accuracy mean:  0.09239850725446429\n",
      "validation accuracy:  0.06582159624413146\n",
      "mean loss:  6.177088140820463\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.06582159624413146, last best metric: 0.04769953051643192\n",
      "epoch:  3\n",
      "train accuracy mean:  0.10496206132192461\n",
      "validation accuracy:  0.11126760563380282\n",
      "mean loss:  5.9777576358368\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.11126760563380282, last best metric: 0.06582159624413146\n",
      "epoch:  4\n",
      "train accuracy mean:  0.11347501240079365\n",
      "validation accuracy:  0.11774647887323944\n",
      "mean loss:  5.826832549025615\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.11774647887323944, last best metric: 0.11126760563380282\n",
      "epoch:  5\n",
      "train accuracy mean:  0.11999657040550595\n",
      "validation accuracy:  0.13380281690140844\n",
      "mean loss:  5.697683136289318\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.13380281690140844, last best metric: 0.11774647887323944\n",
      "epoch:  6\n",
      "train accuracy mean:  0.12613302563864087\n",
      "validation accuracy:  0.12347417840375587\n",
      "mean loss:  5.587139288584392\n",
      "epoch:  7\n",
      "train accuracy mean:  0.13000633603050596\n",
      "validation accuracy:  0.12685446009389673\n",
      "mean loss:  5.4850735279421015\n",
      "epoch:  8\n",
      "train accuracy mean:  0.13393293108258927\n",
      "validation accuracy:  0.13784037558685447\n",
      "mean loss:  5.388621700617175\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.13784037558685447, last best metric: 0.13380281690140844\n",
      "epoch:  9\n",
      "train accuracy mean:  0.1360599578373016\n",
      "validation accuracy:  0.11539906103286385\n",
      "mean loss:  5.294623562445243\n",
      "epoch:  10\n",
      "train accuracy mean:  0.13928416418650794\n",
      "validation accuracy:  0.12629107981220658\n",
      "mean loss:  5.2028829818591475\n",
      "epoch:  11\n",
      "train accuracy mean:  0.14136856321304564\n",
      "validation accuracy:  0.13661971830985917\n",
      "mean loss:  5.108273976482451\n",
      "epoch:  12\n",
      "train accuracy mean:  0.14356582883804564\n",
      "validation accuracy:  0.1504225352112676\n",
      "mean loss:  5.02049371569107\n",
      "Storing best model to no_embeddings_best_model. Current acc: 0.1504225352112676, last best metric: 0.13784037558685447\n",
      "epoch:  13\n",
      "train accuracy mean:  0.14497060624379962\n",
      "validation accuracy:  0.12544600938967135\n",
      "mean loss:  4.93139614801233\n",
      "epoch:  14\n",
      "train accuracy mean:  0.14849950396825398\n",
      "validation accuracy:  0.12140845070422535\n",
      "mean loss:  4.839481624774635\n",
      "epoch:  15\n",
      "train accuracy mean:  0.15221344478546628\n",
      "validation accuracy:  0.10431924882629108\n",
      "mean loss:  4.745154721507181\n",
      "epoch:  16\n",
      "train accuracy mean:  0.15464467850942462\n",
      "validation accuracy:  0.1187793427230047\n",
      "mean loss:  4.6530741741880774\n",
      "epoch:  17\n",
      "train accuracy mean:  0.15659731910342262\n",
      "validation accuracy:  0.11981220657276995\n",
      "mean loss:  4.561925323369603\n",
      "epoch:  18\n",
      "train accuracy mean:  0.1595584929935516\n",
      "validation accuracy:  0.14544600938967137\n",
      "mean loss:  4.466656539899607\n",
      "epoch:  19\n",
      "train accuracy mean:  0.16137840634300596\n",
      "validation accuracy:  0.11859154929577465\n",
      "mean loss:  4.372872122563422\n",
      "epoch:  20\n",
      "train accuracy mean:  0.16677904885912698\n",
      "validation accuracy:  0.12300469483568074\n",
      "mean loss:  4.2811859484451515\n",
      "epoch:  21\n",
      "train accuracy mean:  0.17110431005084326\n",
      "validation accuracy:  0.12892018779342723\n",
      "mean loss:  4.18853772788619\n",
      "epoch:  22\n",
      "train accuracy mean:  0.17624967060391863\n",
      "validation accuracy:  0.14938967136150236\n",
      "mean loss:  4.095169155237575\n",
      "epoch:  23\n",
      "train accuracy mean:  0.1814386276971726\n",
      "validation accuracy:  0.12901408450704224\n",
      "mean loss:  4.003703396767378\n",
      "epoch:  24\n",
      "train accuracy mean:  0.18919057694692462\n",
      "validation accuracy:  0.11708920187793427\n",
      "mean loss:  3.9148344749895236\n",
      "epoch:  25\n",
      "train accuracy mean:  0.19848099965897817\n",
      "validation accuracy:  0.14572769953051642\n",
      "mean loss:  3.820509884506464\n",
      "epoch:  26\n",
      "train accuracy mean:  0.20800829690600198\n",
      "validation accuracy:  0.09915492957746479\n",
      "mean loss:  3.7324758215496936\n",
      "epoch:  27\n",
      "train accuracy mean:  0.2204095749627976\n",
      "validation accuracy:  0.10732394366197183\n",
      "mean loss:  3.650149382495632\n",
      "epoch:  28\n",
      "train accuracy mean:  0.23112172929067462\n",
      "validation accuracy:  0.09727699530516432\n",
      "mean loss:  3.5636288731669388\n",
      "epoch:  29\n",
      "train accuracy mean:  0.24517095656622023\n",
      "validation accuracy:  0.10103286384976526\n",
      "mean loss:  3.4827060600121817\n",
      "epoch:  30\n",
      "train accuracy mean:  0.2590554858010913\n",
      "validation accuracy:  0.11896713615023474\n",
      "mean loss:  3.401087317150086\n",
      "epoch:  31\n",
      "train accuracy mean:  0.27224004836309523\n",
      "validation accuracy:  0.10816901408450705\n",
      "mean loss:  3.3229132615961134\n",
      "epoch:  32\n",
      "train accuracy mean:  0.28614589146205355\n",
      "validation accuracy:  0.13004694835680752\n",
      "mean loss:  3.2472581402398646\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    epoch_losses = []\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "\n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # close for each step\n",
    "\n",
    "    # get metric for training set\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "\n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    print('mean loss: ', np.mean(epoch_losses))\n",
    "\n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'no_embeddings_best_model', val_acc, best_metric)\n",
    "\n",
    "    # patience and last_metric and best_metric update\n",
    "    last_metric = val_acc\n",
    "    counter = counter + 1 if last_metric <= best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "\n",
    "    # check if patience run out\n",
    "    if counter >= patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioModel(\n",
       "  (embeddings): Embedding(10000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=10000, bias=False)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_state = torch.load('no_embeddings_best_model')\n",
    "model.load_state_dict(load_state['model'])\n",
    "model.eval()\n",
    "NGramModel = NGramNeuralModel(ngram_builder, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mchinga\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tata',\n",
       " 'teléfono',\n",
       " 'linchar',\n",
       " 'predicar',\n",
       " 'rededor',\n",
       " 'creó',\n",
       " 'curso',\n",
       " 'pegara',\n",
       " 'sufrir',\n",
       " 'elimino']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'chinga'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mamor\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['puntome',\n",
       " 'narcotrafico',\n",
       " 'entender',\n",
       " 'cool',\n",
       " 'sobredosis',\n",
       " '#milesheizer',\n",
       " 'clientes',\n",
       " 'ves',\n",
       " 'drastico',\n",
       " 'reparaciones']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'amor'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mverga\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['clases',\n",
       " '#btw',\n",
       " 'hablen',\n",
       " 'abdomen',\n",
       " 'blog',\n",
       " 'tardas',\n",
       " 'rayo',\n",
       " 'cabroncito',\n",
       " 'inundo',\n",
       " 'cerebro']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'verga'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185.78585693298882"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.perplexity(val_documents, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self, filename):\n",
    "        self.embeddings = {}\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                values = line.split()\n",
    "                word, rep = values[0], np.array(list(map(float, values[1:])))\n",
    "                self.embeddings[word] = rep\n",
    "                self.d_model = len(rep)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.embeddings.get(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.64168 ,  1.447671, -2.283216, -1.965226, -0.222943,  5.105217,\n",
       "       -0.120701, -0.126822, -3.177338, -3.454396, -0.943083, -0.094476,\n",
       "       -1.18936 , -0.812092, -2.572975, -0.613877, -2.311841,  1.05097 ,\n",
       "        5.634725, -5.827006,  1.237639,  1.071621,  3.822072,  2.395414,\n",
       "        0.169883,  3.256835,  2.897348,  3.274827, -2.936382,  0.272003,\n",
       "       -1.029505, -2.617288, -1.807143,  1.737624,  0.33913 ,  3.93293 ,\n",
       "        1.571361, -4.100074,  4.156816,  1.162366, -0.552316, -0.585887,\n",
       "       -4.767187,  0.253338, -1.124162, -0.115079, -5.606624,  2.976579,\n",
       "        4.426022,  1.019932,  3.76072 , -2.298347,  4.416567, -1.383988,\n",
       "       -1.862506,  0.399053, -1.09689 , -2.28599 ,  2.992802,  0.044008,\n",
       "        3.762375, -6.523126,  0.621278,  2.641829, -1.924327, -1.141184,\n",
       "       -3.831767,  0.549591,  2.260839, -1.318358, -1.134662, -3.788221,\n",
       "       -0.775024,  3.956695, -3.579425, -4.423733,  4.505686,  0.719133,\n",
       "       -1.399557,  3.097209,  0.107541,  2.829867, -0.760249, -0.277246,\n",
       "       -1.225739,  2.157902,  0.518391,  3.437977, -1.087452, -1.529914,\n",
       "        0.399476,  0.887398, -1.994891,  2.912205,  3.257981, -2.599953,\n",
       "       -1.995134, -0.176465,  3.946902, -2.792494])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = Embeddings('data/word2vec_col.txt')\n",
    "embeddings['de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder(embeddings=embeddings)\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "voc_size = ngram_builder.voc_size\n",
    "N = ngram_builder.N\n",
    "d_model = ngram_builder.d_model\n",
    "\n",
    "# optimizer hyperparameters\n",
    "lr = 2.3e-1 \n",
    "epochs = 100\n",
    "patience = epochs//5\n",
    "\n",
    "# scheduler hyperparameters\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, hidden_size=200, emb_mat=ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'min',\n",
    "                patience = lr_patience,\n",
    "                verbose=True,\n",
    "                factor = lr_factor\n",
    "            )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "train accuracy mean:  0.09645589192708333\n",
      "validation accuracy:  0.11126760563380282\n",
      "mean loss:  6.4376890966668725\n",
      "Storing best model to embeddings_best_model. Current acc: 0.11126760563380282, last best metric: 0\n",
      "epoch:  2\n",
      "train accuracy mean:  0.11189923967633929\n",
      "validation accuracy:  0.12929577464788733\n",
      "mean loss:  5.913514082009594\n",
      "Storing best model to embeddings_best_model. Current acc: 0.12929577464788733, last best metric: 0.11126760563380282\n",
      "epoch:  3\n",
      "train accuracy mean:  0.11773778521825397\n",
      "validation accuracy:  0.13483568075117372\n",
      "mean loss:  5.65478578582406\n",
      "Storing best model to embeddings_best_model. Current acc: 0.13483568075117372, last best metric: 0.12929577464788733\n",
      "epoch:  4\n",
      "train accuracy mean:  0.12334284706721231\n",
      "validation accuracy:  0.1031924882629108\n",
      "mean loss:  5.421841428615153\n",
      "epoch:  5\n",
      "train accuracy mean:  0.12587629045758927\n",
      "validation accuracy:  0.11192488262910798\n",
      "mean loss:  5.222658311327298\n",
      "epoch:  6\n",
      "train accuracy mean:  0.13140966021825398\n",
      "validation accuracy:  0.09652582159624414\n",
      "mean loss:  5.042903200102349\n",
      "epoch:  7\n",
      "train accuracy mean:  0.13552953326512898\n",
      "validation accuracy:  0.10112676056338028\n",
      "mean loss:  4.8759124561523395\n",
      "epoch:  8\n",
      "train accuracy mean:  0.14230443560887898\n",
      "validation accuracy:  0.11455399061032864\n",
      "mean loss:  4.724634554858009\n",
      "epoch:  9\n",
      "train accuracy mean:  0.15042308020213294\n",
      "validation accuracy:  0.10676056338028168\n",
      "mean loss:  4.592039776345094\n",
      "epoch:  10\n",
      "train accuracy mean:  0.15995425269717262\n",
      "validation accuracy:  0.107981220657277\n",
      "mean loss:  4.471277489637335\n",
      "epoch:  11\n",
      "train accuracy mean:  0.16726829892113093\n",
      "validation accuracy:  0.12169014084507042\n",
      "mean loss:  4.373410621657968\n",
      "epoch:  12\n",
      "train accuracy mean:  0.17322843036954363\n",
      "validation accuracy:  0.1116431924882629\n",
      "mean loss:  4.288940267637372\n",
      "epoch:  13\n",
      "train accuracy mean:  0.18351430741567462\n",
      "validation accuracy:  0.1031924882629108\n",
      "mean loss:  4.202282222298284\n",
      "epoch:  14\n",
      "train accuracy mean:  0.19330027746775794\n",
      "validation accuracy:  0.1107981220657277\n",
      "mean loss:  4.127642184185485\n",
      "epoch:  15\n",
      "train accuracy mean:  0.2021813771081349\n",
      "validation accuracy:  0.09708920187793427\n",
      "mean loss:  4.059348055627197\n",
      "epoch:  16\n",
      "train accuracy mean:  0.20594327411954363\n",
      "validation accuracy:  0.09483568075117371\n",
      "mean loss:  4.007714653232445\n",
      "epoch:  17\n",
      "train accuracy mean:  0.2118656218998016\n",
      "validation accuracy:  0.09145539906103287\n",
      "mean loss:  3.952876319953551\n",
      "epoch:  18\n",
      "train accuracy mean:  0.21832372271825395\n",
      "validation accuracy:  0.10553990610328638\n",
      "mean loss:  3.91160049661994\n",
      "epoch:  19\n",
      "train accuracy mean:  0.22572011796254962\n",
      "validation accuracy:  0.1104225352112676\n",
      "mean loss:  3.8566857861975827\n",
      "epoch:  20\n",
      "train accuracy mean:  0.23158966548859128\n",
      "validation accuracy:  0.10666666666666667\n",
      "mean loss:  3.826118007923166\n",
      "epoch:  21\n",
      "train accuracy mean:  0.23635040767609128\n",
      "validation accuracy:  0.11136150234741785\n",
      "mean loss:  3.780139407142997\n",
      "epoch:  22\n",
      "train accuracy mean:  0.24123322017609128\n",
      "validation accuracy:  0.10751173708920188\n",
      "mean loss:  3.749281892708192\n",
      "epoch:  23\n",
      "train accuracy mean:  0.2456398615761409\n",
      "validation accuracy:  0.10244131455399061\n",
      "mean loss:  3.7143210005015135\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    epoch_losses = []\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "\n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # close for each step\n",
    "\n",
    "    # get metric for training set\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "\n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    print('mean loss: ', np.mean(epoch_losses))\n",
    "\n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'embeddings_best_model', val_acc, best_metric)\n",
    "\n",
    "    # patience and last_metric and best_metric update\n",
    "    last_metric = val_acc\n",
    "    counter = counter + 1 if last_metric <= best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "\n",
    "    # check if patience run out\n",
    "    if counter >= patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioModel(\n",
       "  (embeddings): Embedding(10000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=10000, bias=False)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_state = torch.load('embeddings_best_model')\n",
    "model.load_state_dict(load_state['model'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGramModel = NGramNeuralModel(ngram_builder, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vete', 'a', 'la', 'verga', '🤔', '</s>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence('vete a la'.split(), use_gpu=use_gpu)\n",
    "print_doc(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinga', 'a', 'tu', 'madre', 'mátenme', '</s>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence('chinga a tu '.split(), use_gpu=use_gpu)\n",
    "print_doc(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estoy a punto vergazos este capítulo de como 💩 haz … <unk> a unos putos putos \n"
     ]
    }
   ],
   "source": [
    "seq = NGramModel.generate_sequence('estoy a punto'.split(), use_gpu=use_gpu)\n",
    "print_doc(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.998171"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('voy a estar en mi casa', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19.667858"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('voy a en estar mi casa', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.023245202"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('chinga a tu madre', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.934837"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('chinga a tu padre', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.591314"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.estimate_prob('a madre tu chinga', use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a vas chingar a tu madre\n",
      "likelihood:  -4.3084855\n",
      "\n",
      "vas a chingar a tu madre\n",
      "likelihood:  -5.482989\n",
      "\n",
      "vas madre a chingar a tu\n",
      "likelihood:  -7.2738085\n",
      "\n",
      "chingar a tu madre vas a\n",
      "likelihood:  -7.515975\n",
      "\n",
      "vas a chingar tu madre a\n",
      "likelihood:  -7.5483837\n",
      "\n",
      "tu vas madre a chingar a\n",
      "likelihood:  -7.9070125\n",
      "\n",
      "tu madre vas a chingar a\n",
      "likelihood:  -7.93376\n",
      "\n",
      "a chingar tu madre vas a\n",
      "likelihood:  -7.964194\n",
      "\n",
      "a vas a chingar tu madre\n",
      "likelihood:  -8.242043\n",
      "\n",
      "a chingar vas a tu madre\n",
      "likelihood:  -8.520243\n",
      "\n",
      "a vas chingar tu madre a\n",
      "likelihood:  -8.681716\n",
      "\n",
      "a madre vas a chingar tu\n",
      "likelihood:  -8.682941\n",
      "\n",
      "madre a vas a chingar tu\n",
      "likelihood:  -8.961501\n",
      "\n",
      "vas chingar a tu madre a\n",
      "likelihood:  -9.287172\n",
      "\n",
      "madre tu vas a chingar a\n",
      "likelihood:  -9.424377\n",
      "\n",
      "a chingar vas tu madre a\n",
      "likelihood:  -9.499789\n",
      "\n",
      "a vas tu madre chingar a\n",
      "likelihood:  -9.680814\n",
      "\n",
      "vas tu madre a chingar a\n",
      "likelihood:  -9.72591\n",
      "\n",
      "a vas tu madre a chingar\n",
      "likelihood:  -9.936181\n",
      "\n",
      "madre tu chingar a vas a\n",
      "likelihood:  -10.059231\n",
      "\n",
      "chingar a vas a tu madre\n",
      "likelihood:  -10.470872\n",
      "\n",
      "a tu chingar madre vas a\n",
      "likelihood:  -10.482743\n",
      "\n",
      "a chingar a tu madre vas\n",
      "likelihood:  -10.483976\n",
      "\n",
      "tu a vas a chingar madre\n",
      "likelihood:  -10.752337\n",
      "\n",
      "vas a tu madre a chingar\n",
      "likelihood:  -10.883236\n",
      "\n",
      "madre vas a chingar a tu\n",
      "likelihood:  -10.901761\n",
      "\n",
      "vas a tu madre chingar a\n",
      "likelihood:  -11.017798\n",
      "\n",
      "vas a a chingar tu madre\n",
      "likelihood:  -11.089592\n",
      "\n",
      "tu vas chingar a madre a\n",
      "likelihood:  -11.14246\n",
      "\n",
      "tu madre chingar a vas a\n",
      "likelihood:  -11.295541\n",
      "\n",
      "madre vas tu a chingar a\n",
      "likelihood:  -11.833676\n",
      "\n",
      "tu vas a chingar a madre\n",
      "likelihood:  -11.838669\n",
      "\n",
      "vas tu a chingar a madre\n",
      "likelihood:  -11.856934\n",
      "\n",
      "chingar tu a madre vas a\n",
      "likelihood:  -11.993791\n",
      "\n",
      "chingar a vas tu madre a\n",
      "likelihood:  -12.057165\n",
      "\n",
      "chingar madre vas a a tu\n",
      "likelihood:  -12.165815\n",
      "\n",
      "vas chingar tu madre a a\n",
      "likelihood:  -12.198259\n",
      "\n",
      "a tu vas a chingar madre\n",
      "likelihood:  -12.625823\n",
      "\n",
      "a a chingar vas tu madre\n",
      "likelihood:  -12.709546\n",
      "\n",
      "tu madre vas a a chingar\n",
      "likelihood:  -12.741095\n",
      "\n",
      "a chingar vas madre a tu\n",
      "likelihood:  -12.774832\n",
      "\n",
      "a chingar tu madre a vas\n",
      "likelihood:  -12.785389\n",
      "\n",
      "tu chingar vas a madre a\n",
      "likelihood:  -12.886546\n",
      "\n",
      "chingar a vas madre a tu\n",
      "likelihood:  -12.899754\n",
      "\n",
      "madre chingar vas a a tu\n",
      "likelihood:  -12.930603\n",
      "\n",
      "a tu madre vas a chingar\n",
      "likelihood:  -12.934748\n",
      "\n",
      "vas a madre chingar a tu\n",
      "likelihood:  -12.949212\n",
      "\n",
      "chingar vas tu madre a a\n",
      "likelihood:  -12.960356\n",
      "\n",
      "madre vas chingar a a tu\n",
      "likelihood:  -12.9946575\n",
      "\n",
      "a a vas chingar tu madre\n",
      "likelihood:  -13.038003\n",
      "\n",
      "chingar a a tu madre vas\n",
      "likelihood:  -13.054129\n",
      "\n",
      "chingar vas a tu madre a\n",
      "likelihood:  -13.103525\n",
      "\n",
      "tu madre a chingar a vas\n",
      "likelihood:  -13.162097\n",
      "\n",
      "madre tu vas a a chingar\n",
      "likelihood:  -13.210866\n",
      "\n",
      "a a chingar tu madre vas\n",
      "likelihood:  -13.426107\n",
      "\n",
      "chingar tu madre a vas a\n",
      "likelihood:  -13.504848\n",
      "\n",
      "chingar vas a a tu madre\n",
      "likelihood:  -13.523647\n",
      "\n",
      "chingar madre vas a tu a\n",
      "likelihood:  -13.536311\n",
      "\n",
      "tu a madre vas a chingar\n",
      "likelihood:  -13.603271\n",
      "\n",
      "tu chingar a madre vas a\n",
      "likelihood:  -13.647656\n",
      "\n",
      "vas chingar a a tu madre\n",
      "likelihood:  -13.65996\n",
      "\n",
      "a tu chingar vas a madre\n",
      "likelihood:  -13.673817\n",
      "\n",
      "madre chingar vas a tu a\n",
      "likelihood:  -13.685355\n",
      "\n",
      "vas madre chingar a a tu\n",
      "likelihood:  -13.822964\n",
      "\n",
      "chingar tu vas madre a a\n",
      "likelihood:  -13.914587\n",
      "\n",
      "tu chingar madre a vas a\n",
      "likelihood:  -13.953055\n",
      "\n",
      "tu a chingar a madre vas\n",
      "likelihood:  -13.954996\n",
      "\n",
      "vas a chingar a madre tu\n",
      "likelihood:  -13.996675\n",
      "\n",
      "a vas chingar a madre tu\n",
      "likelihood:  -14.019035\n",
      "\n",
      "chingar a a vas tu madre\n",
      "likelihood:  -14.053483\n",
      "\n",
      "tu vas chingar a a madre\n",
      "likelihood:  -14.073879\n",
      "\n",
      "chingar vas madre a tu a\n",
      "likelihood:  -14.127687\n",
      "\n",
      "tu madre a vas a chingar\n",
      "likelihood:  -14.198506\n",
      "\n",
      "a vas madre a chingar tu\n",
      "likelihood:  -14.216595\n",
      "\n",
      "madre vas chingar a tu a\n",
      "likelihood:  -14.277628\n",
      "\n",
      "madre tu a vas a chingar\n",
      "likelihood:  -14.335317\n",
      "\n",
      "a tu vas madre a chingar\n",
      "likelihood:  -14.381735\n",
      "\n",
      "chingar a tu madre a vas\n",
      "likelihood:  -14.487398\n",
      "\n",
      "tu a chingar madre vas a\n",
      "likelihood:  -14.530333\n",
      "\n",
      "a chingar madre vas a tu\n",
      "likelihood:  -14.531504\n",
      "\n",
      "tu chingar madre vas a a\n",
      "likelihood:  -14.583139\n",
      "\n",
      "madre a chingar a tu vas\n",
      "likelihood:  -14.649239\n",
      "\n",
      "chingar a madre vas a tu\n",
      "likelihood:  -14.714661\n",
      "\n",
      "a madre tu vas a chingar\n",
      "likelihood:  -14.740127\n",
      "\n",
      "tu a chingar a vas madre\n",
      "likelihood:  -14.785224\n",
      "\n",
      "chingar madre a tu vas a\n",
      "likelihood:  -14.945162\n",
      "\n",
      "vas tu chingar a madre a\n",
      "likelihood:  -14.966887\n",
      "\n",
      "vas madre tu a chingar a\n",
      "likelihood:  -15.055538\n",
      "\n",
      "chingar tu madre vas a a\n",
      "likelihood:  -15.07761\n",
      "\n",
      "a chingar vas a madre tu\n",
      "likelihood:  -15.146157\n",
      "\n",
      "tu vas madre a a chingar\n",
      "likelihood:  -15.185913\n",
      "\n",
      "tu madre chingar a a vas\n",
      "likelihood:  -15.225409\n",
      "\n",
      "vas chingar a madre a tu\n",
      "likelihood:  -15.282843\n",
      "\n",
      "madre a chingar a vas tu\n",
      "likelihood:  -15.283082\n",
      "\n",
      "a tu chingar a vas madre\n",
      "likelihood:  -15.361239\n",
      "\n",
      "madre tu chingar a a vas\n",
      "likelihood:  -15.446615\n",
      "\n",
      "chingar madre tu vas a a\n",
      "likelihood:  -15.452356\n",
      "\n",
      "vas tu a madre chingar a\n",
      "likelihood:  -15.485003\n",
      "\n",
      "a chingar a vas tu madre\n",
      "likelihood:  -15.493352\n",
      "\n",
      "vas a a tu madre chingar\n",
      "likelihood:  -15.552862\n",
      "\n",
      "chingar madre tu a vas a\n",
      "likelihood:  -15.578053\n",
      "\n",
      "a a vas tu madre chingar\n",
      "likelihood:  -15.635621\n",
      "\n",
      "tu chingar vas a a madre\n",
      "likelihood:  -15.692476\n",
      "\n",
      "a chingar madre tu vas a\n",
      "likelihood:  -15.783613\n",
      "\n",
      "chingar tu vas a a madre\n",
      "likelihood:  -15.836306\n",
      "\n",
      "madre vas tu a a chingar\n",
      "likelihood:  -15.929836\n",
      "\n",
      "madre a vas chingar a tu\n",
      "likelihood:  -15.935328\n",
      "\n",
      "a tu vas chingar a madre\n",
      "likelihood:  -15.944546\n",
      "\n",
      "chingar vas madre a a tu\n",
      "likelihood:  -15.9851265\n",
      "\n",
      "a madre chingar a tu vas\n",
      "likelihood:  -16.024593\n",
      "\n",
      "tu chingar a vas madre a\n",
      "likelihood:  -16.058475\n",
      "\n",
      "vas madre chingar a tu a\n",
      "likelihood:  -16.116787\n",
      "\n",
      "a vas chingar madre a tu\n",
      "likelihood:  -16.129957\n",
      "\n",
      "a vas madre chingar a tu\n",
      "likelihood:  -16.149418\n",
      "\n",
      "a tu chingar madre a vas\n",
      "likelihood:  -16.246616\n",
      "\n",
      "madre a tu vas a chingar\n",
      "likelihood:  -16.270468\n",
      "\n",
      "vas tu a madre a chingar\n",
      "likelihood:  -16.36051\n",
      "\n",
      "a madre chingar a vas tu\n",
      "likelihood:  -16.403053\n",
      "\n",
      "vas tu chingar madre a a\n",
      "likelihood:  -16.45551\n",
      "\n",
      "vas a madre a chingar tu\n",
      "likelihood:  -16.49997\n",
      "\n",
      "madre tu a chingar vas a\n",
      "likelihood:  -16.638597\n",
      "\n",
      "vas a chingar madre a tu\n",
      "likelihood:  -16.668663\n",
      "\n",
      "madre chingar a tu vas a\n",
      "likelihood:  -16.669052\n",
      "\n",
      "tu madre a chingar vas a\n",
      "likelihood:  -16.748552\n",
      "\n",
      "vas tu madre chingar a a\n",
      "likelihood:  -16.807547\n",
      "\n",
      "madre tu a chingar a vas\n",
      "likelihood:  -16.809229\n",
      "\n",
      "chingar a madre tu vas a\n",
      "likelihood:  -16.89813\n",
      "\n",
      "vas tu chingar a a madre\n",
      "likelihood:  -16.946959\n",
      "\n",
      "vas madre a a chingar tu\n",
      "likelihood:  -17.046623\n",
      "\n",
      "chingar tu vas a madre a\n",
      "likelihood:  -17.067505\n",
      "\n",
      "tu a chingar vas a madre\n",
      "likelihood:  -17.072899\n",
      "\n",
      "tu a vas chingar a madre\n",
      "likelihood:  -17.114412\n",
      "\n",
      "a vas a chingar madre tu\n",
      "likelihood:  -17.147932\n",
      "\n",
      "madre a a chingar vas tu\n",
      "likelihood:  -17.202206\n",
      "\n",
      "vas chingar madre a tu a\n",
      "likelihood:  -17.276424\n",
      "\n",
      "chingar vas a madre a tu\n",
      "likelihood:  -17.2792\n",
      "\n",
      "a madre vas a tu chingar\n",
      "likelihood:  -17.288654\n",
      "\n",
      "a tu chingar a madre vas\n",
      "likelihood:  -17.381275\n",
      "\n",
      "a madre chingar vas a tu\n",
      "likelihood:  -17.415133\n",
      "\n",
      "vas tu madre a a chingar\n",
      "likelihood:  -17.453014\n",
      "\n",
      "chingar a vas tu a madre\n",
      "likelihood:  -17.464735\n",
      "\n",
      "chingar madre a vas a tu\n",
      "likelihood:  -17.503288\n",
      "\n",
      "tu a chingar vas madre a\n",
      "likelihood:  -17.517582\n",
      "\n",
      "tu a vas madre a chingar\n",
      "likelihood:  -17.573154\n",
      "\n",
      "a chingar vas tu a madre\n",
      "likelihood:  -17.616304\n",
      "\n",
      "a vas chingar madre tu a\n",
      "likelihood:  -17.6196\n",
      "\n",
      "a vas tu a chingar madre\n",
      "likelihood:  -17.665834\n",
      "\n",
      "madre tu chingar vas a a\n",
      "likelihood:  -17.786777\n",
      "\n",
      "tu vas a chingar madre a\n",
      "likelihood:  -17.993967\n",
      "\n",
      "tu vas a a chingar madre\n",
      "likelihood:  -18.09928\n",
      "\n",
      "vas madre a tu a chingar\n",
      "likelihood:  -18.135757\n",
      "\n",
      "madre vas a a chingar tu\n",
      "likelihood:  -18.203009\n",
      "\n",
      "a madre vas chingar a tu\n",
      "likelihood:  -18.230425\n",
      "\n",
      "tu vas a madre chingar a\n",
      "likelihood:  -18.348475\n",
      "\n",
      "tu a vas a madre chingar\n",
      "likelihood:  -18.384226\n",
      "\n",
      "vas a chingar madre tu a\n",
      "likelihood:  -18.390669\n",
      "\n",
      "vas tu a a chingar madre\n",
      "likelihood:  -18.427216\n",
      "\n",
      "vas a a chingar madre tu\n",
      "likelihood:  -18.430536\n",
      "\n",
      "chingar a vas a madre tu\n",
      "likelihood:  -18.435055\n",
      "\n",
      "madre a vas tu a chingar\n",
      "likelihood:  -18.44017\n",
      "\n",
      "a tu chingar vas madre a\n",
      "likelihood:  -18.483269\n",
      "\n",
      "tu chingar vas madre a a\n",
      "likelihood:  -18.56616\n",
      "\n",
      "chingar vas a tu a madre\n",
      "likelihood:  -18.616274\n",
      "\n",
      "vas a tu chingar madre a\n",
      "likelihood:  -18.637594\n",
      "\n",
      "chingar tu a vas a madre\n",
      "likelihood:  -18.64048\n",
      "\n",
      "tu a madre chingar a vas\n",
      "likelihood:  -18.657368\n",
      "\n",
      "a vas a tu madre chingar\n",
      "likelihood:  -18.761827\n",
      "\n",
      "tu chingar a vas a madre\n",
      "likelihood:  -18.777767\n",
      "\n",
      "a vas tu chingar madre a\n",
      "likelihood:  -18.957243\n",
      "\n",
      "madre a chingar vas a tu\n",
      "likelihood:  -18.968842\n",
      "\n",
      "vas chingar a madre tu a\n",
      "likelihood:  -18.985193\n",
      "\n",
      "a tu vas a madre chingar\n",
      "likelihood:  -18.992933\n",
      "\n",
      "madre chingar a vas a tu\n",
      "likelihood:  -19.03957\n",
      "\n",
      "chingar vas tu a madre a\n",
      "likelihood:  -19.111439\n",
      "\n",
      "tu madre chingar vas a a\n",
      "likelihood:  -19.1815\n",
      "\n",
      "chingar a madre a tu vas\n",
      "likelihood:  -19.238998\n",
      "\n",
      "vas chingar a tu a madre\n",
      "likelihood:  -19.280804\n",
      "\n",
      "vas a madre tu a chingar\n",
      "likelihood:  -19.28261\n",
      "\n",
      "a chingar a madre tu vas\n",
      "likelihood:  -19.312424\n",
      "\n",
      "madre chingar tu a vas a\n",
      "likelihood:  -19.39608\n",
      "\n",
      "chingar madre a tu a vas\n",
      "likelihood:  -19.413187\n",
      "\n",
      "chingar tu a madre a vas\n",
      "likelihood:  -19.432705\n",
      "\n",
      "vas tu a chingar madre a\n",
      "likelihood:  -19.444393\n",
      "\n",
      "chingar a vas madre tu a\n",
      "likelihood:  -19.500069\n",
      "\n",
      "vas a a tu chingar madre\n",
      "likelihood:  -19.528164\n",
      "\n",
      "a vas chingar tu a madre\n",
      "likelihood:  -19.530113\n",
      "\n",
      "vas chingar madre tu a a\n",
      "likelihood:  -19.5458\n",
      "\n",
      "tu a madre chingar vas a\n",
      "likelihood:  -19.570145\n",
      "\n",
      "a tu madre chingar a vas\n",
      "likelihood:  -19.586496\n",
      "\n",
      "tu a a chingar vas madre\n",
      "likelihood:  -19.614897\n",
      "\n",
      "vas madre tu chingar a a\n",
      "likelihood:  -19.63364\n",
      "\n",
      "chingar vas a madre tu a\n",
      "likelihood:  -19.71691\n",
      "\n",
      "a vas tu a madre chingar\n",
      "likelihood:  -19.71905\n",
      "\n",
      "a madre chingar tu vas a\n",
      "likelihood:  -19.798693\n",
      "\n",
      "a madre vas tu a chingar\n",
      "likelihood:  -19.818413\n",
      "\n",
      "chingar a a tu vas madre\n",
      "likelihood:  -19.8587\n",
      "\n",
      "tu a a chingar madre vas\n",
      "likelihood:  -19.87529\n",
      "\n",
      "madre a vas a tu chingar\n",
      "likelihood:  -19.894264\n",
      "\n",
      "madre a tu vas chingar a\n",
      "likelihood:  -19.900702\n",
      "\n",
      "a madre a chingar vas tu\n",
      "likelihood:  -19.91764\n",
      "\n",
      "tu vas a madre a chingar\n",
      "likelihood:  -19.946295\n",
      "\n",
      "vas a tu chingar a madre\n",
      "likelihood:  -19.955269\n",
      "\n",
      "a a vas chingar madre tu\n",
      "likelihood:  -20.039492\n",
      "\n",
      "vas madre tu a a chingar\n",
      "likelihood:  -20.220222\n",
      "\n",
      "a chingar vas madre tu a\n",
      "likelihood:  -20.300694\n",
      "\n",
      "a chingar madre a tu vas\n",
      "likelihood:  -20.313305\n",
      "\n",
      "chingar madre vas tu a a\n",
      "likelihood:  -20.331394\n",
      "\n",
      "a chingar tu vas madre a\n",
      "likelihood:  -20.40751\n",
      "\n",
      "vas a tu a chingar madre\n",
      "likelihood:  -20.422369\n",
      "\n",
      "madre vas a tu a chingar\n",
      "likelihood:  -20.42354\n",
      "\n",
      "vas a madre tu chingar a\n",
      "likelihood:  -20.523714\n",
      "\n",
      "a tu vas madre chingar a\n",
      "likelihood:  -20.595345\n",
      "\n",
      "madre chingar a vas tu a\n",
      "likelihood:  -20.6371\n",
      "\n",
      "tu madre a a chingar vas\n",
      "likelihood:  -20.662771\n",
      "\n",
      "a a chingar madre tu vas\n",
      "likelihood:  -20.672026\n",
      "\n",
      "madre chingar vas tu a a\n",
      "likelihood:  -20.680208\n",
      "\n",
      "a a vas tu chingar madre\n",
      "likelihood:  -20.718252\n",
      "\n",
      "madre chingar a a tu vas\n",
      "likelihood:  -20.72098\n",
      "\n",
      "chingar madre a vas tu a\n",
      "likelihood:  -20.721249\n",
      "\n",
      "a vas madre tu chingar a\n",
      "likelihood:  -20.768871\n",
      "\n",
      "vas a tu a madre chingar\n",
      "likelihood:  -20.771284\n",
      "\n",
      "a vas tu chingar a madre\n",
      "likelihood:  -20.803955\n",
      "\n",
      "tu chingar a madre a vas\n",
      "likelihood:  -20.824207\n",
      "\n",
      "a madre tu chingar a vas\n",
      "likelihood:  -20.866426\n",
      "\n",
      "vas a chingar tu a madre\n",
      "likelihood:  -20.90037\n",
      "\n",
      "madre a chingar tu vas a\n",
      "likelihood:  -20.984268\n",
      "\n",
      "vas chingar madre a a tu\n",
      "likelihood:  -21.001698\n",
      "\n",
      "madre tu a vas chingar a\n",
      "likelihood:  -21.008902\n",
      "\n",
      "madre vas a tu chingar a\n",
      "likelihood:  -21.022272\n",
      "\n",
      "vas chingar tu a madre a\n",
      "likelihood:  -21.081226\n",
      "\n",
      "chingar madre tu a a vas\n",
      "likelihood:  -21.08187\n",
      "\n",
      "a madre tu vas chingar a\n",
      "likelihood:  -21.096272\n",
      "\n",
      "a chingar a tu vas madre\n",
      "likelihood:  -21.123962\n",
      "\n",
      "tu madre a vas chingar a\n",
      "likelihood:  -21.135555\n",
      "\n",
      "madre chingar a a vas tu\n",
      "likelihood:  -21.180216\n",
      "\n",
      "a madre tu a chingar vas\n",
      "likelihood:  -21.20935\n",
      "\n",
      "a tu madre chingar vas a\n",
      "likelihood:  -21.210714\n",
      "\n",
      "a chingar a vas madre tu\n",
      "likelihood:  -21.253761\n",
      "\n",
      "madre chingar tu vas a a\n",
      "likelihood:  -21.259392\n",
      "\n",
      "a tu madre a chingar vas\n",
      "likelihood:  -21.331833\n",
      "\n",
      "chingar tu madre a a vas\n",
      "likelihood:  -21.376827\n",
      "\n",
      "madre tu vas chingar a a\n",
      "likelihood:  -21.38734\n",
      "\n",
      "tu a madre a chingar vas\n",
      "likelihood:  -21.412775\n",
      "\n",
      "a tu a chingar madre vas\n",
      "likelihood:  -21.41756\n",
      "\n",
      "tu vas madre chingar a a\n",
      "likelihood:  -21.457626\n",
      "\n",
      "tu a vas madre chingar a\n",
      "likelihood:  -21.458914\n",
      "\n",
      "a vas madre tu a chingar\n",
      "likelihood:  -21.524075\n",
      "\n",
      "a tu a chingar vas madre\n",
      "likelihood:  -21.525652\n",
      "\n",
      "vas a a madre chingar tu\n",
      "likelihood:  -21.547321\n",
      "\n",
      "chingar vas tu a a madre\n",
      "likelihood:  -21.55745\n",
      "\n",
      "tu a a vas chingar madre\n",
      "likelihood:  -21.575562\n",
      "\n",
      "tu a chingar madre a vas\n",
      "likelihood:  -21.725471\n",
      "\n",
      "madre tu a a chingar vas\n",
      "likelihood:  -21.747482\n",
      "\n",
      "chingar a tu vas madre a\n",
      "likelihood:  -21.757164\n",
      "\n",
      "a chingar madre vas tu a\n",
      "likelihood:  -21.779469\n",
      "\n",
      "a a tu madre chingar vas\n",
      "likelihood:  -21.803759\n",
      "\n",
      "a a chingar vas madre tu\n",
      "likelihood:  -21.827953\n",
      "\n",
      "chingar a a madre tu vas\n",
      "likelihood:  -21.83685\n",
      "\n",
      "madre a tu chingar vas a\n",
      "likelihood:  -21.848541\n",
      "\n",
      "tu chingar a a madre vas\n",
      "likelihood:  -21.90497\n",
      "\n",
      "vas madre chingar tu a a\n",
      "likelihood:  -21.911991\n",
      "\n",
      "chingar a madre vas tu a\n",
      "likelihood:  -21.916473\n",
      "\n",
      "a chingar a madre vas tu\n",
      "likelihood:  -21.939732\n",
      "\n",
      "a a tu chingar madre vas\n",
      "likelihood:  -21.945488\n",
      "\n",
      "a chingar madre tu a vas\n",
      "likelihood:  -21.973648\n",
      "\n",
      "chingar a tu vas a madre\n",
      "likelihood:  -22.068314\n",
      "\n",
      "vas chingar a a madre tu\n",
      "likelihood:  -22.127918\n",
      "\n",
      "chingar tu a vas madre a\n",
      "likelihood:  -22.129457\n",
      "\n",
      "a vas a tu chingar madre\n",
      "likelihood:  -22.187828\n",
      "\n",
      "madre a a vas chingar tu\n",
      "likelihood:  -22.216572\n",
      "\n",
      "a a chingar tu vas madre\n",
      "likelihood:  -22.26822\n",
      "\n",
      "a madre tu chingar vas a\n",
      "likelihood:  -22.332233\n",
      "\n",
      "chingar vas madre tu a a\n",
      "likelihood:  -22.433279\n",
      "\n",
      "a chingar madre a vas tu\n",
      "likelihood:  -22.499805\n",
      "\n",
      "madre vas a chingar tu a\n",
      "likelihood:  -22.50985\n",
      "\n",
      "madre a vas tu chingar a\n",
      "likelihood:  -22.644123\n",
      "\n",
      "chingar a madre a vas tu\n",
      "likelihood:  -22.664375\n",
      "\n",
      "madre a chingar vas tu a\n",
      "likelihood:  -22.66857\n",
      "\n",
      "tu chingar a a vas madre\n",
      "likelihood:  -22.674337\n",
      "\n",
      "tu a vas chingar madre a\n",
      "likelihood:  -22.708645\n",
      "\n",
      "vas madre a tu chingar a\n",
      "likelihood:  -22.835548\n",
      "\n",
      "madre a tu a chingar vas\n",
      "likelihood:  -22.878506\n",
      "\n",
      "chingar a madre tu a vas\n",
      "likelihood:  -22.879683\n",
      "\n",
      "madre chingar a tu a vas\n",
      "likelihood:  -22.899569\n",
      "\n",
      "a vas a madre chingar tu\n",
      "likelihood:  -22.987865\n",
      "\n",
      "chingar tu a a vas madre\n",
      "likelihood:  -22.997057\n",
      "\n",
      "tu vas a a madre chingar\n",
      "likelihood:  -23.008022\n",
      "\n",
      "tu chingar madre a a vas\n",
      "likelihood:  -23.066235\n",
      "\n",
      "a a vas madre chingar tu\n",
      "likelihood:  -23.094858\n",
      "\n",
      "vas tu a a madre chingar\n",
      "likelihood:  -23.099445\n",
      "\n",
      "tu a madre vas chingar a\n",
      "likelihood:  -23.113976\n",
      "\n",
      "chingar madre a a tu vas\n",
      "likelihood:  -23.25369\n",
      "\n",
      "madre tu a a vas chingar\n",
      "likelihood:  -23.50176\n",
      "\n",
      "a vas madre a tu chingar\n",
      "likelihood:  -23.532785\n",
      "\n",
      "madre a a chingar tu vas\n",
      "likelihood:  -23.539547\n",
      "\n",
      "vas madre a chingar tu a\n",
      "likelihood:  -23.612648\n",
      "\n",
      "madre vas a a tu chingar\n",
      "likelihood:  -23.62856\n",
      "\n",
      "vas a madre chingar tu a\n",
      "likelihood:  -23.743465\n",
      "\n",
      "chingar a a vas madre tu\n",
      "likelihood:  -23.809568\n",
      "\n",
      "a madre a chingar tu vas\n",
      "likelihood:  -23.863056\n",
      "\n",
      "chingar vas a a madre tu\n",
      "likelihood:  -23.998783\n",
      "\n",
      "a madre chingar tu a vas\n",
      "likelihood:  -23.999722\n",
      "\n",
      "a madre a tu vas chingar\n",
      "likelihood:  -24.041084\n",
      "\n",
      "chingar tu a a madre vas\n",
      "likelihood:  -24.103565\n",
      "\n",
      "chingar a tu a madre vas\n",
      "likelihood:  -24.164352\n",
      "\n",
      "a a chingar madre vas tu\n",
      "likelihood:  -24.276848\n",
      "\n",
      "a chingar tu vas a madre\n",
      "likelihood:  -24.430593\n",
      "\n",
      "madre a a tu chingar vas\n",
      "likelihood:  -24.48582\n",
      "\n",
      "a tu a madre chingar vas\n",
      "likelihood:  -24.501062\n",
      "\n",
      "chingar a a madre vas tu\n",
      "likelihood:  -24.5564\n",
      "\n",
      "vas madre a a tu chingar\n",
      "likelihood:  -24.568714\n",
      "\n",
      "madre a a tu vas chingar\n",
      "likelihood:  -24.720444\n",
      "\n",
      "tu vas chingar madre a a\n",
      "likelihood:  -24.754044\n",
      "\n",
      "chingar madre a a vas tu\n",
      "likelihood:  -24.827244\n",
      "\n",
      "a tu a vas chingar madre\n",
      "likelihood:  -24.845406\n",
      "\n",
      "a madre chingar vas tu a\n",
      "likelihood:  -24.86287\n",
      "\n",
      "madre vas chingar tu a a\n",
      "likelihood:  -25.164886\n",
      "\n",
      "madre chingar tu a a vas\n",
      "likelihood:  -25.24347\n",
      "\n",
      "a vas madre chingar tu a\n",
      "likelihood:  -25.34127\n",
      "\n",
      "vas a madre a tu chingar\n",
      "likelihood:  -25.453926\n",
      "\n",
      "a madre a vas chingar tu\n",
      "likelihood:  -25.699142\n",
      "\n",
      "tu madre a a vas chingar\n",
      "likelihood:  -25.733116\n",
      "\n",
      "a chingar tu a madre vas\n",
      "likelihood:  -25.820698\n",
      "\n",
      "a madre tu a vas chingar\n",
      "likelihood:  -25.97506\n",
      "\n",
      "madre a tu a vas chingar\n",
      "likelihood:  -25.991873\n",
      "\n",
      "madre a tu chingar a vas\n",
      "likelihood:  -25.99427\n",
      "\n",
      "a tu madre a vas chingar\n",
      "likelihood:  -26.11424\n",
      "\n",
      "a tu vas chingar madre a\n",
      "likelihood:  -26.220797\n",
      "\n",
      "a madre a tu chingar vas\n",
      "likelihood:  -26.241196\n",
      "\n",
      "a a madre chingar vas tu\n",
      "likelihood:  -26.404863\n",
      "\n",
      "tu a a vas madre chingar\n",
      "likelihood:  -26.426338\n",
      "\n",
      "vas chingar tu a a madre\n",
      "likelihood:  -26.651392\n",
      "\n",
      "a a madre chingar tu vas\n",
      "likelihood:  -26.97041\n",
      "\n",
      "a a tu chingar vas madre\n",
      "likelihood:  -27.066853\n",
      "\n",
      "tu a madre a vas chingar\n",
      "likelihood:  -27.250254\n",
      "\n",
      "tu a a madre chingar vas\n",
      "likelihood:  -27.282106\n",
      "\n",
      "madre vas tu chingar a a\n",
      "likelihood:  -27.28361\n",
      "\n",
      "a tu madre vas chingar a\n",
      "likelihood:  -27.289202\n",
      "\n",
      "madre a a vas tu chingar\n",
      "likelihood:  -27.32898\n",
      "\n",
      "madre a vas chingar tu a\n",
      "likelihood:  -27.347664\n",
      "\n",
      "a a tu vas madre chingar\n",
      "likelihood:  -27.49336\n",
      "\n",
      "tu madre vas chingar a a\n",
      "likelihood:  -27.895035\n",
      "\n",
      "a a tu madre vas chingar\n",
      "likelihood:  -27.954504\n",
      "\n",
      "a a vas madre tu chingar\n",
      "likelihood:  -27.996593\n",
      "\n",
      "a tu a madre vas chingar\n",
      "likelihood:  -28.117153\n",
      "\n",
      "madre a chingar tu a vas\n",
      "likelihood:  -28.168951\n",
      "\n",
      "chingar a tu a vas madre\n",
      "likelihood:  -28.209064\n",
      "\n",
      "a tu a vas madre chingar\n",
      "likelihood:  -28.241697\n",
      "\n",
      "a a tu vas chingar madre\n",
      "likelihood:  -28.47279\n",
      "\n",
      "a madre a vas tu chingar\n",
      "likelihood:  -28.501442\n",
      "\n",
      "vas a a madre tu chingar\n",
      "likelihood:  -28.544237\n",
      "\n",
      "a a madre tu vas chingar\n",
      "likelihood:  -29.150307\n",
      "\n",
      "a vas a madre tu chingar\n",
      "likelihood:  -29.24225\n",
      "\n",
      "a madre vas tu chingar a\n",
      "likelihood:  -29.93504\n",
      "\n",
      "a a madre vas chingar tu\n",
      "likelihood:  -29.966505\n",
      "\n",
      "a a madre tu chingar vas\n",
      "likelihood:  -30.638538\n",
      "\n",
      "tu a a madre vas chingar\n",
      "likelihood:  -31.45301\n",
      "\n",
      "a chingar tu a vas madre\n",
      "likelihood:  -31.52551\n",
      "\n",
      "a madre vas chingar tu a\n",
      "likelihood:  -32.51433\n",
      "\n",
      "a a madre vas tu chingar\n",
      "likelihood:  -34.89305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_structures('vas a chingar a tu madre'.split(), NGramModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_to(word, ngram_builder, embeddings, N):\n",
    "    # get word id\n",
    "    word_id = ngram_builder.get_ids([word])[0]\n",
    "    word_rep = embeddings[word_id]\n",
    "    # get norms to normalize later\n",
    "    embeddings_norm = np.linalg.norm(embeddings, axis=1)\n",
    "    word_norm = embeddings_norm[word_id]\n",
    "    # sim distance\n",
    "    distances = np.dot(word_rep, embeddings.T)\n",
    "    # normalize distances (cos distance)\n",
    "    distances = np.squeeze(distances/(word_norm * embeddings_norm))\n",
    "    \n",
    "    # most similar word is surely the word itself, so ignore the most similar\n",
    "    return np.argsort(distances)[-(N+1):-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mchinga\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mentarte',\n",
       " 'reputisima',\n",
       " 'concha',\n",
       " 'chingue',\n",
       " 'put',\n",
       " 'reputa',\n",
       " 'putisima',\n",
       " 'chingar',\n",
       " 'chiga',\n",
       " 'chingas']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'chinga'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mamor\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['llanto',\n",
       " 'hombre',\n",
       " 'pensamiento',\n",
       " 'desprecio',\n",
       " 'orgullo',\n",
       " 'sufrimiento',\n",
       " 'alma',\n",
       " 'corazon',\n",
       " 'corazón',\n",
       " 'cariño']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'amor'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar to:  \u001b[1mverga\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['berga',\n",
       " 'caca',\n",
       " 'fregada',\n",
       " 'vergaaaaaa',\n",
       " 'ñonga',\n",
       " 'mierda',\n",
       " 'chingada',\n",
       " 'pija',\n",
       " 'verg',\n",
       " 'vrg']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'verga'\n",
    "indexes = most_similar_to(word, ngram_builder, model.embeddings.weight.detach().cpu().numpy(), 10)\n",
    "print('Most Similar to: ', bold_string(word))\n",
    "ngram_builder.inverse(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cos Distance Among all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_distance(data):\n",
    "    N = len(data)\n",
    "    distances = np.zeros((N, N))\n",
    "    magnitudes = np.linalg.norm(data, axis=1)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(i+1):\n",
    "            distances[i, j] = np.dot(data[i], data[j])/(magnitudes[i] * magnitudes[j])\n",
    "            if i != j:\n",
    "                distances[j, i] = distances[i, j]\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(dist_matrix, n):\n",
    "    N = len(dist_matrix)\n",
    "    \n",
    "    # get indexes of elements to be compared. dist_matrix should be symmetric, so we dont need to consider each pair of distances twice\n",
    "    indexes = [(i,j) for i in range(N) for j in range(i+1) if i!=j]\n",
    "\n",
    "    # get x and y indexes\n",
    "    x_indexes = tuple([ind[0] for ind in indexes])\n",
    "    y_indexes = tuple([ind[1] for ind in indexes])\n",
    "    \n",
    "    # get values of matrix\n",
    "    row_max = dist_matrix[x_indexes, y_indexes]\n",
    "    \n",
    "    # desc sort elements retrieved and get their positions\n",
    "    max_elements = np.flip(np.argsort(row_max))[:n]\n",
    "    \n",
    "    # return indexes in positions retrieved in previous step\n",
    "    return [indexes[max_index] for max_index in max_elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Most Similar among all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = cos_distance(model.embeddings.weight.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['goooooool', 'gooooool'],\n",
       " ['jajajajajajajajajaja', 'jajajajajajajajaja'],\n",
       " ['goooool', 'gooooool'],\n",
       " ['jajajajajajajajaja', 'jajajajajajajaja'],\n",
       " ['goooool', 'gooool'],\n",
       " ['goooooool', 'goooool'],\n",
       " ['jajajajajajajaja', 'jajajajajajaja'],\n",
       " ['jajajajajajajajajajaja', 'jajajajajajajajajaja'],\n",
       " ['jajajajajajaja', 'jajajajajaja'],\n",
       " ['yaaaaaa', 'yaaaaa']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar = get_most_similar(dist_matrix, 10)\n",
    "similar = [list(pair) for pair in similar]\n",
    "ngram_builder.inverse(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258.8639973177619"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramModel.perplexity(val_documents, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioModel(nn.Module):\n",
    "    def __init__(self, N, voc_size, d_model, hidden_size=128, emb_mat=None, dropout=0.1):\n",
    "        \n",
    "        super(BengioModel, self).__init__()\n",
    "        # parameters\n",
    "        self.N           = N\n",
    "        self.d_model     = d_model\n",
    "        self.voc_size    = voc_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Matriz entrenable de embeddings, tamaño vocab_size x Ngram.d_model\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(emb_mat), freeze=False)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(d_model * (N-1), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, voc_size, bias=False)\n",
    "        self.W = nn.Linear(d_model * (N-1), voc_size, bias=False)\n",
    "        \n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        # Calcula el embedding para cada palabra.\n",
    "        x = self.embeddings(input_seq)\n",
    "        x = x.view(-1, (self.N-1) * self.d_model)\n",
    "        direct_link = self.W(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x + direct_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_builder = NGramBuilder(embeddings=embeddings)\n",
    "ngram_docs, ngram_labels = ngram_builder.fit(documents, N=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "voc_size = ngram_builder.voc_size\n",
    "N = ngram_builder.N\n",
    "d_model = ngram_builder.d_model\n",
    "\n",
    "# optimizer hyperparameters\n",
    "lr = 2.3e-1 \n",
    "epochs = 100\n",
    "patience = epochs//5\n",
    "\n",
    "# scheduler hyperparameters\n",
    "lr_patience = 10\n",
    "lr_factor = 0.5\n",
    "\n",
    "# gpu available?\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# build model and move to gpu if possible\n",
    "model = BengioModel(N=N, voc_size=voc_size, d_model=d_model, hidden_size=200, emb_mat=ngram_builder.emb_matrix)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                'min',\n",
    "                patience = lr_patience,\n",
    "                verbose=True,\n",
    "                factor = lr_factor\n",
    "            )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6417,  1.4477, -2.2832, -1.9652, -0.2229,  5.1052, -0.1207, -0.1268,\n",
       "        -3.1773, -3.4544, -0.9431, -0.0945, -1.1894, -0.8121, -2.5730, -0.6139,\n",
       "        -2.3118,  1.0510,  5.6347, -5.8270,  1.2376,  1.0716,  3.8221,  2.3954,\n",
       "         0.1699,  3.2568,  2.8973,  3.2748, -2.9364,  0.2720, -1.0295, -2.6173,\n",
       "        -1.8071,  1.7376,  0.3391,  3.9329,  1.5714, -4.1001,  4.1568,  1.1624,\n",
       "        -0.5523, -0.5859, -4.7672,  0.2533, -1.1242, -0.1151, -5.6066,  2.9766,\n",
       "         4.4260,  1.0199,  3.7607, -2.2983,  4.4166, -1.3840, -1.8625,  0.3991,\n",
       "        -1.0969, -2.2860,  2.9928,  0.0440,  3.7624, -6.5231,  0.6213,  2.6418,\n",
       "        -1.9243, -1.1412, -3.8318,  0.5496,  2.2608, -1.3184, -1.1347, -3.7882,\n",
       "        -0.7750,  3.9567, -3.5794, -4.4237,  4.5057,  0.7191, -1.3996,  3.0972,\n",
       "         0.1075,  2.8299, -0.7602, -0.2772, -1.2257,  2.1579,  0.5184,  3.4380,\n",
       "        -1.0875, -1.5299,  0.3995,  0.8874, -1.9949,  2.9122,  3.2580, -2.6000,\n",
       "        -1.9951, -0.1765,  3.9469, -2.7925], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.weight[ngram_builder.word2id['de']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "train accuracy mean:  0.07270352802579365\n",
      "validation accuracy:  0.06816901408450704\n",
      "mean loss:  9.02942226951321\n",
      "Storing best model to dirlink_best_model. Current acc: 0.06816901408450704, last best metric: 0\n",
      "epoch:  2\n",
      "train accuracy mean:  0.087799072265625\n",
      "validation accuracy:  0.06938967136150234\n",
      "mean loss:  7.408335320651531\n",
      "Storing best model to dirlink_best_model. Current acc: 0.06938967136150234, last best metric: 0.06816901408450704\n",
      "epoch:  3\n",
      "train accuracy mean:  0.10818626767113095\n",
      "validation accuracy:  0.07549295774647888\n",
      "mean loss:  6.580813377474745\n",
      "Storing best model to dirlink_best_model. Current acc: 0.07549295774647888, last best metric: 0.06938967136150234\n",
      "epoch:  4\n",
      "train accuracy mean:  0.13936505998883927\n",
      "validation accuracy:  0.07971830985915493\n",
      "mean loss:  6.044397950793306\n",
      "Storing best model to dirlink_best_model. Current acc: 0.07971830985915493, last best metric: 0.07549295774647888\n",
      "epoch:  5\n",
      "train accuracy mean:  0.16627139136904762\n",
      "validation accuracy:  0.06967136150234741\n",
      "mean loss:  5.645293759647757\n",
      "epoch:  6\n",
      "train accuracy mean:  0.19033958798363093\n",
      "validation accuracy:  0.08375586854460094\n",
      "mean loss:  5.344871313000719\n",
      "Storing best model to dirlink_best_model. Current acc: 0.08375586854460094, last best metric: 0.07971830985915493\n",
      "epoch:  7\n",
      "train accuracy mean:  0.21153041294642858\n",
      "validation accuracy:  0.09126760563380282\n",
      "mean loss:  5.098544028277199\n",
      "Storing best model to dirlink_best_model. Current acc: 0.09126760563380282, last best metric: 0.08375586854460094\n",
      "epoch:  8\n",
      "train accuracy mean:  0.2293425060453869\n",
      "validation accuracy:  0.09727699530516432\n",
      "mean loss:  4.9008163445008295\n",
      "Storing best model to dirlink_best_model. Current acc: 0.09727699530516432, last best metric: 0.09126760563380282\n",
      "epoch:  9\n",
      "train accuracy mean:  0.24463326590401788\n",
      "validation accuracy:  0.08666666666666667\n",
      "mean loss:  4.717689569418629\n",
      "epoch:  10\n",
      "train accuracy mean:  0.2572854662698413\n",
      "validation accuracy:  0.08187793427230047\n",
      "mean loss:  4.5991239080516\n",
      "epoch:  11\n",
      "train accuracy mean:  0.2691069103422619\n",
      "validation accuracy:  0.08929577464788732\n",
      "mean loss:  4.4752615806646645\n",
      "epoch:  12\n",
      "train accuracy mean:  0.28142680819072424\n",
      "validation accuracy:  0.11389671361502347\n",
      "mean loss:  4.341352065249036\n",
      "Storing best model to dirlink_best_model. Current acc: 0.11389671361502347, last best metric: 0.09727699530516432\n",
      "epoch:  13\n",
      "train accuracy mean:  0.29055107964409727\n",
      "validation accuracy:  0.09267605633802817\n",
      "mean loss:  4.240237220656127\n",
      "epoch:  14\n",
      "train accuracy mean:  0.30111016167534727\n",
      "validation accuracy:  0.08112676056338028\n",
      "mean loss:  4.150921692916502\n",
      "epoch:  15\n",
      "train accuracy mean:  0.31096540178571425\n",
      "validation accuracy:  0.07671361502347418\n",
      "mean loss:  4.054213038956125\n",
      "epoch:  16\n",
      "train accuracy mean:  0.31793600415426587\n",
      "validation accuracy:  0.10967136150234742\n",
      "mean loss:  3.975780322061231\n",
      "epoch:  17\n",
      "train accuracy mean:  0.32603042844742064\n",
      "validation accuracy:  0.0847887323943662\n",
      "mean loss:  3.9133373809357486\n",
      "epoch:  18\n",
      "train accuracy mean:  0.33297923254588296\n",
      "validation accuracy:  0.08854460093896714\n",
      "mean loss:  3.8404405442997813\n",
      "epoch:  19\n",
      "train accuracy mean:  0.3405994233630952\n",
      "validation accuracy:  0.09248826291079812\n",
      "mean loss:  3.775259988537679\n",
      "epoch:  20\n",
      "train accuracy mean:  0.3478204636346726\n",
      "validation accuracy:  0.09427230046948357\n",
      "mean loss:  3.703489786169181\n",
      "epoch:  21\n",
      "train accuracy mean:  0.3533460828993056\n",
      "validation accuracy:  0.09671361502347418\n",
      "mean loss:  3.6417799264503024\n",
      "epoch:  22\n",
      "train accuracy mean:  0.3578127906436012\n",
      "validation accuracy:  0.09211267605633802\n",
      "mean loss:  3.6107685232224562\n",
      "epoch:  23\n",
      "train accuracy mean:  0.36185031467013884\n",
      "validation accuracy:  0.10018779342723004\n",
      "mean loss:  3.5643208861971893\n",
      "epoch:  24\n",
      "train accuracy mean:  0.3694288465711806\n",
      "validation accuracy:  0.1092018779342723\n",
      "mean loss:  3.5129132277021804\n",
      "epoch:  25\n",
      "train accuracy mean:  0.3744947645399306\n",
      "validation accuracy:  0.0923943661971831\n",
      "mean loss:  3.478783971319596\n",
      "epoch:  26\n",
      "train accuracy mean:  0.37930636935763884\n",
      "validation accuracy:  0.10206572769953051\n",
      "mean loss:  3.4246619450859725\n",
      "epoch:  27\n",
      "train accuracy mean:  0.3836592416914682\n",
      "validation accuracy:  0.09051643192488262\n",
      "mean loss:  3.382600351857642\n",
      "epoch:  28\n",
      "train accuracy mean:  0.3880053323412698\n",
      "validation accuracy:  0.10150234741784038\n",
      "mean loss:  3.3468754569378993\n",
      "epoch:  29\n",
      "train accuracy mean:  0.3942188081287202\n",
      "validation accuracy:  0.09417840375586854\n",
      "mean loss:  3.3033882175416998\n",
      "epoch:  30\n",
      "train accuracy mean:  0.3966505262586805\n",
      "validation accuracy:  0.09276995305164319\n",
      "mean loss:  3.282473687082529\n",
      "epoch:  31\n",
      "train accuracy mean:  0.4032239157056052\n",
      "validation accuracy:  0.09389671361502347\n",
      "mean loss:  3.220044118973116\n",
      "epoch:  32\n",
      "train accuracy mean:  0.4064176044766865\n",
      "validation accuracy:  0.10084507042253521\n",
      "mean loss:  3.1956525166363767\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "last_metric = 0\n",
    "val_metrics = []\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', 1 + epoch)\n",
    "    epoch_metrics = []\n",
    "    epoch_losses = []\n",
    "    for inputs, targets in train_loader:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        # feed model and get loss\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        # metric with train dataset\n",
    "        preds = get_preds(output)\n",
    "        epoch_metrics.append(accuracy(preds, targets.cpu().numpy()))\n",
    "\n",
    "        # step to optimize \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # close for each step\n",
    "\n",
    "    # get metric for training set\n",
    "    train_acc = np.mean(epoch_metrics)\n",
    "    val_acc = eval_model(val_loader, model, use_gpu)\n",
    "    val_metrics.append(val_acc)\n",
    "\n",
    "    # print metrics\n",
    "    print('train accuracy mean: ', train_acc)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    print('mean loss: ', np.mean(epoch_losses))\n",
    "\n",
    "    # store model if necessary\n",
    "    state = {\n",
    "                'epoch' : epoch + 1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model': model.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric\n",
    "            }\n",
    "    checkpoint(state, 'dirlink_best_model', val_acc, best_metric)\n",
    "\n",
    "    # patience and last_metric and best_metric update\n",
    "    last_metric = val_acc\n",
    "    counter = counter + 1 if last_metric <= best_metric else 0\n",
    "    best_metric = val_acc if val_acc > best_metric else best_metric\n",
    "\n",
    "    # check if patience run out\n",
    "    if counter >= patience:\n",
    "        break\n",
    "# close for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0190,  1.2483, -0.7047, -0.7333, -0.0585,  1.7611, -0.0795, -0.7876,\n",
       "        -0.5539, -1.0019, -0.2009, -0.3094, -0.8989, -0.5496, -0.8654, -0.6820,\n",
       "        -0.3051,  0.2228,  2.1209, -2.1065,  0.6612,  0.7110,  1.2170,  0.4393,\n",
       "        -0.1922,  0.6692,  1.3587,  1.2519, -1.1299,  0.3805, -0.9622, -0.5148,\n",
       "        -0.9844,  0.7157, -0.2246,  2.2004,  0.4344, -1.5050,  1.8277,  0.2176,\n",
       "         0.4949, -0.3045, -1.7985, -0.2959, -0.9041,  0.5150, -1.7921,  0.8672,\n",
       "         3.4313,  0.4163,  0.8759, -0.5347,  1.3212, -1.1592, -0.9640, -0.8578,\n",
       "         0.1290, -0.3243,  0.9489,  0.2504,  1.6267, -1.8357, -0.1039,  1.0557,\n",
       "        -0.0244, -0.7485, -1.3023, -0.5901,  0.4038, -0.1492,  0.2911, -2.1364,\n",
       "        -0.4225,  1.5543, -1.0871, -2.0006,  2.5803, -0.4677, -0.4921, -0.0424,\n",
       "         0.1109,  1.0093, -0.8610,  0.0930, -0.2415,  0.3658, -0.0446,  0.8197,\n",
       "        -0.3869, -0.7651,  0.7372,  0.1834, -0.4229,  1.2800,  0.9234, -0.3022,\n",
       "        -1.0632,  0.7113,  0.9043, -1.0610], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.weight[ngram_builder.word2id['de']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGramModel = NGramNeuralModel(ngram_builder, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a chinga tu madre\n",
      "likelihood:  -0.02420224\n",
      "\n",
      "chinga a tu madre\n",
      "likelihood:  -0.077396\n",
      "\n",
      "madre a chinga tu\n",
      "likelihood:  -0.5944432\n",
      "\n",
      "a madre chinga tu\n",
      "likelihood:  -0.95215386\n",
      "\n",
      "madre chinga a tu\n",
      "likelihood:  -1.2357066\n",
      "\n",
      "a tu chinga madre\n",
      "likelihood:  -1.5082778\n",
      "\n",
      "chinga tu a madre\n",
      "likelihood:  -2.3956115\n",
      "\n",
      "tu madre chinga a\n",
      "likelihood:  -3.570664\n",
      "\n",
      "chinga madre a tu\n",
      "likelihood:  -3.5873508\n",
      "\n",
      "a chinga madre tu\n",
      "likelihood:  -4.7002716\n",
      "\n",
      "tu chinga madre a\n",
      "likelihood:  -4.8660855\n",
      "\n",
      "tu chinga a madre\n",
      "likelihood:  -4.930147\n",
      "\n",
      "chinga a madre tu\n",
      "likelihood:  -5.6753926\n",
      "\n",
      "madre tu chinga a\n",
      "likelihood:  -5.90559\n",
      "\n",
      "chinga tu madre a\n",
      "likelihood:  -8.489605\n",
      "\n",
      "madre tu a chinga\n",
      "likelihood:  -9.7399435\n",
      "\n",
      "tu a chinga madre\n",
      "likelihood:  -12.767059\n",
      "\n",
      "a tu madre chinga\n",
      "likelihood:  -13.23884\n",
      "\n",
      "chinga madre tu a\n",
      "likelihood:  -14.129946\n",
      "\n",
      "tu a madre chinga\n",
      "likelihood:  -16.036716\n",
      "\n",
      "tu madre a chinga\n",
      "likelihood:  -18.482548\n",
      "\n",
      "madre a tu chinga\n",
      "likelihood:  -21.813423\n",
      "\n",
      "madre chinga tu a\n",
      "likelihood:  -24.532393\n",
      "\n",
      "a madre tu chinga\n",
      "likelihood:  -26.28875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = 'chinga a tu madre'.split()\n",
    "test_structures(tokens, NGramModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
